{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xenq_server.components.web_query import WebsiteScraper, Summarizer, DuckDuckGoSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine  = DuckDuckGoSearch()\n",
    "smr = Summarizer()\n",
    "scraper = WebsiteScraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xenq_server.components.web_query import WebQuery\n",
    "web_query= WebQuery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is a NLP scientist?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-15 14:02:01,452: INFO: lib: response: https://lite.duckduckgo.com/lite/ 200]\n"
     ]
    }
   ],
   "source": [
    "output1 = await web_query.search_web(\"Who is a NLP scientist?\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [{'title': 'How to Become an NLP Scientist - Western Governors University',\n",
    "  'href': 'https://www.wgu.edu/career-guide/information-technology/nlp-scientist-career.html',\n",
    "  'body': 'An NLP scientist is responsible for the technical creation and coding of NLP devices and applications. Ultimately, these professionals provide machines with the ability to understand human languages. Whether an NLP scientist is responsible for the development of a physical device or a mobile application, their project will require the ...',\n",
    "  'processed': ['How to Become an NLP Scientist is responsible for the technical creation and coding of NLP devices and applications. Ultimately, these professionals provide machines with the ability to understand human languages.',\n",
    "   'NLP scientists benefit from undergraduate degrees in computer-related fields. NLP scientists also benefit from experience with the devices or applications to receive NLP technology.',\n",
    "   'An NLP scientist can fulfill a wide variety of tasks. While working to create programs that understand human language, NLP scientists will often interact with other team members.',\n",
    "   'An NLP scientist must satisfy basic education requirements. This includes a bachelor’s degree in computer science or a closely-related technological field.',\n",
    "   \"Lay the groundwork for the computing breakthroughs that will enable tomorrow's technologies. Utilize your previous college courses or IT experience to help you complete your degree faster. Tuition: $4,085 per 6-month term.\",\n",
    "   'A degree in computer science can give you critical IT skills for your future. The M.S. in Computer Science at WGU has three specializations for students to choose from. The specializations are Computing Systems, Human-Computer Interaction, and Machine Learning and Artificial Intelligence.',\n",
    "   'The salary of an NLP scientist averages $105,178 per year, with a range of roughly $78,000 to $139,000 per year. Employment for NLP scientists is expected to see growth of 22% from 2020 to 2021.',\n",
    "   'NLP scientists develop applications, collaborate with team members, and solve problems. Daily, the skills an NLP scientist uses can include: Text representation technique implementation. The ability to associate meaning with individual words in an N LP system.',\n",
    "   'ming languages before systems are approved. These and other skills allow NLP scientists to fulfill the core responsibilities of their job while working well alongside fellow team members. Learn more about degree programs that can prepare you for this meaningful career.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.']},\n",
    " {'title': 'Natural language processing - Wikipedia',\n",
    "  'href': 'https://en.wikipedia.org/wiki/Natural_language_processing',\n",
    "  'body': 'Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence.It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.',\n",
    "  'processed': ['Natural language processing is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is closely related to information retrieval.',\n",
    "   \"Natural language processing has its roots in the 1950s. Alan Turing proposed what is now called the Turing test as a criterion of intelligence. The premise of symbolic NLP is well-summarized by John Searle 's Chinese room experiment.\",\n",
    "   'wn experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, funding for machine translation was dramatically reduced.',\n",
    "   \"mation about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. Ross Quillian 's successful work on natural language was demonstrated with a vocabulary of only twenty words. The 1980s and early 1990s mark the heyday of symbolic methods in NLP.\",\n",
    "   'esearch on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar. Other lines of research were continued. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.',\n",
    "   'Early successes in statistical methods in NLP occurred in the field of machine translation. These systems were able to take advantage of existing multilingual textual corpora. Most other systems depended on corpora specifically developed for the tasks implemented.',\n",
    "   'With the growth of the web, increasing amounts of raw (unannotated) language data have become available. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers.',\n",
    "   'multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.) In the 2010s, representation learning and deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. This is increasingly important in medicine and healthcare where NLP helps analyze notes and text in electronic health records.',\n",
    "   'Symbolic approach was historically the first approach used both by AI in general and by NLP. Machine learning approaches, which include both statistical and neural networks, have many advantages over the symbolic approach.',\n",
    "   'Rule-based systems are commonly used when the amount of training data is insufficient to successfully apply machine learning methods. In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, caused by the inefficiencies of the rule-based approaches.',\n",
    "   'ems of hard if–then rules , were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models , applied to part-of-speech tagging, announced the end of the oldRule-based approach. A major drawback of statistical methods is that they require elaborate feature engineering.',\n",
    "   'Natural language processing tasks can be subdivided into categories for convenience. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.',\n",
    "   'Speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.',\n",
    "   'The task of removing inflectional endings only and to return the base dictionary form of a word is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. Sometimes this process is also used in cases like bag of words creation in data mining.',\n",
    "   'Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts.',\n",
    "   'ming is the process of reducing inflected (or sometimes derived) words to a base form. Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.',\n",
    "   'Determine the parse tree (grammatical analysis) of a given sentence. For a typical sentence there may be thousands of potential parses. Dependency parsing focuses on the relationships between words in a sentence. Constituency parsing uses a probabilistic context-free grammar (PCFG)',\n",
    "   'Named entity recognition (NER) determines which items in the text map to proper names, such as people or places. Capitalization can aid in recognizing named entities in languages such as English, but is often inaccurate or insufficient.',\n",
    "   'not capitalize names that serve as adjectives . Another name for this task is token classification. Sentiment analysis is a computational method used to identify and classify the emotional intent behind text.',\n",
    "   'More than one meaning is required for a word to make sense in a context. We have to select the meaning which makes the most sense in context. Given a piece of text, produce a formal representation of its semantics.',\n",
    "   'Coreference is concerned with matching up pronouns with nouns or names to which they refer. Discourse analysis can be extended to include full-fledged discourse analysis.',\n",
    "   'This rubric includes several related tasks. One task is discourse parsing, i.e. identifying the discourse structure of a connected text. Another possible task is recognizing and classifying the speech acts in a chunk of text.',\n",
    "   'Argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Topic segmentation and recognition is a closely related task.',\n",
    "   'Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, this can now (2019) be considered a largely solved problem.',\n",
    "   'a natural language into formal logic. Natural-language understanding (NLU) Convert chunks of text into more formal representations such as first-order logic structures.',\n",
    "   \"The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed ). The first published work by a neural network was published in 2018, 1 the Road.\",\n",
    "   'Document AI platform sits on top of the NLP technology. It enables users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types.',\n",
    "   'Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. Interest on increasingly abstract, \"cognitive\" aspects of natural language. Increasing interest in multilinguality, and, potentially, multimodality.',\n",
    "   'Most higher-level NLP applications involve aspects that emulate intelligent behaviour. Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses\"',\n",
    "   'Cognitive linguistics is an area of computational linguistics that has strong ties with cognitive psychology. NLP algorithms are built from the perspective of cognitive science and linguistics.',\n",
    "   'tive NLP algorithm alike without additional information. Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece ofText being analyzed.',\n",
    "   'Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Ideas of cognitive NLP are inherent to neural models multimodal NLP and developments in artificial intelligence.',\n",
    "   'Del approaches [ 59 ] and new directions in artificial general intelligence based on the free energy principle. Karl J. Friston at University College London.',\n",
    "   'implification Transformer (machine learning model) Truecasing Question answering Word2vec References [ edit ] ^ \"NLP\" . ^ \"The history of machine translation in a nutshell\" (PDF)',\n",
    "   'Chomskyan linguistics encourages the investigation of \" corner cases \" that stress the limits of its theoretical models. The creation of such corpora of real-world data is a fundamental part of machine-learning algorithms for natural language processing.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.',\n",
    "   'Until 2015, deep learning had evolved into the major framework of NLP. This was an early Deep Learning tutorial at the ACL 2012.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token:  \"U B U W E B :: Racter\"',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.',\n",
    "   'Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.',\n",
    "   \"NLP models are limited by the amount of context they receive. For example, Cain's Jawbone is a puzzle that has stumped humans for decades.\",\n",
    "   'use [difficulties] for researchers who hope to use them to do things such as analyze ancient languages. There are few historical records on long-gone civilizations to serve as training data for such a purpose.',\n",
    "   'Natural language processing is a branch of computer science. It is used in the fields of machine learning and natural language understanding. The term \"natural language processing\" is used to describe a variety of techniques.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token: ',\n",
    "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to Become an NLP Scientist\"\n",
    "import random\n",
    "for data in output:\n",
    "    processed = [chunk for chunk in data[\"processed\"] if query not in chunk]\n",
    "    total = len(processed)\n",
    "    if total <= 10:\n",
    "        data[\"processed\"] = processed  # Keep all if 10 or fewer\n",
    "    else:\n",
    "        indices = sorted(random.sample(range(total), 10))  # Unique, sorted indices\n",
    "        data[\"processed\"] = [processed[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'How to Become an NLP Scientist - Western Governors University',\n",
       "  'href': 'https://www.wgu.edu/career-guide/information-technology/nlp-scientist-career.html',\n",
       "  'body': 'An NLP scientist is responsible for the technical creation and coding of NLP devices and applications. Ultimately, these professionals provide machines with the ability to understand human languages. Whether an NLP scientist is responsible for the development of a physical device or a mobile application, their project will require the ...',\n",
       "  'processed': ['How to Become an NLP Scientist is responsible for the technical creation and coding of NLP devices and applications. Ultimately, these professionals provide machines with the ability to understand human languages.',\n",
       "   'NLP scientists benefit from undergraduate degrees in computer-related fields. NLP scientists also benefit from experience with the devices or applications to receive NLP technology.',\n",
       "   'An NLP scientist can fulfill a wide variety of tasks. While working to create programs that understand human language, NLP scientists will often interact with other team members.',\n",
       "   'An NLP scientist must satisfy basic education requirements. This includes a bachelor’s degree in computer science or a closely-related technological field.',\n",
       "   \"Lay the groundwork for the computing breakthroughs that will enable tomorrow's technologies. Utilize your previous college courses or IT experience to help you complete your degree faster. Tuition: $4,085 per 6-month term.\",\n",
       "   'A degree in computer science can give you critical IT skills for your future. The M.S. in Computer Science at WGU has three specializations for students to choose from. The specializations are Computing Systems, Human-Computer Interaction, and Machine Learning and Artificial Intelligence.',\n",
       "   'The salary of an NLP scientist averages $105,178 per year, with a range of roughly $78,000 to $139,000 per year. Employment for NLP scientists is expected to see growth of 22% from 2020 to 2021.',\n",
       "   'NLP scientists develop applications, collaborate with team members, and solve problems. Daily, the skills an NLP scientist uses can include: Text representation technique implementation. The ability to associate meaning with individual words in an N LP system.',\n",
       "   'ming languages before systems are approved. These and other skills allow NLP scientists to fulfill the core responsibilities of their job while working well alongside fellow team members. Learn more about degree programs that can prepare you for this meaningful career.',\n",
       "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.']},\n",
       " {'title': 'Natural language processing - Wikipedia',\n",
       "  'href': 'https://en.wikipedia.org/wiki/Natural_language_processing',\n",
       "  'body': 'Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence.It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.',\n",
       "  'processed': ['Natural language processing is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is closely related to information retrieval.',\n",
       "   'With the growth of the web, increasing amounts of raw (unannotated) language data have become available. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers.',\n",
       "   'Rule-based systems are commonly used when the amount of training data is insufficient to successfully apply machine learning methods. In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, caused by the inefficiencies of the rule-based approaches.',\n",
       "   'Determine the parse tree (grammatical analysis) of a given sentence. For a typical sentence there may be thousands of potential parses. Dependency parsing focuses on the relationships between words in a sentence. Constituency parsing uses a probabilistic context-free grammar (PCFG)',\n",
       "   'Named entity recognition (NER) determines which items in the text map to proper names, such as people or places. Capitalization can aid in recognizing named entities in languages such as English, but is often inaccurate or insufficient.',\n",
       "   'Coreference is concerned with matching up pronouns with nouns or names to which they refer. Discourse analysis can be extended to include full-fledged discourse analysis.',\n",
       "   'Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, this can now (2019) be considered a largely solved problem.',\n",
       "   'tive NLP algorithm alike without additional information. Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece ofText being analyzed.',\n",
       "   'Del approaches [ 59 ] and new directions in artificial general intelligence based on the free energy principle. Karl J. Friston at University College London.',\n",
       "   'Your task: If this content contains information relevant to the query \"How to Become an NLP Scientist\", summarize only the relevant parts. If it is not relevant at all, respond only with this exact token.']}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-14 09:53:23,858: INFO: lib: response: https://lite.duckduckgo.com/lite/ 200]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'How to Become an NLP Scientist - Western Governors University',\n",
       "  'href': 'https://www.wgu.edu/career-guide/information-technology/nlp-scientist-career.html',\n",
       "  'body': 'An NLP scientist is responsible for the technical creation and coding of NLP devices and applications. Ultimately, these professionals provide machines with the ability to understand human languages. Whether an NLP scientist is responsible for the development of a physical device or a mobile application, their project will require the ...',\n",
       "  'processed': ['How to Become an NLP Scientist. An NLP scientist is responsible for the technical creation and coding of NLP devices and applications. Ultimately, these professionals provide machines with the ability to understand human languages.',\n",
       "   'NLP scientists benefit from undergraduate degrees in computer-related fields. They also benefit from certifications that improve their proficiency in specific programming languages. Predictive text programs are used to forecast what technology users are thinking and recommend corresponding text.',\n",
       "   'nd on your employer, and the nature of your employment. NLP scientists will often interact with other team members, manipulate programming languages, and solve technology-related problems. Designing NLP systems for integration into physical devices, software programs, mobile platforms, and other applications.',\n",
       "   'An NLP scientist needs a bachelor’s degree in computer science or a closely-related technological field. This degree will teach you skills that are of immediate use to NLP scientists as you grow more familiar with modern analytics tools.',\n",
       "   \"Lay the groundwork for the computing breakthroughs that will enable tomorrow's technologies. Utilize your previous college courses or IT experience to help you complete your degree faster. You'll have the opportunity to earn these certifications: Linux Essentials Axelos ITIL Foundation.\",\n",
       "   'A degree in computer science can give you critical IT skills for your future. The M.S. in Computer Science at WGU has three specializations for students to choose from. The specializations are Computing Systems, Human-Computer Interaction, and Machine Learning and Artificial Intelligence.',\n",
       "   'The salary of an NLP scientist averages $105,178 per year. Employment for NLP scientists is expected to see growth of 22% from 2020 to 203.',\n",
       "   'These skills help them develop applications, collaborate with team members, and solve problems that might arise in their work environment. Daily, the skills an NLP scientist uses can include: Text representation technique implementation. Code troubleshooting. Problem-solving.',\n",
       "   'ming languages before systems are approved. These and other skills allow NLP scientists to fulfill the core responsibilities of their job while working well alongside fellow team members. Learn more about degree programs that can prepare you for this meaningful career.',\n",
       "   'larships Support Military Scholarships Financial Aid Faculty Testimonials Student Communities Privacy Policy | Cookie Policy | Contact Us | Sitemap ©2025 WGU. All rights reserved.']},\n",
       " {'title': 'Natural language processing - Wikipedia',\n",
       "  'href': 'https://en.wikipedia.org/wiki/Natural_language_processing',\n",
       "  'body': 'Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence.It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.',\n",
       "  'processed': ['Natural language processing is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language. For human brain processing, see Language processing in the brain.',\n",
       "   \"Natural language processing has its roots in the 1950s. Alan Turing proposed what is now called the Turing test as a criterion of intelligence. The premise of symbolic NLP is well-summarized by John Searle 's Chinese room experiment.\",\n",
       "   'wn experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.',\n",
       "   \"mation about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. Ross Quillian 's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.\",\n",
       "   'esearch on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar ) Other lines of research were continued. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.',\n",
       "   'Early successes in statistical methods in NLP occurred in the field of machine translation. These systems were able to take advantage of existing multilingual textual corpora. Most other systems depended on corpora specifically developed for the tasks implemented by these systems.',\n",
       "   'With the growth of the web, increasing amounts of raw (unannotated) language data have become available. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers.',\n",
       "   'multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.) In the 2010s, representation learning and deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records.',\n",
       "   'Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular. Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach.',\n",
       "   'rds accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce. In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule based approaches.',\n",
       "   'ems of hard if–then rules , were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models , applied to part-of-speech tagging, announced the end of the oldRule-based approach. A major drawback of statistical methods is that they require elaborate feature engineering.',\n",
       "   'Natural language processing tasks can be subdivided into categories for convenience. Some tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. A coarse division is given below.',\n",
       "   'Speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. Text-to-speech can be used to aid the visually impaired.',\n",
       "   'Lemmatization is another technique for reducing words to their normalized form. Separate words into indivi.d with its corresponding numerical token. These numerical tokens are then used in various deep learning methods.',\n",
       "   'Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech.',\n",
       "   'ming The process of reducing inflected (or sometimes derived) words to a base form. Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary. Sentence breaking (also known as \" sentence boundary disambiguation \")',\n",
       "   'ind the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks , but these same characters can serve other purposes (e.g., marking abbreviations ). Parsing Determine the parse tree (grammatical analysis) of a given sentence.',\n",
       "   'Capitalization can aid in recognizing named entities in languages such as English. However, this information cannot aid in determining the type of named entity. Many other languages in non-Western scripts do not have any capitalization at all.',\n",
       "   'not capitalize names that serve as adjectives . Another name for this task is token classification. The goal of terminology extraction is to automatically extract relevant terms from a given corpus.',\n",
       "   'More than one meaning is required for a word to have the most sense in context. Many words—typically proper names—refer to named entities. Given a piece of text, produce a formal representation of its semantics.',\n",
       "   'Coreference is concerned with matching up pronouns with nouns or names to which they refer. Discourse analysis can be extended to include full-fledged discourse analysis.',\n",
       "   'This rubric includes several related tasks. One task is discourse parsing, i.e. identifying the discourse structure of a connected text. Another possible task is recognizing and classifying the speech acts in a chunk of text.',\n",
       "   'Argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.',\n",
       "   'Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, this can now (2019) be considered a largely solved problem.',\n",
       "   'a natural language into formal logic. Machine translation (MT) Automatically translate text from one human language to another. Natural-language understanding (NLU) Convert chunks of text into more formal representations.',\n",
       "   'Natural-language generation (NLG): Convert information from computer databases or semantic intents into readable human language. Book generation is an extension of natural language generation and other NLP tasks. The first machine-generated book was created by a rule-based system in 1984.',\n",
       "   'Document AI platform sits on top of the NLP technology. Allows users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types.',\n",
       "   'iven a description of a scene, generate a 3D model of the scene. Text-to-video: Given a video description, create a video that matches the description.',\n",
       "   'Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses\"',\n",
       "   'h psychology and linguistics. Especially during the age of symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science.',\n",
       "   'tive NLP algorithm alike without additional information. Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece ofText being analyzed.',\n",
       "   'Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Ideas of cognitive NLP have been revived as an approach to achieve explainability under the notion of \"cognitive AI\"',\n",
       "   'del approaches [ 59 ] and new directions in artificial general intelligence based on the free energy principle [ 60 ] by British neuroscientist and theoretician at University College London Karl J. Friston.',\n",
       "   'implification Transformer (machine learning model) Truecasing Question answering Word2vec References [ edit ] ^ \"NLP\" . ^ Hutchins, J. (2005). \"The history of machine translation in a nutshell\"',\n",
       "   'Chomskyan linguistics encourages the investigation of \" corner cases \" that stress the limits of its theoretical models. The creation and use of such corpora of real-world data is a fundamental part of machine-learning algorithms for natural language processing.',\n",
       "   'ngio, Yoshua; Ducharme, Réjean; Vincent, Pascal; Janvin, Christian (March 1, 2003). \"A neural probabilistic language model\" . The Journal of Machine Learning Research . 3 : 1137– 1155 – via ACM Digital Library.',\n",
       "   'Choe, Do Kook; Charniak, Eugene. \"Parsing as Language Modeling\" . Emnlp 2016 . Archived from the original on 2018-10-23 . Retrieved 2018- 10-22 . Vinyals, Oriol; et al. (2014). \"Grammar as a Foreign Language\" (PDF) . Nips2015 . arXiv : 1412.7449 . Bibcode : 2014arXiv1412. 7449V.',\n",
       "   'Until 2015, deep learning had evolved into the major framework of NLP. This was an early Deep Learning tutorial at the ACL 2012. Until then, neural learning was basically rejected because of its lack of statistical interpretability.',\n",
       "   'Segev, Elad (2022). Semantic Network Analysis in Social Sciences. London: Routledge. ISBN 9780367636524.',\n",
       "   'Argumentation Mining is a form of natural language processing. It can be used to extract information from text. It has been used to help with the PASCAL Textual Entailment Challenge.',\n",
       "   '\"Shared Task: Grammatical Error Correction\" . www.comp.nus.edu.sg . Retrieved 2021-01-11. \"Document Understanding AI on Google Cloud (Cloud Next \\'19) – YouTube\" .www.youtube.com . 11 April 2019.',\n",
       "   \"OpenAI's DALL-E AI image generator can now edit pictures, too. Google announces AI advances in text-to-video, language translation, more.\",\n",
       "   'Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology. \"Ask the Cognitive Scientist\" . American Federation of Teachers . 8 August 2014.',\n",
       "   '\"Fluid Construction Grammar – A fully operational processing system for construction grammars\" . Retrieved 2021-01-11 . \"ACL Member Portal | The Association for Computational Linguistics Member Portal\" . www.aclweb.org.',\n",
       "   'Bates, M (1995). \"Models of natural language understanding\" . Proceedings of the National Academy of Sciences of the United States of America . 92 (22): 9977– 9982.',\n",
       "   'use [difficulties] for researchers who hope to use them to do things such as analyze ancient languages. In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.',\n",
       "   'f Statistical Natural Language Processing . The MIT Press. David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language. Springer-Verlag. Semantic parsing Syntactic parsing Part-of-speech tagging.',\n",
       "   'Lemmatisation Lexical analysis Text chunking Stemming Sentence segmentation Word segmentation Automatic summarization Multi-document summarization Sentence extraction Text simplification Machine translation Computer-assisted Example-based Rule-based Statistical Transfer-based Neural Distributional semantics models.',\n",
       "   'age generation Optical character recognition Topic model Document classification Latent Dirichlet allocation Pachinko allocation Computer-assisted reviewing Automated essay scoring Concordancer Grammar checker Predictive text Pronunciation assessment Spell checker Natural language user interface Chatbot Interactive fiction (c.f. Syntax guessing ) Question answering Virtual assistant Voice user interface Related Formal semantics Hallucination Natural Language Toolkit spaCy Portal : Language Authority control databases : National United States Japan Czech Republic Israel',\n",
       "   'All articles needing additional references from May 2024. All articles with unsourced statements. Articles with unsourcing statements from May 2016. Commons category link from Wikidata Search Search Natural language processing 70 languages.']}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = search_engine.search_top_url(\"Who is a NLP scientist?\", 2)\n",
    "# urls\n",
    "processed_content = []\n",
    "for url in urls:\n",
    "    url[\"processed\"] = scraper.run_pipeline(url[\"href\"])[1]\n",
    "    url[\"processed\"] = await smr.summarize_text(url[\"processed\"])\n",
    "\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await smr.summarize_text(urls[0][\"processed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to Become an NLP Scientist. An NLP scientist is responsible for the technical creation and coding of NLP devices and applications. Ultimately, these professionals provide machines with the ability to understand human languages.',\n",
       " 'NLP scientists benefit from undergraduate degrees in computer-related fields. They also benefit from certifications that improve their proficiency in specific programming languages. Predictive text programs are used to forecast what technology users are thinking and recommend corresponding text.',\n",
       " 'nd on your employer, and the nature of your employment. NLP scientists will often interact with other team members, manipulate programming languages, and solve technology-related problems. Designing NLP systems for integration into physical devices, software programs, mobile platforms, and other applications.',\n",
       " 'An NLP scientist needs a bachelor’s degree in computer science or a closely-related technological field. This degree will teach you skills that are of immediate use to NLP scientists as you grow more familiar with modern analytics tools.',\n",
       " \"Lay the groundwork for the computing breakthroughs that will enable tomorrow's technologies. Utilize your previous college courses or IT experience to help you complete your degree faster. You'll have the opportunity to earn these certifications: Linux Essentials Axelos ITIL Foundation.\",\n",
       " 'A degree in computer science can give you critical IT skills for your future. The M.S. in Computer Science at WGU has three specializations for students to choose from. The specializations are Computing Systems, Human-Computer Interaction, and Machine Learning and Artificial Intelligence.',\n",
       " 'The salary of an NLP scientist averages $105,178 per year. Employment for NLP scientists is expected to see growth of 22% from 2020 to 203.',\n",
       " 'These skills help them develop applications, collaborate with team members, and solve problems that might arise in their work environment. Daily, the skills an NLP scientist uses can include: Text representation technique implementation. Code troubleshooting. Problem-solving.',\n",
       " 'ming languages before systems are approved. These and other skills allow NLP scientists to fulfill the core responsibilities of their job while working well alongside fellow team members. Learn more about degree programs that can prepare you for this meaningful career.',\n",
       " 'larships Support Military Scholarships Financial Aid Faculty Testimonials Student Communities Privacy Policy | Cookie Policy | Contact Us | Sitemap ©2025 WGU. All rights reserved.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 1000, but your input_length is only 489. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=244)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 177. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=88)\n",
      "Your max_length is set to 1000, but your input_length is only 177. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=88)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 27\u001b[0m\n\u001b[1;32m      3\u001b[0m ARTICLE \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI do\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m five more times, sometimes only within two weeks of each other.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     26\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mARTICLE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:280\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:173\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    178\u001b[0m     ):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1360\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1357\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1358\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1359\u001b[0m     )\n\u001b[0;32m-> 1360\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:202\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    200\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 202\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:2482\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2475\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2476\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2477\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2478\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2479\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2480\u001b[0m     )\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2482\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2492\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2493\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2494\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2495\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2501\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2502\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:3902\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3899\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3900\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m-> 3902\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3906\u001b[0m     model_outputs,\n\u001b[1;32m   3907\u001b[0m     model_kwargs,\n\u001b[1;32m   3908\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3909\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1654\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1650\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1651\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1652\u001b[0m         )\n\u001b[0;32m-> 1654\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1672\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1673\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1533\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1527\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1528\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1529\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1530\u001b[0m     )\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1533\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1378\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1366\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1367\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         use_cache,\n\u001b[1;32m   1376\u001b[0m     )\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:705\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    703\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m--> 705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_layer_norm\u001b[49m(hidden_states)\n\u001b[1;32m    707\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1915\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1917\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ARTICLE = [\"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\",\n",
    "\"\"\"Natural language processing - Wikipedia Jump to content From Wikipedia, the free encyclopedia Field of linguistics and computer science This article is about computer processing. For human brain processing, see Language processing in the brain . This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources: \"Natural language processing\" – news · newspapers · books · scholar · JSTOR ( May 2024 ) ( Learn how and when to remove this message ) Natural language processing ( NLP ) is a subfield of computer science and especially artificial intelligence . It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval , knowledge representation and computational linguistics , a subfield of linguistics . Major tasks in natural language processing are speech reco ...\n",
    "\"\"\",\"\"\"Natural language processing - Wikipedia Jump to content From Wikipedia, the free encyclopedia Field of linguistics and computer science This article is about computer processing. For human brain processing, see Language processing in the brain . This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources: \"Natural language processing\" – news · newspapers · books · scholar · JSTOR ( May 2024 ) ( Learn how and when to remove this message ) Natural language processing ( NLP ) is a subfield of computer science and especially artificial intelligence . It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval , knowledge representation and computational linguistics , a subfield of linguistics . Major tasks in natural language processing are speech reco ...\n",
    "\"\"\",\"\"\"Natural language processing - Wikipedia Jump to content From Wikipedia, the free encyclopedia Field of linguistics and computer science This article is about computer processing. For human brain processing, see Language processing in the brain . This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources: \"Natural language processing\" – news · newspapers · books · scholar · JSTOR ( May 2024 ) ( Learn how and when to remove this message ) Natural language processing ( NLP ) is a subfield of computer science and especially artificial intelligence . It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval , knowledge representation and computational linguistics , a subfield of linguistics . Major tasks in natural language processing are speech reco ...\n",
    "\"\"\",\n",
    "]\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summary = summarizer(ARTICLE, max_length=1000, min_length=100, do_sample=False)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1484\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1401\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:285\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 285\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:308\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m hf_raise_for_status(response)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[0;31mReadTimeout\u001b[0m: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 6417a447-0225-4d23-88e2-5c82c4ab5114)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1068\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1599\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1600\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1601\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1603\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhead_call_error\u001b[39;00m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# List of documents to summarize\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:965\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1112\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1109\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1110\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1112\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1114\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/configuration_utils.py:590\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/configuration_utils.py:649\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:491\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# Here we only raise if both flags for missing entry and connection errors are True (because it can be raised\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 491\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    492\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load the files, and couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find them in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cached files.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# later on anyway and re-raised if needed\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, HTTPError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import  AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# List of documents to summarize\n",
    "texts = [\"Your long document 1...\", \"Document 2...\", \"Document 3...\"]\n",
    "\n",
    "# Tokenize as a batch\n",
    "inputs = tokenizer(ARTICLE, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "\n",
    "# Move to GPU if available\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "# Generate summaries (batch processing)\n",
    "summaries_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                               max_length=200, min_length=30, do_sample=False)\n",
    "\n",
    "# Decode summaries\n",
    "summaries = tokenizer.batch_decode(summaries_ids, skip_special_tokens=True)\n",
    "\n",
    "for s in summaries:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/urllib3/connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1484\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1401\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:285\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 285\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:308\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m hf_raise_for_status(response)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[0;31mReadTimeout\u001b[0m: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9b7b5f4d-f440-4869-a38c-4ad7076d682c)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1068\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1599\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1600\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1601\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1603\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhead_call_error\u001b[39;00m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Example: Sentiment analysis\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment-analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is amazing!\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/pipelines/__init__.py:851\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 adapter_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    849\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 851\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    856\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1112\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1109\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1110\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1112\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1114\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/configuration_utils.py:590\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/configuration_utils.py:649\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:491\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# Here we only raise if both flags for missing entry and connection errors are True (because it can be raised\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 491\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    492\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load the files, and couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find them in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cached files.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# later on anyway and re-raised if needed\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, HTTPError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Example: Sentiment analysis\n",
    "pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "print(pipe(\"This is amazing!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liana Barrientos, 39, is charged with two counts of offering a false instrument for filing in the first degree. In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. If convicted, she faces up to four years in prison.\n",
      "Natural language processing is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval.\n",
      "Natural language processing is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval.\n",
      "Natural language processing is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval.\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries (batch processing)\n",
    "summaries_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                               max_length=200, min_length=30, do_sample=False)\n",
    "\n",
    "# Decode summaries\n",
    "summaries = tokenizer.batch_decode(summaries_ids, skip_special_tokens=True)\n",
    "\n",
    "for s in summaries:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men, and at one time, she was married to eight men at once. Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force. If convicted, she faces up to four years in prison.'},\n",
       " {'summary_text': 'Natural language processing is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language. For human brain processing, see Language processing in the brain. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. ( Learn how and when to remove this message ) Click here for more information about natural language processing. Click here to go to the Natural language processing Wikipedia page.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 Preprocessed Text:\n",
      "Natural language processing - Wikipedia Jump to content From Wikipedia, the free encyclopedia Field of linguistics and computer science This article is about computer processing. For human brain processing, see Language processing in the brain . This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources: \"Natural language processing\" – news · newspapers · books · scholar · JSTOR ( May 2024 ) ( Learn how and when to remove this message ) Natural language processing ( NLP ) is a subfield of computer science and especially artificial intelligence . It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval , knowledge representation and computational linguistics , a subfield of linguistics . Major tasks in natural language processing are speech reco ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7416"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "class WebsiteScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.raw_html = None\n",
    "        self.cleaned_text = None\n",
    "        self.preprocessed_text = None\n",
    "        self.structured_content = None\n",
    "\n",
    "    # Step 1: Scrape the webpage\n",
    "    def scrape(self):\n",
    "        \"\"\"Fetch raw HTML content from the given URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()\n",
    "            self.raw_html = response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] Failed to scrape {self.url}: {e}\")\n",
    "            self.raw_html = None\n",
    "\n",
    "    # Step 2: Clean HTML and extract readable text\n",
    "    def clean_html(self):\n",
    "        \"\"\"Remove scripts/styles and extract plain visible text.\"\"\"\n",
    "        if not self.raw_html:\n",
    "            print(\"[WARN] No HTML to clean.\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(self.raw_html, 'html.parser')\n",
    "        for tag in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):\n",
    "            tag.decompose()\n",
    "        self.cleaned_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Step 3: Preprocess plain text (basic cleanup)\n",
    "    def preprocess_text(self):\n",
    "        \"\"\"Clean and normalize whitespace in the text.\"\"\"\n",
    "        if not self.cleaned_text:\n",
    "            print(\"[WARN] No text to preprocess.\")\n",
    "            return\n",
    "        text = re.sub(r'\\s+', ' ', self.cleaned_text)\n",
    "        self.preprocessed_text = text.strip()\n",
    "\n",
    "    # Step 4: Structure HTML content into sections\n",
    "    def structure_content(self):\n",
    "        \"\"\"Extract structured info: title, meta, headings, paragraphs, lists.\"\"\"\n",
    "        if not self.raw_html:\n",
    "            print(\"[WARN] No HTML to structure.\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(self.raw_html, 'html.parser')\n",
    "        content = {\n",
    "            \"title\": None,\n",
    "            \"meta_description\": None,\n",
    "            \"headings\": [],\n",
    "            \"paragraphs\": [],\n",
    "            \"lists\": [],\n",
    "        }\n",
    "\n",
    "        title_tag = soup.find('title')\n",
    "        if title_tag:\n",
    "            content['title'] = title_tag.get_text()\n",
    "\n",
    "        meta_desc = soup.find('meta', attrs={\"name\": \"description\"})\n",
    "        if meta_desc:\n",
    "            content['meta_description'] = meta_desc.get('content', '')\n",
    "\n",
    "        for level in range(1, 4):\n",
    "            for h in soup.find_all(f'h{level}'):\n",
    "                content['headings'].append({\n",
    "                    \"level\": level,\n",
    "                    \"text\": h.get_text(strip=True)\n",
    "                })\n",
    "\n",
    "        content['paragraphs'] = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "\n",
    "        for ul in soup.find_all('ul'):\n",
    "            content['lists'].append({\n",
    "                \"type\": \"unordered\",\n",
    "                \"items\": [li.get_text(strip=True) for li in ul.find_all('li')]\n",
    "            })\n",
    "\n",
    "        for ol in soup.find_all('ol'):\n",
    "            content['lists'].append({\n",
    "                \"type\": \"ordered\",\n",
    "                \"items\": [li.get_text(strip=True) for li in ol.find_all('li')]\n",
    "            })\n",
    "\n",
    "        self.structured_content = content\n",
    "\n",
    "    # Step 5: Full pipeline\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the full scraping and processing pipeline.\"\"\"\n",
    "        self.scrape()\n",
    "        if not self.raw_html:\n",
    "            return None, None  # Failed scrape\n",
    "\n",
    "        self.clean_html()\n",
    "        self.preprocess_text()\n",
    "        self.structure_content()\n",
    "\n",
    "        return self.structured_content, self.preprocessed_text\n",
    "\n",
    "    # Optional: Export structured content to JSON\n",
    "    def export_structured_json(self, path=\"structured_content.json\"):\n",
    "        if not self.structured_content:\n",
    "            print(\"[WARN] No structured content to export.\")\n",
    "            return\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.structured_content, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"[INFO] Structured content saved to {path}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "    scraper = WebsiteScraper(url)\n",
    "    structured, preprocessed = scraper.run_pipeline()\n",
    "\n",
    "    # if structured:\n",
    "    #     print(\"\\n📘 Structured Content:\")\n",
    "    #     print(json.dumps(structured, indent=4))\n",
    "\n",
    "    if preprocessed:\n",
    "        print(\"\\n🧹 Preprocessed Text:\")\n",
    "        print(preprocessed[:1000], \"...\")  # Print only first 1000 chars for brevity\n",
    "len(preprocessed.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"Natural language processing - Wikipedia\",\n",
      "    \"meta_description\": null,\n",
      "    \"headings\": [\n",
      "        {\n",
      "            \"level\": 1,\n",
      "            \"text\": \"Natural language processing\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"Contents\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"History\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"Approaches: Symbolic, statistical, neural networks\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"Common NLP tasks\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"General tendencies and (possible) future directions\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"See also\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"References\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"Further reading\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 2,\n",
      "            \"text\": \"External links\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Symbolic NLP (1950s \\u2013 early 1990s)\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Statistical NLP (1990s\\u2013present)\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Statistical approach\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Neural networks\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Text and speech processing\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Morphological analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Syntactic analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Lexical semantics (of individual words in context)\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Relational semantics (semantics of individual sentences)\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Discourse (semantics beyond individual sentences)\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Higher-level NLP applications\"\n",
      "        },\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"Cognition\"\n",
      "        }\n",
      "    ],\n",
      "    \"paragraphs\": [\n",
      "        \"Natural language processing(NLP) is a subfield ofcomputer scienceand especiallyartificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded innatural languageand is thus closely related toinformation retrieval,knowledge representationandcomputational linguistics, a subfield oflinguistics.\",\n",
      "        \"Major tasks in natural language processing arespeech recognition,text classification,natural-language understanding, andnatural-language generation.\",\n",
      "        \"Natural language processing has its roots in the 1950s.[1]Already in 1950,Alan Turingpublished an article titled \\\"Computing Machinery and Intelligence\\\" which proposed what is now called theTuring testas a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\",\n",
      "        \"The premise of symbolic NLP is well-summarized byJohn Searle'sChinese roomexperiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\",\n",
      "        \"Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction ofmachine learningalgorithms for language processing.  This was due to both the steady increase in computational power (seeMoore's law) and the gradual lessening of the dominance ofChomskyantheories of linguistics (e.g.transformational grammar), whose theoretical underpinnings discouraged the sort ofcorpus linguisticsthat underlies the machine-learning approach to language processing.[8]\",\n",
      "        \"Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19]such as by writing grammars or devising heuristic rules forstemming.\",\n",
      "        \"Machine learningapproaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\",\n",
      "        \"Rule-based systems are commonly used:\",\n",
      "        \"In the late 1980s and mid-1990s, the statistical approach ended a period ofAI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\",\n",
      "        \"The earliestdecision trees, producing systems of hardif\\u2013then rules, were still very similar to the old rule-based approaches.\\nOnly the introduction of hiddenMarkov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\",\n",
      "        \"A major drawback of statistical methods is that they require elaboratefeature engineering. Since 2015,[22]the statistical approach has been replaced by theneural networksapproach, usingsemantic networks[23]andword embeddingsto capture semantic properties of words.\",\n",
      "        \"Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore.\",\n",
      "        \"Neural machine translation, based on then-newly inventedsequence-to-sequencetransformations, made obsolete the intermediate steps, such as word alignment, previously necessary forstatistical machine translation.\",\n",
      "        \"The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\",\n",
      "        \"Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\",\n",
      "        \"Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46]\",\n",
      "        \"Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\",\n",
      "        \"Cognitionrefers to \\\"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\\\"[47]Cognitive scienceis the interdisciplinary, scientific study of the mind and its processes.[48]Cognitive linguisticsis an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49]Especially during the age ofsymbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\",\n",
      "        \"As an example,George Lakoffoffers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50]with two defining aspects:\",\n",
      "        \"Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53]functional grammar,[54]construction grammar,[55]computational psycholinguistics and cognitive neuroscience (e.g.,ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56]of theACL). More recently, ideas of cognitive NLP have been revived as an approach to achieveexplainability, e.g., under the notion of \\\"cognitive AI\\\".[57]Likewise, ideas of cognitive NLP are inherent to neural modelsmultimodalNLP (although rarely made explicit)[58]and developments inartificial intelligence, specifically tools and technologies usinglarge language modelapproaches[59]and new directions inartificial general intelligencebased on thefree energy principle[60]by British neuroscientist and theoretician at University College LondonKarl J. Friston.\"\n",
      "    ],\n",
      "    \"lists\": [\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Main page\",\n",
      "                \"Contents\",\n",
      "                \"Current events\",\n",
      "                \"Random article\",\n",
      "                \"About Wikipedia\",\n",
      "                \"Contact us\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Help\",\n",
      "                \"Learn to edit\",\n",
      "                \"Community portal\",\n",
      "                \"Recent changes\",\n",
      "                \"Upload file\",\n",
      "                \"Special pages\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Donate\",\n",
      "                \"Create account\",\n",
      "                \"Log in\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Donate\",\n",
      "                \"Create account\",\n",
      "                \"Log in\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Contributions\",\n",
      "                \"Talk\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"(Top)\",\n",
      "                \"1HistoryToggle History subsection1.1Symbolic NLP (1950s \\u2013 early 1990s)1.2Statistical NLP (1990s\\u2013present)\",\n",
      "                \"1.1Symbolic NLP (1950s \\u2013 early 1990s)\",\n",
      "                \"1.2Statistical NLP (1990s\\u2013present)\",\n",
      "                \"2Approaches: Symbolic, statistical, neural networksToggle Approaches: Symbolic, statistical, neural networks subsection2.1Statistical approach2.2Neural networks\",\n",
      "                \"2.1Statistical approach\",\n",
      "                \"2.2Neural networks\",\n",
      "                \"3Common NLP tasksToggle Common NLP tasks subsection3.1Text and speech processing3.2Morphological analysis3.3Syntactic analysis3.4Lexical semantics (of individual words in context)3.5Relational semantics (semantics of individual sentences)3.6Discourse (semantics beyond individual sentences)3.7Higher-level NLP applications\",\n",
      "                \"3.1Text and speech processing\",\n",
      "                \"3.2Morphological analysis\",\n",
      "                \"3.3Syntactic analysis\",\n",
      "                \"3.4Lexical semantics (of individual words in context)\",\n",
      "                \"3.5Relational semantics (semantics of individual sentences)\",\n",
      "                \"3.6Discourse (semantics beyond individual sentences)\",\n",
      "                \"3.7Higher-level NLP applications\",\n",
      "                \"4General tendencies and (possible) future directionsToggle General tendencies and (possible) future directions subsection4.1Cognition\",\n",
      "                \"4.1Cognition\",\n",
      "                \"5See also\",\n",
      "                \"6References\",\n",
      "                \"7Further reading\",\n",
      "                \"8External links\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"1.1Symbolic NLP (1950s \\u2013 early 1990s)\",\n",
      "                \"1.2Statistical NLP (1990s\\u2013present)\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"2.1Statistical approach\",\n",
      "                \"2.2Neural networks\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"3.1Text and speech processing\",\n",
      "                \"3.2Morphological analysis\",\n",
      "                \"3.3Syntactic analysis\",\n",
      "                \"3.4Lexical semantics (of individual words in context)\",\n",
      "                \"3.5Relational semantics (semantics of individual sentences)\",\n",
      "                \"3.6Discourse (semantics beyond individual sentences)\",\n",
      "                \"3.7Higher-level NLP applications\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"4.1Cognition\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Afrikaans\",\n",
      "                \"\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629\",\n",
      "                \"\\u0531\\u0580\\u0565\\u0582\\u0574\\u057f\\u0561\\u0570\\u0561\\u0575\\u0565\\u0580\\u0567\\u0576\",\n",
      "                \"Az\\u0259rbaycanca\",\n",
      "                \"\\u09ac\\u09be\\u0982\\u09b2\\u09be\",\n",
      "                \"\\u95a9\\u5357\\u8a9e / B\\u00e2n-l\\u00e2m-g\\u00fa\",\n",
      "                \"\\u0411\\u0435\\u043b\\u0430\\u0440\\u0443\\u0441\\u043a\\u0430\\u044f\",\n",
      "                \"\\u0411\\u0435\\u043b\\u0430\\u0440\\u0443\\u0441\\u043a\\u0430\\u044f (\\u0442\\u0430\\u0440\\u0430\\u0448\\u043a\\u0435\\u0432\\u0456\\u0446\\u0430)\",\n",
      "                \"\\u0411\\u044a\\u043b\\u0433\\u0430\\u0440\\u0441\\u043a\\u0438\",\n",
      "                \"Bosanski\",\n",
      "                \"Brezhoneg\",\n",
      "                \"Catal\\u00e0\",\n",
      "                \"\\u010ce\\u0161tina\",\n",
      "                \"Cymraeg\",\n",
      "                \"Dansk\",\n",
      "                \"Deutsch\",\n",
      "                \"Eesti\",\n",
      "                \"\\u0395\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac\",\n",
      "                \"Espa\\u00f1ol\",\n",
      "                \"Esperanto\",\n",
      "                \"Euskara\",\n",
      "                \"\\u0641\\u0627\\u0631\\u0633\\u06cc\",\n",
      "                \"Fran\\u00e7ais\",\n",
      "                \"Gaeilge\",\n",
      "                \"Galego\",\n",
      "                \"\\ud55c\\uad6d\\uc5b4\",\n",
      "                \"\\u0540\\u0561\\u0575\\u0565\\u0580\\u0565\\u0576\",\n",
      "                \"\\u0939\\u093f\\u0928\\u094d\\u0926\\u0940\",\n",
      "                \"Hrvatski\",\n",
      "                \"Bahasa Indonesia\",\n",
      "                \"IsiZulu\",\n",
      "                \"\\u00cdslenska\",\n",
      "                \"Italiano\",\n",
      "                \"\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea\",\n",
      "                \"\\u0c95\\u0ca8\\u0ccd\\u0ca8\\u0ca1\",\n",
      "                \"\\u10e5\\u10d0\\u10e0\\u10d7\\u10e3\\u10da\\u10d8\",\n",
      "                \"Latvie\\u0161u\",\n",
      "                \"Lietuvi\\u0173\",\n",
      "                \"\\u041c\\u0430\\u043a\\u0435\\u0434\\u043e\\u043d\\u0441\\u043a\\u0438\",\n",
      "                \"\\u092e\\u0930\\u093e\\u0920\\u0940\",\n",
      "                \"\\u0645\\u0635\\u0631\\u0649\",\n",
      "                \"\\u041c\\u043e\\u043d\\u0433\\u043e\\u043b\",\n",
      "                \"\\u1019\\u103c\\u1014\\u103a\\u1019\\u102c\\u1018\\u102c\\u101e\\u102c\",\n",
      "                \"Nederlands\",\n",
      "                \"\\u65e5\\u672c\\u8a9e\",\n",
      "                \"Norsk bokm\\u00e5l\",\n",
      "                \"\\u0b13\\u0b21\\u0b3c\\u0b3f\\u0b06\",\n",
      "                \"\\u067e\\u069a\\u062a\\u0648\",\n",
      "                \"Picard\",\n",
      "                \"Piemont\\u00e8is\",\n",
      "                \"Polski\",\n",
      "                \"Portugu\\u00eas\",\n",
      "                \"Qaraqalpaqsha\",\n",
      "                \"Rom\\u00e2n\\u0103\",\n",
      "                \"Runa Simi\",\n",
      "                \"\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439\",\n",
      "                \"Shqip\",\n",
      "                \"Simple English\",\n",
      "                \"\\u06a9\\u0648\\u0631\\u062f\\u06cc\",\n",
      "                \"\\u0421\\u0440\\u043f\\u0441\\u043a\\u0438 / srpski\",\n",
      "                \"Srpskohrvatski / \\u0441\\u0440\\u043f\\u0441\\u043a\\u043e\\u0445\\u0440\\u0432\\u0430\\u0442\\u0441\\u043a\\u0438\",\n",
      "                \"Suomi\",\n",
      "                \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\",\n",
      "                \"\\u0c24\\u0c46\\u0c32\\u0c41\\u0c17\\u0c41\",\n",
      "                \"\\u0e44\\u0e17\\u0e22\",\n",
      "                \"T\\u00fcrk\\u00e7e\",\n",
      "                \"\\u0423\\u043a\\u0440\\u0430\\u0457\\u043d\\u0441\\u044c\\u043a\\u0430\",\n",
      "                \"Ti\\u1ebfng Vi\\u1ec7t\",\n",
      "                \"\\u7cb5\\u8a9e\",\n",
      "                \"\\u4e2d\\u6587\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Article\",\n",
      "                \"Talk\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Read\",\n",
      "                \"Edit\",\n",
      "                \"View history\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Read\",\n",
      "                \"Edit\",\n",
      "                \"View history\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"What links here\",\n",
      "                \"Related changes\",\n",
      "                \"Upload file\",\n",
      "                \"Permanent link\",\n",
      "                \"Page information\",\n",
      "                \"Cite this page\",\n",
      "                \"Get shortened URL\",\n",
      "                \"Download QR code\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Download as PDF\",\n",
      "                \"Printable version\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Wikimedia Commons\",\n",
      "                \"Wikiversity\",\n",
      "                \"Wikidata item\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"1950s: TheGeorgetown experimentin 1954 involved fullyautomatic translationof more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2]However, real progress was much slower, and after theALPAC reportin 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[3]) until the late 1980s when the firststatistical machine translationsystems were developed.\",\n",
      "                \"1960s: Some notably successful natural language processing systems developed in the 1960s wereSHRDLU, a natural language system working in restricted \\\"blocks worlds\\\" with restricted vocabularies, andELIZA, a simulation of aRogerian psychotherapist, written byJoseph Weizenbaumbetween 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \\\"patient\\\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \\\"My head hurts\\\" with \\\"Why do you say your head hurts?\\\".Ross Quillian's successful work on natural language was demonstrated with a vocabulary of onlytwentywords, because that was all that would fit in a computer  memory at the time.[4]\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"1970s: During the 1970s, many programmers began to write \\\"conceptualontologies\\\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the firstchatterbotswere written (e.g.,PARRY).\",\n",
      "                \"1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development ofHPSGas a computational operationalization ofgenerative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g.,Lesk algorithm), reference (e.g., within Centering Theory[6]) and other areas of natural language understanding (e.g., in theRhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots withRacterandJabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[7]\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"1990s: Many of the notable early successes in statistical methods in NLP occurred in the field ofmachine translation, due especially to work at IBM Research, such asIBM alignment models.  These systems were able to take advantage of existing multilingualtextual corporathat had been produced by theParliament of Canadaand theEuropean Unionas a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\",\n",
      "                \"2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused onunsupervisedandsemi-supervised learningalgorithms.  Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult thansupervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of theWorld Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enoughtime complexityto be practical.\",\n",
      "                \"2003:word n-gram model, at the time the best statistical algorithm, is outperformed by amulti-layer perceptron(with a single hidden layer andcontext lengthof several words, trained on up to 14 million words, byBengioet al.)[9]\",\n",
      "                \"2010:Tom\\u00e1\\u0161 Mikolov(then a PhD student atBrno University of Technology) with co-authors applied a simplerecurrent neural networkwith a single hidden layer to language modelling,[10]and in the following years he went on to developWord2vec. In the 2010s,representation learninganddeep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12]can achieve state-of-the-art results in many natural language tasks, e.g., inlanguage modeling[13]and parsing.[14][15]This is increasingly importantin medicine and healthcare, where NLP helps analyze notes and text inelectronic health recordsthat would otherwise be inaccessible for study when seeking to improve care[16]or protect patient privacy.[17]\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading tointractabilityproblems.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by theApertiumsystem,\",\n",
      "                \"for preprocessing in NLP pipelines, e.g.,tokenization, or\",\n",
      "                \"for postprocessing and transforming the output of NLP pipelines, e.g., forknowledge extractionfrom syntactic parses.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Formal system\",\n",
      "                \"Alphabet\",\n",
      "                \"Syntax\",\n",
      "                \"Formal semantics\",\n",
      "                \"Semantics (programming languages)\",\n",
      "                \"Formal grammar\",\n",
      "                \"Formation rule\",\n",
      "                \"Well-formed formula\",\n",
      "                \"Automata theory\",\n",
      "                \"Regular expression\",\n",
      "                \"Production\",\n",
      "                \"Ground expression\",\n",
      "                \"Atomic formula\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Formal methods\",\n",
      "                \"Propositional calculus\",\n",
      "                \"Predicate logic\",\n",
      "                \"Mathematical notation\",\n",
      "                \"Natural language processing\",\n",
      "                \"Programming language theory\",\n",
      "                \"Mathematical linguistics\",\n",
      "                \"Computational linguistics\",\n",
      "                \"Syntax analysis\",\n",
      "                \"Formal verification\",\n",
      "                \"Automated theorem proving\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"v\",\n",
      "                \"t\",\n",
      "                \"e\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Interest on increasingly abstract, \\\"cognitive\\\" aspects of natural language (1999\\u20132001: shallow parsing, 2002\\u201303: named entity recognition, 2006\\u201309/2017\\u201318: dependency syntax, 2004\\u201305/2008\\u201309 semantic role labelling, 2011\\u201312 coreference, 2015\\u201316: discourse parsing, 2019: semantic parsing).\",\n",
      "                \"Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\",\n",
      "                \"Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"1 the Road\",\n",
      "                \"Artificial intelligence detection software\",\n",
      "                \"Automated essay scoring\",\n",
      "                \"Biomedical text mining\",\n",
      "                \"Compound term processing\",\n",
      "                \"Computational linguistics\",\n",
      "                \"Computer-assisted reviewing\",\n",
      "                \"Controlled natural language\",\n",
      "                \"Deep learning\",\n",
      "                \"Deep linguistic processing\",\n",
      "                \"Distributional semantics\",\n",
      "                \"Foreign language reading aid\",\n",
      "                \"Foreign language writing aid\",\n",
      "                \"Information extraction\",\n",
      "                \"Information retrieval\",\n",
      "                \"Language and Communication Technologies\",\n",
      "                \"Language model\",\n",
      "                \"Language technology\",\n",
      "                \"Latent semantic indexing\",\n",
      "                \"Multi-agent system\",\n",
      "                \"Native-language identification\",\n",
      "                \"Natural-language programming\",\n",
      "                \"Natural-language understanding\",\n",
      "                \"Natural-language search\",\n",
      "                \"Outline of natural language processing\",\n",
      "                \"Query expansion\",\n",
      "                \"Query understanding\",\n",
      "                \"Reification (linguistics)\",\n",
      "                \"Speech processing\",\n",
      "                \"Spoken dialogue systems\",\n",
      "                \"Text-proofing\",\n",
      "                \"Text simplification\",\n",
      "                \"Transformer (machine learning model)\",\n",
      "                \"Truecasing\",\n",
      "                \"Question answering\",\n",
      "                \"Word2vec\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Bates, M (1995).\\\"Models of natural language understanding\\\".Proceedings of the National Academy of Sciences of the United States of America.92(22):9977\\u20139982.Bibcode:1995PNAS...92.9977B.doi:10.1073/pnas.92.22.9977.PMC40721.PMID7479812.\",\n",
      "                \"Steven Bird, Ewan Klein, and Edward Loper (2009).Natural Language Processing with Python. O'Reilly Media.ISBN978-0-596-51649-9.\",\n",
      "                \"Kenna Hughes-Castleberry, \\\"A Murder Mystery Puzzle: The literary puzzleCain's Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\\\",Scientific American, vol. 329, no. 4 (November 2023), pp. 81\\u201382. \\\"This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount ofcontextthey receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyzeancient languages. In some cases, there are few historical records on long-gonecivilizationsto serve astraining datafor such a purpose.\\\" (p. 82.)\",\n",
      "                \"Daniel Jurafsky and James H. Martin (2008).Speech and Language Processing, 2nd edition. Pearson Prentice Hall.ISBN978-0-13-187321-6.\",\n",
      "                \"Mohamed Zakaria Kurdi (2016).Natural Language Processing and Computational Linguistics: speech, morphology, and syntax, Volume 1. ISTE-Wiley.ISBN978-1848218482.\",\n",
      "                \"Mohamed Zakaria Kurdi (2017).Natural Language Processing and Computational Linguistics: semantics, discourse, and applications, Volume 2. ISTE-Wiley.ISBN978-1848219212.\",\n",
      "                \"Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch\\u00fctze (2008).Introduction to Information Retrieval. Cambridge University Press.ISBN978-0-521-86571-5.Official html and pdf versions available without charge.\",\n",
      "                \"Christopher D. Manning and Hinrich Sch\\u00fctze (1999).Foundations of Statistical Natural Language Processing. The MIT Press.ISBN978-0-262-13360-9.\",\n",
      "                \"David M. W. Powers and Christopher C. R. Turk (1989).Machine Learning of Natural Language. Springer-Verlag.ISBN978-0-387-19557-5.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Media related toNatural language processingat Wikimedia Commons\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"v\",\n",
      "                \"t\",\n",
      "                \"e\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"AI-complete\",\n",
      "                \"Bag-of-words\",\n",
      "                \"n-gramBigramTrigram\",\n",
      "                \"Bigram\",\n",
      "                \"Trigram\",\n",
      "                \"Computational linguistics\",\n",
      "                \"Natural language understanding\",\n",
      "                \"Stop words\",\n",
      "                \"Text processing\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Bigram\",\n",
      "                \"Trigram\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Argument mining\",\n",
      "                \"Collocation extraction\",\n",
      "                \"Concept mining\",\n",
      "                \"Coreference resolution\",\n",
      "                \"Deep linguistic processing\",\n",
      "                \"Distant reading\",\n",
      "                \"Information extraction\",\n",
      "                \"Named-entity recognition\",\n",
      "                \"Ontology learning\",\n",
      "                \"ParsingSemantic parsingSyntactic parsing\",\n",
      "                \"Semantic parsing\",\n",
      "                \"Syntactic parsing\",\n",
      "                \"Part-of-speech tagging\",\n",
      "                \"Semantic analysis\",\n",
      "                \"Semantic role labeling\",\n",
      "                \"Semantic decomposition\",\n",
      "                \"Semantic similarity\",\n",
      "                \"Sentiment analysis\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Semantic parsing\",\n",
      "                \"Syntactic parsing\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Terminology extraction\",\n",
      "                \"Text mining\",\n",
      "                \"Textual entailment\",\n",
      "                \"Truecasing\",\n",
      "                \"Word-sense disambiguation\",\n",
      "                \"Word-sense induction\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Compound-term processing\",\n",
      "                \"Lemmatisation\",\n",
      "                \"Lexical analysis\",\n",
      "                \"Text chunking\",\n",
      "                \"Stemming\",\n",
      "                \"Sentence segmentation\",\n",
      "                \"Word segmentation\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Multi-document summarization\",\n",
      "                \"Sentence extraction\",\n",
      "                \"Text simplification\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Computer-assisted\",\n",
      "                \"Example-based\",\n",
      "                \"Rule-based\",\n",
      "                \"Statistical\",\n",
      "                \"Transfer-based\",\n",
      "                \"Neural\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"BERT\",\n",
      "                \"Document-term matrix\",\n",
      "                \"Explicit semantic analysis\",\n",
      "                \"fastText\",\n",
      "                \"GloVe\",\n",
      "                \"Language model(large)\",\n",
      "                \"Latent semantic analysis\",\n",
      "                \"Seq2seq\",\n",
      "                \"Word embedding\",\n",
      "                \"Word2vec\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Corpus linguistics\",\n",
      "                \"Lexical resource\",\n",
      "                \"Linguistic Linked Open Data\",\n",
      "                \"Machine-readable dictionary\",\n",
      "                \"Parallel text\",\n",
      "                \"PropBank\",\n",
      "                \"Semantic network\",\n",
      "                \"Simple Knowledge Organization System\",\n",
      "                \"Speech corpus\",\n",
      "                \"Text corpus\",\n",
      "                \"Thesaurus (information retrieval)\",\n",
      "                \"Treebank\",\n",
      "                \"Universal Dependencies\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"BabelNet\",\n",
      "                \"Bank of English\",\n",
      "                \"DBpedia\",\n",
      "                \"FrameNet\",\n",
      "                \"Google Ngram Viewer\",\n",
      "                \"UBY\",\n",
      "                \"WordNet\",\n",
      "                \"Wikidata\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Speech recognition\",\n",
      "                \"Speech segmentation\",\n",
      "                \"Speech synthesis\",\n",
      "                \"Natural language generation\",\n",
      "                \"Optical character recognition\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Document classification\",\n",
      "                \"Latent Dirichlet allocation\",\n",
      "                \"Pachinko allocation\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Automated essay scoring\",\n",
      "                \"Concordancer\",\n",
      "                \"Grammar checker\",\n",
      "                \"Predictive text\",\n",
      "                \"Pronunciation assessment\",\n",
      "                \"Spell checker\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Chatbot\",\n",
      "                \"Interactive fiction(c.f.Syntax guessing)\",\n",
      "                \"Question answering\",\n",
      "                \"Virtual assistant\",\n",
      "                \"Voice user interface\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Formal semantics\",\n",
      "                \"Hallucination\",\n",
      "                \"Natural Language Toolkit\",\n",
      "                \"spaCy\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Language\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"United States\",\n",
      "                \"Japan\",\n",
      "                \"Czech Republic\",\n",
      "                \"Israel\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Natural language processing\",\n",
      "                \"Computational fields of study\",\n",
      "                \"Computational linguistics\",\n",
      "                \"Speech recognition\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"All accuracy disputes\",\n",
      "                \"Accuracy disputes from December 2013\",\n",
      "                \"Harv and Sfn no-target errors\",\n",
      "                \"CS1 errors: periodical ignored\",\n",
      "                \"CS1 maint: location\",\n",
      "                \"Articles with short description\",\n",
      "                \"Short description is different from Wikidata\",\n",
      "                \"Articles needing additional references from May 2024\",\n",
      "                \"All articles needing additional references\",\n",
      "                \"All articles with unsourced statements\",\n",
      "                \"Articles with unsourced statements from May 2024\",\n",
      "                \"Commons category link from Wikidata\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"This page was last edited on 14 March 2025, at 19:45(UTC).\",\n",
      "                \"Text is available under theCreative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to theTerms of UseandPrivacy Policy. Wikipedia\\u00ae is a registered trademark of theWikimedia Foundation, Inc., a non-profit organization.\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"Privacy policy\",\n",
      "                \"About Wikipedia\",\n",
      "                \"Disclaimers\",\n",
      "                \"Contact Wikipedia\",\n",
      "                \"Code of Conduct\",\n",
      "                \"Developers\",\n",
      "                \"Statistics\",\n",
      "                \"Cookie statement\",\n",
      "                \"Mobile view\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": [\n",
      "                \"\",\n",
      "                \"\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"unordered\",\n",
      "            \"items\": []\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ordered\",\n",
      "            \"items\": [\n",
      "                \"Apply the theory ofconceptual metaphor, explained by Lakoff as \\\"the understanding of one idea, in terms of another\\\" which provides an idea of the intent of the author.[51]For example, consider the English wordbig. When used in a comparison (\\\"That is a big tree\\\"), the author's intent is to imply that the tree isphysically largerelative to other trees or the authors experience.  When used metaphorically (\\\"Tomorrow is a big day\\\"), the author's intent to implyimportance.  The intent behind other usages, like in \\\"She is a big person\\\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.\",\n",
      "                \"Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of aprobabilistic context-free grammar(PCFG). The mathematical equation for such algorithms is presented inUS Patent 9269353:[52]\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"ordered\",\n",
      "            \"items\": [\n",
      "                \"^\\\"NLP\\\".\",\n",
      "                \"^Hutchins, J. (2005).\\\"The history of machine translation in a nutshell\\\"(PDF).[self-published source]\",\n",
      "                \"^\\\"ALPAC: the (in)famous report\\\", John Hutchins, MT News International, no. 14, June 1996, pp. 9\\u201312.\",\n",
      "                \"^Crevier 1993, pp.\\u00a0146\\u2013148harvnb error: no target: CITEREFCrevier1993 (help), see alsoBuchanan 2005, p.\\u00a056harvnb error: no target: CITEREFBuchanan2005 (help): \\\"Early programs were necessarily limited in scope by the size and speed of memory\\\"\",\n",
      "                \"^Koskenniemi, Kimmo(1983),Two-level morphology: A general computational model of word-form recognition and production(PDF), Department of General Linguistics,University of Helsinki\",\n",
      "                \"^Joshi, A. K., & Weinstein, S. (1981, August).Control of Inference: Role of Some Aspects of Discourse Structure-Centering. InIJCAI(pp. 385\\u2013387).\",\n",
      "                \"^Guida, G.; Mauri, G. (July 1986). \\\"Evaluation of natural language processing systems: Issues and approaches\\\".Proceedings of the IEEE.74(7):1026\\u20131035.doi:10.1109/PROC.1986.13580.ISSN1558-2256.S2CID30688575.\",\n",
      "                \"^Chomskyan linguistics encourages the investigation of \\\"corner cases\\\" that stress the limits of its theoretical models (comparable topathologicalphenomena in mathematics), typically created usingthought experiments, rather than the systematic investigation of typical phenomena that occur in real-world data, as is the case incorpus linguistics.  The creation and use of suchcorporaof real-world data is a fundamental part of machine-learning algorithms for natural language processing.  In addition, theoretical underpinnings of Chomskyan linguistics such as the so-called \\\"poverty of the stimulus\\\" argument entail that general learning algorithms, as are typically used in machine learning, cannot be successful in language processing.  As a result, the Chomskyan paradigm discouraged the application of such models to language processing.\",\n",
      "                \"^Bengio, Yoshua; Ducharme, R\\u00e9jean; Vincent, Pascal; Janvin, Christian (March 1, 2003).\\\"A neural probabilistic language model\\\".The Journal of Machine Learning Research.3:1137\\u20131155 \\u2013 via ACM Digital Library.\",\n",
      "                \"^Mikolov, Tom\\u00e1\\u0161; Karafi\\u00e1t, Martin; Burget, Luk\\u00e1\\u0161; \\u010cernock\\u00fd, Jan; Khudanpur, Sanjeev (26 September 2010).\\\"Recurrent neural network based language model\\\"(PDF).Interspeech 2010. pp.1045\\u20131048.doi:10.21437/Interspeech.2010-343.S2CID17048224.{{cite book}}:|journal=ignored (help)\",\n",
      "                \"^Goldberg, Yoav (2016). \\\"A Primer on Neural Network Models for Natural Language Processing\\\".Journal of Artificial Intelligence Research.57:345\\u2013420.arXiv:1807.10854.doi:10.1613/jair.4992.S2CID8273530.\",\n",
      "                \"^Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016).Deep Learning. MIT Press.\",\n",
      "                \"^Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016).Exploring the Limits of Language Modeling.arXiv:1602.02410.Bibcode:2016arXiv160202410J.\",\n",
      "                \"^Choe, Do Kook; Charniak, Eugene.\\\"Parsing as Language Modeling\\\".Emnlp 2016. Archived fromthe originalon 2018-10-23. Retrieved2018-10-22.\",\n",
      "                \"^Vinyals, Oriol; et\\u00a0al. (2014).\\\"Grammar as a Foreign Language\\\"(PDF).Nips2015.arXiv:1412.7449.Bibcode:2014arXiv1412.7449V.\",\n",
      "                \"^Turchin, Alexander; Florez Builes, Luisa F. (2021-03-19).\\\"Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review\\\".Journal of Diabetes Science and Technology.15(3):553\\u2013560.doi:10.1177/19322968211000831.ISSN1932-2968.PMC8120048.PMID33736486.\",\n",
      "                \"^Lee, Jennifer; Yang, Samuel; Holland-Hall, Cynthia; Sezgin, Emre; Gill, Manjot; Linwood, Simon; Huang, Yungui; Hoffman, Jeffrey (2022-06-10).\\\"Prevalence of Sensitive Terms in Clinical Notes Using Natural Language Processing Techniques: Observational Study\\\".JMIR Medical Informatics.10(6): e38482.doi:10.2196/38482.ISSN2291-9694.PMC9233261.PMID35687381.\",\n",
      "                \"^Winograd, Terry (1971).Procedures as a Representation for Data in a Computer Program for Understanding Natural Language(Thesis).\",\n",
      "                \"^Schank, Roger C.; Abelson, Robert P. (1977).Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Hillsdale: Erlbaum.ISBN0-470-99033-3.\",\n",
      "                \"^Mark Johnson. How the statistical revolution changes (computational) linguistics.Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics.\",\n",
      "                \"^Philip Resnik. Four revolutions.Language Log, February 5, 2011.\",\n",
      "                \"^Socher, Richard.\\\"Deep Learning For NLP-ACL 2012 Tutorial\\\".www.socher.org. Retrieved2020-08-17.This was an early Deep Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability. Until 2015, deep learning had evolved into the major framework of NLP. [Link is broken, tryhttp://web.stanford.edu/class/cs224n/]\",\n",
      "                \"^Segev, Elad (2022).Semantic Network Analysis in Social Sciences. London: Routledge.ISBN9780367636524.Archivedfrom the original on 5 December 2021. Retrieved5 December2021.\",\n",
      "                \"^Yi, Chucai;Tian, Yingli(2012), \\\"Assistive Text Reading from Complex Background for Blind Persons\\\",Camera-Based Document Analysis and Recognition, Lecture Notes in Computer Science, vol.\\u00a07139, Springer Berlin Heidelberg, pp.15\\u201328,CiteSeerX10.1.1.668.869,doi:10.1007/978-3-642-29364-1_2,ISBN9783642293634\",\n",
      "                \"^ab\\\"Natural Language Processing (NLP) - A Complete Guide\\\".www.deeplearning.ai. 2023-01-11. Retrieved2024-05-05.\",\n",
      "                \"^\\\"What is Natural Language Processing? Intro to NLP in Machine Learning\\\".GyanSetu!. 2020-12-06. Retrieved2021-01-09.\",\n",
      "                \"^Kishorjit, N.; Vidya, Raj RK.; Nirmal, Y.; Sivaji, B. (2012).\\\"Manipuri Morpheme Identification\\\"(PDF).Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP). COLING 2012, Mumbai, December 2012:95\\u2013108.{{cite journal}}:  CS1 maint: location (link)\",\n",
      "                \"^Klein, Dan; Manning, Christopher D. (2002).\\\"Natural language grammar induction using a constituent-context model\\\"(PDF).Advances in Neural Information Processing Systems.\",\n",
      "                \"^Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Math\\u00e9, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023).\\\"Precision information extraction for rare disease epidemiology at scale\\\".Journal of Translational Medicine.21(1): 157.doi:10.1186/s12967-023-04011-y.PMC9972634.PMID36855134.\",\n",
      "                \"^PASCAL Recognizing Textual Entailment Challenge (RTE-7)https://tac.nist.gov//2011/RTE/\",\n",
      "                \"^Lippi, Marco; Torroni, Paolo (2016-04-20).\\\"Argumentation Mining: State of the Art and Emerging Trends\\\".ACM Transactions on Internet Technology.16(2):1\\u201325.doi:10.1145/2850417.hdl:11585/523460.ISSN1533-5399.S2CID9561587.\",\n",
      "                \"^\\\"Argument Mining \\u2013 IJCAI2016 Tutorial\\\".www.i3s.unice.fr. Retrieved2021-03-09.\",\n",
      "                \"^\\\"NLP Approaches to Computational Argumentation \\u2013 ACL 2016, Berlin\\\". Retrieved2021-03-09.\",\n",
      "                \"^Administration.\\\"Centre for Language Technology (CLT)\\\".Macquarie University. Retrieved2021-01-11.\",\n",
      "                \"^\\\"Shared Task: Grammatical Error Correction\\\".www.comp.nus.edu.sg. Retrieved2021-01-11.\",\n",
      "                \"^\\\"Shared Task: Grammatical Error Correction\\\".www.comp.nus.edu.sg. Retrieved2021-01-11.\",\n",
      "                \"^Duan, Yucong; Cruz, Christophe (2011).\\\"Formalizing Semantic of Natural Language through Conceptualization from Existence\\\".International Journal of Innovation, Management and Technology.2(1):37\\u201342. Archived fromthe originalon 2011-10-09.\",\n",
      "                \"^\\\"U B U W E B\\u00a0:: Racter\\\".www.ubu.com. Retrieved2020-08-17.\",\n",
      "                \"^Writer, Beta (2019).Lithium-Ion Batteries.doi:10.1007/978-3-030-16800-1.ISBN978-3-030-16799-8.S2CID155818532.\",\n",
      "                \"^\\\"Document Understanding AI on Google Cloud (Cloud Next '19) \\u2013 YouTube\\\".www.youtube.com. 11 April 2019. Archived fromthe originalon 2021-10-30. Retrieved2021-01-11.\",\n",
      "                \"^Robertson, Adi (2022-04-06).\\\"OpenAI's DALL-E AI image generator can now edit pictures, too\\\".The Verge. Retrieved2022-06-07.\",\n",
      "                \"^\\\"The Stanford Natural Language Processing Group\\\".nlp.stanford.edu. Retrieved2022-06-07.\",\n",
      "                \"^Coyne, Bob; Sproat, Richard (2001-08-01).\\\"WordsEye\\\".Proceedings of the 28th annual conference on Computer graphics and interactive techniques. SIGGRAPH '01. New York, NY, USA: Association for Computing Machinery. pp.487\\u2013496.doi:10.1145/383259.383316.ISBN978-1-58113-374-5.S2CID3842372.\",\n",
      "                \"^\\\"Google announces AI advances in text-to-video, language translation, more\\\".VentureBeat. 2022-11-02. Retrieved2022-11-09.\",\n",
      "                \"^Vincent, James (2022-09-29).\\\"Meta's new text-to-video AI generator is like DALL-E for video\\\".The Verge. Retrieved2022-11-09.\",\n",
      "                \"^\\\"Previous shared tasks | CoNLL\\\".www.conll.org. Retrieved2021-01-11.\",\n",
      "                \"^\\\"Cognition\\\".Lexico.Oxford University PressandDictionary.com. Archived fromthe originalon July 15, 2020. Retrieved6 May2020.\",\n",
      "                \"^\\\"Ask the Cognitive Scientist\\\".American Federation of Teachers. 8 August 2014.Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind.\",\n",
      "                \"^Robinson, Peter (2008).Handbook of Cognitive Linguistics and Second Language Acquisition. Routledge. pp.3\\u20138.ISBN978-0-805-85352-0.\",\n",
      "                \"^Lakoff, George (1999).Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp.569\\u2013583.ISBN978-0-465-05674-3.\",\n",
      "                \"^Strauss, Claudia (1999).A Cognitive Theory of Cultural Meaning. Cambridge University Press. pp.156\\u2013164.ISBN978-0-521-59541-4.\",\n",
      "                \"^US patent 9269353\",\n",
      "                \"^\\\"Universal Conceptual Cognitive Annotation (UCCA)\\\".Universal Conceptual Cognitive Annotation (UCCA). Retrieved2021-01-11.\",\n",
      "                \"^Rodr\\u00edguez, F. C., & Mairal-Us\\u00f3n, R. (2016).Building an RRG computational grammar.Onomazein, (34), 86\\u2013117.\",\n",
      "                \"^\\\"Fluid Construction Grammar \\u2013 A fully operational processing system for construction grammars\\\". Retrieved2021-01-11.\",\n",
      "                \"^\\\"ACL Member Portal | The Association for Computational Linguistics Member Portal\\\".www.aclweb.org. Retrieved2021-01-11.\",\n",
      "                \"^\\\"Chunks and Rules\\\".W3C. Retrieved2021-01-11.\",\n",
      "                \"^Socher, Richard; Karpathy, Andrej; Le, Quoc V.; Manning, Christopher D.; Ng, Andrew Y. (2014).\\\"Grounded Compositional Semantics for Finding and Describing Images with Sentences\\\".Transactions of the Association for Computational Linguistics.2:207\\u2013218.doi:10.1162/tacl_a_00177.S2CID2317858.\",\n",
      "                \"^Dasgupta, Ishita; Lampinen, Andrew K.; Chan, Stephanie C. Y.; Creswell, Antonia; Kumaran, Dharshan; McClelland, James L.; Hill, Felix (2022). \\\"Language models show human-like content effects on reasoning, Dasgupta, Lampinen et al\\\".arXiv:2207.07051[cs.CL].\",\n",
      "                \"^Friston, Karl J. (2022).Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference. The MIT Press.ISBN978-0-262-36997-8.\"\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "Preprocessed Text:\n",
      "Natural language processing - Wikipedia Jump to content From Wikipedia, the free encyclopedia Field of linguistics and computer science This article is about computer processing. For human brain processing, see Language processing in the brain . This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed. Find sources: \"Natural language processing\" – news · newspapers · books · scholar · JSTOR ( May 2024 ) ( Learn how and when to remove this message ) Natural language processing ( NLP ) is a subfield of computer science and especially artificial intelligence . It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval , knowledge representation and computational linguistics , a subfield of linguistics . Major tasks in natural language processing are speech recognition , text classification , natural-language understanding , and natural-language generation . History [ edit ] Further information: History of natural language processing Natural language processing has its roots in the 1950s. [ 1 ] Already in 1950, Alan Turing published an article titled \" Computing Machinery and Intelligence \" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. Symbolic NLP (1950s – early 1990s) [ edit ] The premise of symbolic NLP is well-summarized by John Searle 's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. 1950s : The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. [ 2 ] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe [ 3 ] ) until the late 1980s when the first statistical machine translation systems were developed. 1960s : Some notably successful natural language processing systems developed in the 1960s were SHRDLU , a natural language system working in restricted \" blocks worlds \" with restricted vocabularies, and ELIZA , a simulation of a Rogerian psychotherapist , written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian 's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time. [ 4 ] 1970s : During the 1970s, many programmers began to write \"conceptual ontologies \", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY ). 1980s : The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar ), morphology (e.g., two-level morphology [ 5 ] ), semantics (e.g., Lesk algorithm ), reference (e.g., within Centering Theory [ 6 ] ) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory ). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky . An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period. [ 7 ] Statistical NLP (1990s–present) [ edit ] Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law ) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar ), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. [ 8 ] 1990s : Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation , due especially to work at IBM Research, such as IBM alignment models . These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data. 2000s : With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning , and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web ), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical. 2003: word n-gram model , at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.) [ 9 ] 2010: Tomáš Mikolov (then a PhD student at Brno University of Technology ) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling, [ 10 ] and in the following years he went on to develop Word2vec . In the 2010s, representation learning and deep neural network -style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques [ 11 ] [ 12 ] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling [ 13 ] and parsing. [ 14 ] [ 15 ] This is increasingly important in medicine and healthcare , where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care [ 16 ] or protect patient privacy. [ 17 ] Approaches: Symbolic, statistical, neural networks [ edit ] Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: [ 18 ] [ 19 ] such as by writing grammars or devising heuristic rules for stemming . Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally. language models , produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce. the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems. Rule-based systems are commonly used: when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, for preprocessing in NLP pipelines, e.g., tokenization , or for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses. Statistical approach [ edit ] In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter , which was caused by the inefficiencies of the rule-based approaches. [ 20 ] [ 21 ] The earliest decision trees , producing systems of hard if–then rules , were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models , applied to part-of-speech tagging, announced the end of the old rule-based approach. Neural networks [ edit ] Further information: Artificial neural network A major drawback of statistical methods is that they require elaborate feature engineering . Since 2015, [ 22 ] the statistical approach has been replaced by the neural networks approach, using semantic networks [ 23 ] and word embeddings to capture semantic properties of words. Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. Neural machine translation , based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation . Common NLP tasks [ edit ] The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Text and speech processing [ edit ] Optical character recognition (OCR) Given an image representing printed text, determine the corresponding text. Speech recognition Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \" AI-complete \" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation , so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. Speech segmentation Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it. Text-to-speech Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired. [ 24 ] Word segmentation ( Tokenization ) Tokenization is a process used in text analysis that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods. [ 25 ] For a language like English , this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese , Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining. [ citation needed ] Morphological analysis [ edit ] Lemmatization The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form. [ 26 ] Morphological segmentation Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology ( i.e. , the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology , and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei , a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms. [ 27 ] Part-of-speech tagging Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective ; and \"out\" can be any of at least five different parts of speech. Stemming The process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary. Syntactic analysis [ edit ] Part of a series on Formal languages Key concepts Formal system Alphabet Syntax Formal semantics Semantics (programming languages) Formal grammar Formation rule Well-formed formula Automata theory Regular expression Production Ground expression Atomic formula Applications Formal methods Propositional calculus Predicate logic Mathematical notation Natural language processing Programming language theory Mathematical linguistics Computational linguistics Syntax analysis Formal verification Automated theorem proving v t e Grammar induction [ 28 ] Generate a formal grammar that describes a language's syntax. Sentence breaking (also known as \" sentence boundary disambiguation \") Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks , but these same characters can serve other purposes (e.g., marking abbreviations ). Parsing Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing . Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar ). Lexical semantics (of individual words in context) [ edit ] Lexical semantics What is the computational meaning of individual words in context? Distributional semantics How can we learn semantic representations from data? Named entity recognition (NER) Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity , and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic ) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns , regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives . Another name for this task is token classification. [ 29 ] Sentiment analysis (see also Multimodal sentiment analysis ) Sentiment analysis is a computational method used to identify and classify the emotional intent behind text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams , Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. [ 25 ] Terminology extraction The goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD) Many words have more than one meaning ; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet . Entity linking Many words—typically proper names—refer to named entities ; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context. Relational semantics (semantics of individual sentences) [ edit ] Relationship extraction Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom). Semantic parsing Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing ) or in accordance with a logical formalism (e.g., in DRT parsing ). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below). Semantic role labelling (see also implicit semantic role labelling below) Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames ), then identify and classify the frame elements ( semantic roles ). Discourse (semantics beyond individual sentences) [ edit ] Coreference resolution Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions . For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to). Discourse analysis This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no question, content question, statement, assertion, etc.). Implicit semantic role labelling Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames ) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages . Recognizing textual entailment Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false. [ 30 ] Topic segmentation and recognition Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment. Argument mining The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. [ 31 ] Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse. [ 32 ] [ 33 ] Higher-level NLP applications [ edit ] Automatic summarization (text summarization) Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper. Grammatical error correction Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. [ 34 ] [ 35 ] [ 36 ] As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2 , this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications. Logic translation Translate a text from a natural language into formal logic. Machine translation (MT) Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \" AI-complete \", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly. Natural-language understanding (NLU) Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption , or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization. [ 37 ] Natural-language generation (NLG): Convert information from computer databases or semantic intents into readable human language. Book generation Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed ). [ 38 ] The first published work by a neural network was published in 2018, 1 the Road , marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models . The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries , Springer, Cham). [ 39 ] Unlike Racter and 1 the Road , this is grounded on factual knowledge and based on text summarization. Document AI A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants. [ 40 ] Dialogue management Computer systems intended to converse with a human. Question answering Given a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation Given a description of an image, generate an image that matches the description. [ 41 ] Text-to-scene generation Given a description of a scene, generate a 3D model of the scene. [ 42 ] [ 43 ] Text-to-video Given a description of a video, generate a video that matches the description. [ 44 ] [ 45 ] General tendencies and (possible) future directions [ edit ] Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed: [ 46 ] Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing). Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages) Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems) Cognition [ edit ] Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" [ 47 ] Cognitive science is the interdisciplinary, scientific study of the mind and its processes. [ 48 ] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [ 49 ] Especially during the age of symbolic NLP , the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, [ 50 ] with two defining aspects: Apply the theory of conceptual metaphor , explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. [ 51 ] For example, consider the English word big . When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance . The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information. Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US Patent 9269353 : [ 52 ] R M M ( t o k e n N ) = P M M ( t o k e n N ) × 1 2 d ( ∑ i = − d d ( ( P M M ( t o k e n N ) × P F ( t o k e n N − i , t o k e n N , t o k e n N + i ) ) i ) {\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\times {\\frac {1}{2d}}\\left(\\sum _{i=-d}^{d}{((PMM(token_{N})}\\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\\right)} Where RMM is the relative measure of meaning token is any block of text, sentence, phrase or word N is the number of tokens being analyzed PMM is the probable measure of meaning based on a corpora d is the non zero location of the token along the sequence of N tokens PF is the probability function specific to a language Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, [ 53 ] functional grammar, [ 54 ] construction grammar, [ 55 ] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R ), however, with limited uptake in mainstream NLP (as measured by presence on major conferences [ 56 ] of the ACL ). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability , e.g., under the notion of \"cognitive AI\". [ 57 ] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) [ 58 ] and developments in artificial intelligence , specifically tools and technologies using large language model approaches [ 59 ] and new directions in artificial general intelligence based on the free energy principle [ 60 ] by British neuroscientist and theoretician at University College London Karl J. Friston . See also [ edit ] 1 the Road Artificial intelligence detection software Automated essay scoring Biomedical text mining Compound term processing Computational linguistics Computer-assisted reviewing Controlled natural language Deep learning Deep linguistic processing Distributional semantics Foreign language reading aid Foreign language writing aid Information extraction Information retrieval Language and Communication Technologies Language model Language technology Latent semantic indexing Multi-agent system Native-language identification Natural-language programming Natural-language understanding Natural-language search Outline of natural language processing Query expansion Query understanding Reification (linguistics) Speech processing Spoken dialogue systems Text-proofing Text simplification Transformer (machine learning model) Truecasing Question answering Word2vec References [ edit ] ^ \"NLP\" . ^ Hutchins, J. (2005). \"The history of machine translation in a nutshell\" (PDF) . [ self-published source ] ^ \"ALPAC: the (in)famous report\", John Hutchins, MT News International, no. 14, June 1996, pp. 9–12. ^ Crevier 1993 , pp. 146–148 harvnb error: no target: CITEREFCrevier1993 ( help ) , see also Buchanan 2005 , p. 56 harvnb error: no target: CITEREFBuchanan2005 ( help ) : \"Early programs were necessarily limited in scope by the size and speed of memory\" ^ Koskenniemi, Kimmo (1983), Two-level morphology: A general computational model of word-form recognition and production (PDF) , Department of General Linguistics, University of Helsinki ^ Joshi, A. K., & Weinstein, S. (1981, August). Control of Inference: Role of Some Aspects of Discourse Structure-Centering . In IJCAI (pp. 385–387). ^ Guida, G.; Mauri, G. (July 1986). \"Evaluation of natural language processing systems: Issues and approaches\". Proceedings of the IEEE . 74 (7): 1026– 1035. doi : 10.1109/PROC.1986.13580 . ISSN 1558-2256 . S2CID 30688575 . ^ Chomskyan linguistics encourages the investigation of \" corner cases \" that stress the limits of its theoretical models (comparable to pathological phenomena in mathematics), typically created using thought experiments , rather than the systematic investigation of typical phenomena that occur in real-world data, as is the case in corpus linguistics . The creation and use of such corpora of real-world data is a fundamental part of machine-learning algorithms for natural language processing. In addition, theoretical underpinnings of Chomskyan linguistics such as the so-called \" poverty of the stimulus \" argument entail that general learning algorithms, as are typically used in machine learning, cannot be successful in language processing. As a result, the Chomskyan paradigm discouraged the application of such models to language processing. ^ Bengio, Yoshua; Ducharme, Réjean; Vincent, Pascal; Janvin, Christian (March 1, 2003). \"A neural probabilistic language model\" . The Journal of Machine Learning Research . 3 : 1137– 1155 – via ACM Digital Library. ^ Mikolov, Tomáš; Karafiát, Martin; Burget, Lukáš; Černocký, Jan; Khudanpur, Sanjeev (26 September 2010). \"Recurrent neural network based language model\" (PDF) . Interspeech 2010 . pp. 1045– 1048. doi : 10.21437/Interspeech.2010-343 . S2CID 17048224 . {{ cite book }} : |journal= ignored ( help ) ^ Goldberg, Yoav (2016). \"A Primer on Neural Network Models for Natural Language Processing\". Journal of Artificial Intelligence Research . 57 : 345– 420. arXiv : 1807.10854 . doi : 10.1613/jair.4992 . S2CID 8273530 . ^ Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). Deep Learning . MIT Press. ^ Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). Exploring the Limits of Language Modeling . arXiv : 1602.02410 . Bibcode : 2016arXiv160202410J . ^ Choe, Do Kook; Charniak, Eugene. \"Parsing as Language Modeling\" . Emnlp 2016 . Archived from the original on 2018-10-23 . Retrieved 2018-10-22 . ^ Vinyals, Oriol; et al. (2014). \"Grammar as a Foreign Language\" (PDF) . Nips2015 . arXiv : 1412.7449 . Bibcode : 2014arXiv1412.7449V . ^ Turchin, Alexander; Florez Builes, Luisa F. (2021-03-19). \"Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review\" . Journal of Diabetes Science and Technology . 15 (3): 553– 560. doi : 10.1177/19322968211000831 . ISSN 1932-2968 . PMC 8120048 . PMID 33736486 . ^ Lee, Jennifer; Yang, Samuel; Holland-Hall, Cynthia; Sezgin, Emre; Gill, Manjot; Linwood, Simon; Huang, Yungui; Hoffman, Jeffrey (2022-06-10). \"Prevalence of Sensitive Terms in Clinical Notes Using Natural Language Processing Techniques: Observational Study\" . JMIR Medical Informatics . 10 (6): e38482. doi : 10.2196/38482 . ISSN 2291-9694 . PMC 9233261 . PMID 35687381 . ^ Winograd, Terry (1971). Procedures as a Representation for Data in a Computer Program for Understanding Natural Language (Thesis). ^ Schank, Roger C.; Abelson, Robert P. (1977). Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures . Hillsdale: Erlbaum. ISBN 0-470-99033-3 . ^ Mark Johnson. How the statistical revolution changes (computational) linguistics. Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics. ^ Philip Resnik. Four revolutions. Language Log, February 5, 2011. ^ Socher, Richard. \"Deep Learning For NLP-ACL 2012 Tutorial\" . www.socher.org . Retrieved 2020-08-17 . This was an early Deep Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability. Until 2015, deep learning had evolved into the major framework of NLP. [Link is broken, try http://web.stanford.edu/class/cs224n/ ] ^ Segev, Elad (2022). Semantic Network Analysis in Social Sciences . London: Routledge. ISBN 9780367636524 . Archived from the original on 5 December 2021 . Retrieved 5 December 2021 . ^ Yi, Chucai; Tian, Yingli (2012), \"Assistive Text Reading from Complex Background for Blind Persons\", Camera-Based Document Analysis and Recognition , Lecture Notes in Computer Science, vol. 7139, Springer Berlin Heidelberg, pp. 15– 28, CiteSeerX 10.1.1.668.869 , doi : 10.1007/978-3-642-29364-1_2 , ISBN 9783642293634 ^ a b \"Natural Language Processing (NLP) - A Complete Guide\" . www.deeplearning.ai . 2023-01-11 . Retrieved 2024-05-05 . ^ \"What is Natural Language Processing? Intro to NLP in Machine Learning\" . GyanSetu! . 2020-12-06 . Retrieved 2021-01-09 . ^ Kishorjit, N.; Vidya, Raj RK.; Nirmal, Y.; Sivaji, B. (2012). \"Manipuri Morpheme Identification\" (PDF) . Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP) . COLING 2012, Mumbai, December 2012: 95– 108. {{ cite journal }} : CS1 maint: location ( link ) ^ Klein, Dan; Manning, Christopher D. (2002). \"Natural language grammar induction using a constituent-context model\" (PDF) . Advances in Neural Information Processing Systems . ^ Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Mathé, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023). \"Precision information extraction for rare disease epidemiology at scale\" . Journal of Translational Medicine . 21 (1): 157. doi : 10.1186/s12967-023-04011-y . PMC 9972634 . PMID 36855134 . ^ PASCAL Recognizing Textual Entailment Challenge (RTE-7) https://tac.nist.gov//2011/RTE/ ^ Lippi, Marco; Torroni, Paolo (2016-04-20). \"Argumentation Mining: State of the Art and Emerging Trends\" . ACM Transactions on Internet Technology . 16 (2): 1– 25. doi : 10.1145/2850417 . hdl : 11585/523460 . ISSN 1533-5399 . S2CID 9561587 . ^ \"Argument Mining – IJCAI2016 Tutorial\" . www.i3s.unice.fr . Retrieved 2021-03-09 . ^ \"NLP Approaches to Computational Argumentation – ACL 2016, Berlin\" . Retrieved 2021-03-09 . ^ Administration. \"Centre for Language Technology (CLT)\" . Macquarie University . Retrieved 2021-01-11 . ^ \"Shared Task: Grammatical Error Correction\" . www.comp.nus.edu.sg . Retrieved 2021-01-11 . ^ \"Shared Task: Grammatical Error Correction\" . www.comp.nus.edu.sg . Retrieved 2021-01-11 . ^ Duan, Yucong; Cruz, Christophe (2011). \"Formalizing Semantic of Natural Language through Conceptualization from Existence\" . International Journal of Innovation, Management and Technology . 2 (1): 37– 42. Archived from the original on 2011-10-09. ^ \"U B U W E B :: Racter\" . www.ubu.com . Retrieved 2020-08-17 . ^ Writer, Beta (2019). Lithium-Ion Batteries . doi : 10.1007/978-3-030-16800-1 . ISBN 978-3-030-16799-8 . S2CID 155818532 . ^ \"Document Understanding AI on Google Cloud (Cloud Next '19) – YouTube\" . www.youtube.com . 11 April 2019. Archived from the original on 2021-10-30 . Retrieved 2021-01-11 . ^ Robertson, Adi (2022-04-06). \"OpenAI's DALL-E AI image generator can now edit pictures, too\" . The Verge . Retrieved 2022-06-07 . ^ \"The Stanford Natural Language Processing Group\" . nlp.stanford.edu . Retrieved 2022-06-07 . ^ Coyne, Bob; Sproat, Richard (2001-08-01). \"WordsEye\" . Proceedings of the 28th annual conference on Computer graphics and interactive techniques . SIGGRAPH '01. New York, NY, USA: Association for Computing Machinery. pp. 487– 496. doi : 10.1145/383259.383316 . ISBN 978-1-58113-374-5 . S2CID 3842372 . ^ \"Google announces AI advances in text-to-video, language translation, more\" . VentureBeat . 2022-11-02 . Retrieved 2022-11-09 . ^ Vincent, James (2022-09-29). \"Meta's new text-to-video AI generator is like DALL-E for video\" . The Verge . Retrieved 2022-11-09 . ^ \"Previous shared tasks | CoNLL\" . www.conll.org . Retrieved 2021-01-11 . ^ \"Cognition\" . Lexico . Oxford University Press and Dictionary.com . Archived from the original on July 15, 2020 . Retrieved 6 May 2020 . ^ \"Ask the Cognitive Scientist\" . American Federation of Teachers . 8 August 2014. Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind. ^ Robinson, Peter (2008). Handbook of Cognitive Linguistics and Second Language Acquisition . Routledge. pp. 3– 8. ISBN 978-0-805-85352-0 . ^ Lakoff, George (1999). Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm . New York Basic Books. pp. 569– 583. ISBN 978-0-465-05674-3 . ^ Strauss, Claudia (1999). A Cognitive Theory of Cultural Meaning . Cambridge University Press. pp. 156– 164. ISBN 978-0-521-59541-4 . ^ US patent 9269353 ^ \"Universal Conceptual Cognitive Annotation (UCCA)\" . Universal Conceptual Cognitive Annotation (UCCA) . Retrieved 2021-01-11 . ^ Rodríguez, F. C., & Mairal-Usón, R. (2016). Building an RRG computational grammar . Onomazein , (34), 86–117. ^ \"Fluid Construction Grammar – A fully operational processing system for construction grammars\" . Retrieved 2021-01-11 . ^ \"ACL Member Portal | The Association for Computational Linguistics Member Portal\" . www.aclweb.org . Retrieved 2021-01-11 . ^ \"Chunks and Rules\" . W3C . Retrieved 2021-01-11 . ^ Socher, Richard; Karpathy, Andrej; Le, Quoc V.; Manning, Christopher D.; Ng, Andrew Y. (2014). \"Grounded Compositional Semantics for Finding and Describing Images with Sentences\" . Transactions of the Association for Computational Linguistics . 2 : 207– 218. doi : 10.1162/tacl_a_00177 . S2CID 2317858 . ^ Dasgupta, Ishita; Lampinen, Andrew K.; Chan, Stephanie C. Y.; Creswell, Antonia; Kumaran, Dharshan; McClelland, James L.; Hill, Felix (2022). \"Language models show human-like content effects on reasoning, Dasgupta, Lampinen et al\". arXiv : 2207.07051 [ cs.CL ]. ^ Friston, Karl J. (2022). Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference . The MIT Press. ISBN 978-0-262-36997-8 . Further reading [ edit ] Bates, M (1995). \"Models of natural language understanding\" . Proceedings of the National Academy of Sciences of the United States of America . 92 (22): 9977– 9982. Bibcode : 1995PNAS...92.9977B . doi : 10.1073/pnas.92.22.9977 . PMC 40721 . PMID 7479812 . Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python . O'Reilly Media. ISBN 978-0-596-51649-9 . Kenna Hughes-Castleberry , \"A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone , which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\", Scientific American , vol. 329, no. 4 (November 2023), pp. 81–82. \"This murder mystery competition has revealed that although NLP ( natural-language processing ) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages . In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.\" (p. 82.) Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing , 2nd edition. Pearson Prentice Hall. ISBN 978-0-13-187321-6 . Mohamed Zakaria Kurdi (2016). Natural Language Processing and Computational Linguistics: speech, morphology, and syntax , Volume 1. ISTE-Wiley. ISBN 978-1848218482 . Mohamed Zakaria Kurdi (2017). Natural Language Processing and Computational Linguistics: semantics, discourse, and applications , Volume 2. ISTE-Wiley. ISBN 978-1848219212 . Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008). Introduction to Information Retrieval . Cambridge University Press. ISBN 978-0-521-86571-5 . Official html and pdf versions available without charge. Christopher D. Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language Processing . The MIT Press. ISBN 978-0-262-13360-9 . David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language . Springer-Verlag. ISBN 978-0-387-19557-5 . External links [ edit ] Media related to Natural language processing at Wikimedia Commons v t e Natural language processing General terms AI-complete Bag-of-words n-gram Bigram Trigram Computational linguistics Natural language understanding Stop words Text processing Text analysis Argument mining Collocation extraction Concept mining Coreference resolution Deep linguistic processing Distant reading Information extraction Named-entity recognition Ontology learning Parsing Semantic parsing Syntactic parsing Part-of-speech tagging Semantic analysis Semantic role labeling Semantic decomposition Semantic similarity Sentiment analysis Terminology extraction Text mining Textual entailment Truecasing Word-sense disambiguation Word-sense induction Text segmentation Compound-term processing Lemmatisation Lexical analysis Text chunking Stemming Sentence segmentation Word segmentation Automatic summarization Multi-document summarization Sentence extraction Text simplification Machine translation Computer-assisted Example-based Rule-based Statistical Transfer-based Neural Distributional semantics models BERT Document-term matrix Explicit semantic analysis fastText GloVe Language model ( large ) Latent semantic analysis Seq2seq Word embedding Word2vec Language resources , datasets and corpora Types and standards Corpus linguistics Lexical resource Linguistic Linked Open Data Machine-readable dictionary Parallel text PropBank Semantic network Simple Knowledge Organization System Speech corpus Text corpus Thesaurus (information retrieval) Treebank Universal Dependencies Data BabelNet Bank of English DBpedia FrameNet Google Ngram Viewer UBY WordNet Wikidata Automatic identification and data capture Speech recognition Speech segmentation Speech synthesis Natural language generation Optical character recognition Topic model Document classification Latent Dirichlet allocation Pachinko allocation Computer-assisted reviewing Automated essay scoring Concordancer Grammar checker Predictive text Pronunciation assessment Spell checker Natural language user interface Chatbot Interactive fiction (c.f. Syntax guessing ) Question answering Virtual assistant Voice user interface Related Formal semantics Hallucination Natural Language Toolkit spaCy Portal : Language Authority control databases : National United States Japan Czech Republic Israel Retrieved from \" https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=1280467837 \" Categories : Natural language processing Computational fields of study Computational linguistics Speech recognition Hidden categories: All accuracy disputes Accuracy disputes from December 2013 Harv and Sfn no-target errors CS1 errors: periodical ignored CS1 maint: location Articles with short description Short description is different from Wikidata Articles needing additional references from May 2024 All articles needing additional references All articles with unsourced statements Articles with unsourced statements from May 2024 Commons category link from Wikidata Search Search Natural language processing 70 languages Add topic\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Step 1: Scraping the webpage\n",
    "def scrape_website(url):\n",
    "    \"\"\"Scrape the website and extract the raw HTML content.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Will raise an HTTPError if the status code is 4xx/5xx\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping the website: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Parse and clean HTML content\n",
    "def clean_html(raw_html):\n",
    "    \"\"\"Clean the HTML content to remove unwanted tags and scripts.\"\"\"\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    # Remove unwanted tags (e.g., scripts, styles, nav, footer, etc.)\n",
    "    for tag in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Extract the visible text\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    return text\n",
    "\n",
    "# Step 3: Structure the content into sections (headings, paragraphs, etc.)\n",
    "def structure_content(raw_html):\n",
    "    \"\"\"Structure the content into a dictionary format for LLMs.\"\"\"\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    structured_content = {\n",
    "        \"title\": None,\n",
    "        \"meta_description\": None,\n",
    "        \"headings\": [],\n",
    "        \"paragraphs\": [],\n",
    "        \"lists\": [],\n",
    "    }\n",
    "\n",
    "    # Example: Getting the title and meta description\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        structured_content['title'] = title_tag.get_text()\n",
    "\n",
    "    meta_desc_tag = soup.find('meta', attrs={\"name\": \"description\"})\n",
    "    if meta_desc_tag:\n",
    "        structured_content['meta_description'] = meta_desc_tag.get('content', '')\n",
    "\n",
    "    # Collect headings (h1, h2, h3)\n",
    "    for level in range(1, 4):\n",
    "        for heading in soup.find_all(f'h{level}'):\n",
    "            structured_content['headings'].append({\n",
    "                \"level\": level,\n",
    "                \"text\": heading.get_text(strip=True)\n",
    "            })\n",
    "    \n",
    "    # Collect paragraphs\n",
    "    structured_content['paragraphs'] = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "\n",
    "    # Collect lists (unordered and ordered)\n",
    "    for ul in soup.find_all('ul'):\n",
    "        list_items = [li.get_text(strip=True) for li in ul.find_all('li')]\n",
    "        structured_content['lists'].append({\n",
    "            \"type\": \"unordered\",\n",
    "            \"items\": list_items\n",
    "        })\n",
    "\n",
    "    for ol in soup.find_all('ol'):\n",
    "        list_items = [li.get_text(strip=True) for li in ol.find_all('li')]\n",
    "        structured_content['lists'].append({\n",
    "            \"type\": \"ordered\",\n",
    "            \"items\": list_items\n",
    "        })\n",
    "\n",
    "    return structured_content\n",
    "\n",
    "# Step 4: Preprocess text (clean up unwanted elements)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text by removing unwanted characters.\"\"\"\n",
    "    # Remove extra spaces, newline characters, etc.\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Optionally, remove stopwords here (depending on LLM needs)\n",
    "\n",
    "    # Return cleaned text\n",
    "    return text.strip()\n",
    "\n",
    "# Step 5: Putting everything together into a pipeline\n",
    "def process_website_for_llm(url):\n",
    "    \"\"\"Complete pipeline for scraping, cleaning, and preparing content for LLM.\"\"\"\n",
    "    \n",
    "    # Scrape the website content\n",
    "    raw_html = scrape_website(url)\n",
    "    if not raw_html:\n",
    "        return None\n",
    "    \n",
    "    # Clean and preprocess the HTML content\n",
    "    cleaned_text = clean_html(raw_html)\n",
    "    preprocessed_text = preprocess_text(cleaned_text)\n",
    "    \n",
    "    # Structure the content into a format for LLM processing\n",
    "    structured_content = structure_content(raw_html)\n",
    "\n",
    "    # Return the structured content and the cleaned/preprocessed text\n",
    "    return structured_content, preprocessed_text\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"  # Example URL\n",
    "\n",
    "    structured_content, preprocessed_text = process_website_for_llm(url)\n",
    "    \n",
    "    # Output structured content (optional, can be printed or saved)\n",
    "    print(json.dumps(structured_content, indent=4))\n",
    "\n",
    "    # Output cleaned and preprocessed text\n",
    "    print(\"\\nPreprocessed Text:\")\n",
    "    print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Natural language processing - Wikipedia\n",
      "\n",
      "**Meta Description**: No meta description\n",
      "\n",
      "## Natural language processing\n",
      "\n",
      "### Contents\n",
      "\n",
      "### History\n",
      "\n",
      "### Approaches: Symbolic, statistical, neural networks\n",
      "\n",
      "### Common NLP tasks\n",
      "\n",
      "### General tendencies and (possible) future directions\n",
      "\n",
      "### See also\n",
      "\n",
      "### References\n",
      "\n",
      "### Further reading\n",
      "\n",
      "### External links\n",
      "\n",
      "#### Symbolic NLP (1950s – early 1990s)\n",
      "\n",
      "#### Statistical NLP (1990s–present)\n",
      "\n",
      "#### Statistical approach\n",
      "\n",
      "#### Neural networks\n",
      "\n",
      "#### Text and speech processing\n",
      "\n",
      "#### Morphological analysis\n",
      "\n",
      "#### Syntactic analysis\n",
      "\n",
      "#### Lexical semantics (of individual words in context)\n",
      "\n",
      "#### Relational semantics (semantics of individual sentences)\n",
      "\n",
      "#### Discourse (semantics beyond individual sentences)\n",
      "\n",
      "#### Higher-level NLP applications\n",
      "\n",
      "#### Cognition\n",
      "\n",
      "Natural language processing(NLP) is a subfield ofcomputer scienceand especiallyartificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded innatural languageand is thus closely related toinformation retrieval,knowledge representationandcomputational linguistics, a subfield oflinguistics.\n",
      "\n",
      "Major tasks in natural language processing arespeech recognition,text classification,natural-language understanding, andnatural-language generation.\n",
      "\n",
      "Natural language processing has its roots in the 1950s.[1]Already in 1950,Alan Turingpublished an article titled \"Computing Machinery and Intelligence\" which proposed what is now called theTuring testas a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n",
      "\n",
      "The premise of symbolic NLP is well-summarized byJohn Searle'sChinese roomexperiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n",
      "\n",
      "Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction ofmachine learningalgorithms for language processing.  This was due to both the steady increase in computational power (seeMoore's law) and the gradual lessening of the dominance ofChomskyantheories of linguistics (e.g.transformational grammar), whose theoretical underpinnings discouraged the sort ofcorpus linguisticsthat underlies the machine-learning approach to language processing.[8]\n",
      "\n",
      "Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19]such as by writing grammars or devising heuristic rules forstemming.\n",
      "\n",
      "Machine learningapproaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\n",
      "\n",
      "Rule-based systems are commonly used:\n",
      "\n",
      "In the late 1980s and mid-1990s, the statistical approach ended a period ofAI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\n",
      "\n",
      "The earliestdecision trees, producing systems of hardif–then rules, were still very similar to the old rule-based approaches.\n",
      "Only the introduction of hiddenMarkov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\n",
      "\n",
      "A major drawback of statistical methods is that they require elaboratefeature engineering. Since 2015,[22]the statistical approach has been replaced by theneural networksapproach, usingsemantic networks[23]andword embeddingsto capture semantic properties of words.\n",
      "\n",
      "Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore.\n",
      "\n",
      "Neural machine translation, based on then-newly inventedsequence-to-sequencetransformations, made obsolete the intermediate steps, such as word alignment, previously necessary forstatistical machine translation.\n",
      "\n",
      "The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n",
      "\n",
      "Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n",
      "\n",
      "Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46]\n",
      "\n",
      "Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n",
      "\n",
      "Cognitionrefers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[47]Cognitive scienceis the interdisciplinary, scientific study of the mind and its processes.[48]Cognitive linguisticsis an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49]Especially during the age ofsymbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\n",
      "\n",
      "As an example,George Lakoffoffers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50]with two defining aspects:\n",
      "\n",
      "Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53]functional grammar,[54]construction grammar,[55]computational psycholinguistics and cognitive neuroscience (e.g.,ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56]of theACL). More recently, ideas of cognitive NLP have been revived as an approach to achieveexplainability, e.g., under the notion of \"cognitive AI\".[57]Likewise, ideas of cognitive NLP are inherent to neural modelsmultimodalNLP (although rarely made explicit)[58]and developments inartificial intelligence, specifically tools and technologies usinglarge language modelapproaches[59]and new directions inartificial general intelligencebased on thefree energy principle[60]by British neuroscientist and theoretician at University College LondonKarl J. Friston.\n",
      "\n",
      "\n",
      "* Main page\n",
      "* Contents\n",
      "* Current events\n",
      "* Random article\n",
      "* About Wikipedia\n",
      "* Contact us\n",
      "\n",
      "\n",
      "* Help\n",
      "* Learn to edit\n",
      "* Community portal\n",
      "* Recent changes\n",
      "* Upload file\n",
      "* Special pages\n",
      "\n",
      "\n",
      "* Donate\n",
      "* Create account\n",
      "* Log in\n",
      "\n",
      "\n",
      "* Donate\n",
      "* Create account\n",
      "* Log in\n",
      "\n",
      "\n",
      "* Contributions\n",
      "* Talk\n",
      "\n",
      "\n",
      "* (Top)\n",
      "* 1HistoryToggle History subsection1.1Symbolic NLP (1950s – early 1990s)1.2Statistical NLP (1990s–present)\n",
      "* 1.1Symbolic NLP (1950s – early 1990s)\n",
      "* 1.2Statistical NLP (1990s–present)\n",
      "* 2Approaches: Symbolic, statistical, neural networksToggle Approaches: Symbolic, statistical, neural networks subsection2.1Statistical approach2.2Neural networks\n",
      "* 2.1Statistical approach\n",
      "* 2.2Neural networks\n",
      "* 3Common NLP tasksToggle Common NLP tasks subsection3.1Text and speech processing3.2Morphological analysis3.3Syntactic analysis3.4Lexical semantics (of individual words in context)3.5Relational semantics (semantics of individual sentences)3.6Discourse (semantics beyond individual sentences)3.7Higher-level NLP applications\n",
      "* 3.1Text and speech processing\n",
      "* 3.2Morphological analysis\n",
      "* 3.3Syntactic analysis\n",
      "* 3.4Lexical semantics (of individual words in context)\n",
      "* 3.5Relational semantics (semantics of individual sentences)\n",
      "* 3.6Discourse (semantics beyond individual sentences)\n",
      "* 3.7Higher-level NLP applications\n",
      "* 4General tendencies and (possible) future directionsToggle General tendencies and (possible) future directions subsection4.1Cognition\n",
      "* 4.1Cognition\n",
      "* 5See also\n",
      "* 6References\n",
      "* 7Further reading\n",
      "* 8External links\n",
      "\n",
      "\n",
      "* 1.1Symbolic NLP (1950s – early 1990s)\n",
      "* 1.2Statistical NLP (1990s–present)\n",
      "\n",
      "\n",
      "* 2.1Statistical approach\n",
      "* 2.2Neural networks\n",
      "\n",
      "\n",
      "* 3.1Text and speech processing\n",
      "* 3.2Morphological analysis\n",
      "* 3.3Syntactic analysis\n",
      "* 3.4Lexical semantics (of individual words in context)\n",
      "* 3.5Relational semantics (semantics of individual sentences)\n",
      "* 3.6Discourse (semantics beyond individual sentences)\n",
      "* 3.7Higher-level NLP applications\n",
      "\n",
      "\n",
      "* 4.1Cognition\n",
      "\n",
      "\n",
      "* Afrikaans\n",
      "* العربية\n",
      "* Արեւմտահայերէն\n",
      "* Azərbaycanca\n",
      "* বাংলা\n",
      "* 閩南語 / Bân-lâm-gú\n",
      "* Беларуская\n",
      "* Беларуская (тарашкевіца)\n",
      "* Български\n",
      "* Bosanski\n",
      "* Brezhoneg\n",
      "* Català\n",
      "* Čeština\n",
      "* Cymraeg\n",
      "* Dansk\n",
      "* Deutsch\n",
      "* Eesti\n",
      "* Ελληνικά\n",
      "* Español\n",
      "* Esperanto\n",
      "* Euskara\n",
      "* فارسی\n",
      "* Français\n",
      "* Gaeilge\n",
      "* Galego\n",
      "* 한국어\n",
      "* Հայերեն\n",
      "* हिन्दी\n",
      "* Hrvatski\n",
      "* Bahasa Indonesia\n",
      "* IsiZulu\n",
      "* Íslenska\n",
      "* Italiano\n",
      "* עברית\n",
      "* ಕನ್ನಡ\n",
      "* ქართული\n",
      "* Latviešu\n",
      "* Lietuvių\n",
      "* Македонски\n",
      "* मराठी\n",
      "* مصرى\n",
      "* Монгол\n",
      "* မြန်မာဘာသာ\n",
      "* Nederlands\n",
      "* 日本語\n",
      "* Norsk bokmål\n",
      "* ଓଡ଼ିଆ\n",
      "* پښتو\n",
      "* Picard\n",
      "* Piemontèis\n",
      "* Polski\n",
      "* Português\n",
      "* Qaraqalpaqsha\n",
      "* Română\n",
      "* Runa Simi\n",
      "* Русский\n",
      "* Shqip\n",
      "* Simple English\n",
      "* کوردی\n",
      "* Српски / srpski\n",
      "* Srpskohrvatski / српскохрватски\n",
      "* Suomi\n",
      "* தமிழ்\n",
      "* తెలుగు\n",
      "* ไทย\n",
      "* Türkçe\n",
      "* Українська\n",
      "* Tiếng Việt\n",
      "* 粵語\n",
      "* 中文\n",
      "\n",
      "\n",
      "* Article\n",
      "* Talk\n",
      "\n",
      "\n",
      "* Read\n",
      "* Edit\n",
      "* View history\n",
      "\n",
      "\n",
      "* Read\n",
      "* Edit\n",
      "* View history\n",
      "\n",
      "\n",
      "* What links here\n",
      "* Related changes\n",
      "* Upload file\n",
      "* Permanent link\n",
      "* Page information\n",
      "* Cite this page\n",
      "* Get shortened URL\n",
      "* Download QR code\n",
      "\n",
      "\n",
      "* Download as PDF\n",
      "* Printable version\n",
      "\n",
      "\n",
      "* Wikimedia Commons\n",
      "* Wikiversity\n",
      "* Wikidata item\n",
      "\n",
      "\n",
      "* 1950s: TheGeorgetown experimentin 1954 involved fullyautomatic translationof more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2]However, real progress was much slower, and after theALPAC reportin 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[3]) until the late 1980s when the firststatistical machine translationsystems were developed.\n",
      "* 1960s: Some notably successful natural language processing systems developed in the 1960s wereSHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, andELIZA, a simulation of aRogerian psychotherapist, written byJoseph Weizenbaumbetween 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".Ross Quillian's successful work on natural language was demonstrated with a vocabulary of onlytwentywords, because that was all that would fit in a computer  memory at the time.[4]\n",
      "\n",
      "\n",
      "* 1970s: During the 1970s, many programmers began to write \"conceptualontologies\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the firstchatterbotswere written (e.g.,PARRY).\n",
      "* 1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development ofHPSGas a computational operationalization ofgenerative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g.,Lesk algorithm), reference (e.g., within Centering Theory[6]) and other areas of natural language understanding (e.g., in theRhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots withRacterandJabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[7]\n",
      "\n",
      "\n",
      "* 1990s: Many of the notable early successes in statistical methods in NLP occurred in the field ofmachine translation, due especially to work at IBM Research, such asIBM alignment models.  These systems were able to take advantage of existing multilingualtextual corporathat had been produced by theParliament of Canadaand theEuropean Unionas a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n",
      "* 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused onunsupervisedandsemi-supervised learningalgorithms.  Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult thansupervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of theWorld Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enoughtime complexityto be practical.\n",
      "* 2003:word n-gram model, at the time the best statistical algorithm, is outperformed by amulti-layer perceptron(with a single hidden layer andcontext lengthof several words, trained on up to 14 million words, byBengioet al.)[9]\n",
      "* 2010:Tomáš Mikolov(then a PhD student atBrno University of Technology) with co-authors applied a simplerecurrent neural networkwith a single hidden layer to language modelling,[10]and in the following years he went on to developWord2vec. In the 2010s,representation learninganddeep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12]can achieve state-of-the-art results in many natural language tasks, e.g., inlanguage modeling[13]and parsing.[14][15]This is increasingly importantin medicine and healthcare, where NLP helps analyze notes and text inelectronic health recordsthat would otherwise be inaccessible for study when seeking to improve care[16]or protect patient privacy.[17]\n",
      "\n",
      "\n",
      "* both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.\n",
      "\n",
      "\n",
      "* language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.\n",
      "\n",
      "\n",
      "* the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading tointractabilityproblems.\n",
      "\n",
      "\n",
      "* when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by theApertiumsystem,\n",
      "* for preprocessing in NLP pipelines, e.g.,tokenization, or\n",
      "* for postprocessing and transforming the output of NLP pipelines, e.g., forknowledge extractionfrom syntactic parses.\n",
      "\n",
      "\n",
      "* Formal system\n",
      "* Alphabet\n",
      "* Syntax\n",
      "* Formal semantics\n",
      "* Semantics (programming languages)\n",
      "* Formal grammar\n",
      "* Formation rule\n",
      "* Well-formed formula\n",
      "* Automata theory\n",
      "* Regular expression\n",
      "* Production\n",
      "* Ground expression\n",
      "* Atomic formula\n",
      "\n",
      "\n",
      "* Formal methods\n",
      "* Propositional calculus\n",
      "* Predicate logic\n",
      "* Mathematical notation\n",
      "* Natural language processing\n",
      "* Programming language theory\n",
      "* Mathematical linguistics\n",
      "* Computational linguistics\n",
      "* Syntax analysis\n",
      "* Formal verification\n",
      "* Automated theorem proving\n",
      "\n",
      "\n",
      "* v\n",
      "* t\n",
      "* e\n",
      "\n",
      "\n",
      "* Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).\n",
      "* Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\n",
      "* Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\n",
      "\n",
      "\n",
      "* Apply the theory ofconceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author.[51]For example, consider the English wordbig. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree isphysically largerelative to other trees or the authors experience.  When used metaphorically (\"Tomorrow is a big day\"), the author's intent to implyimportance.  The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.\n",
      "* Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of aprobabilistic context-free grammar(PCFG). The mathematical equation for such algorithms is presented inUS Patent 9269353:[52]\n",
      "\n",
      "\n",
      "* 1 the Road\n",
      "* Artificial intelligence detection software\n",
      "* Automated essay scoring\n",
      "* Biomedical text mining\n",
      "* Compound term processing\n",
      "* Computational linguistics\n",
      "* Computer-assisted reviewing\n",
      "* Controlled natural language\n",
      "* Deep learning\n",
      "* Deep linguistic processing\n",
      "* Distributional semantics\n",
      "* Foreign language reading aid\n",
      "* Foreign language writing aid\n",
      "* Information extraction\n",
      "* Information retrieval\n",
      "* Language and Communication Technologies\n",
      "* Language model\n",
      "* Language technology\n",
      "* Latent semantic indexing\n",
      "* Multi-agent system\n",
      "* Native-language identification\n",
      "* Natural-language programming\n",
      "* Natural-language understanding\n",
      "* Natural-language search\n",
      "* Outline of natural language processing\n",
      "* Query expansion\n",
      "* Query understanding\n",
      "* Reification (linguistics)\n",
      "* Speech processing\n",
      "* Spoken dialogue systems\n",
      "* Text-proofing\n",
      "* Text simplification\n",
      "* Transformer (machine learning model)\n",
      "* Truecasing\n",
      "* Question answering\n",
      "* Word2vec\n",
      "\n",
      "\n",
      "* ^\"NLP\".\n",
      "* ^Hutchins, J. (2005).\"The history of machine translation in a nutshell\"(PDF).[self-published source]\n",
      "* ^\"ALPAC: the (in)famous report\", John Hutchins, MT News International, no. 14, June 1996, pp. 9–12.\n",
      "* ^Crevier 1993, pp. 146–148harvnb error: no target: CITEREFCrevier1993 (help), see alsoBuchanan 2005, p. 56harvnb error: no target: CITEREFBuchanan2005 (help): \"Early programs were necessarily limited in scope by the size and speed of memory\"\n",
      "* ^Koskenniemi, Kimmo(1983),Two-level morphology: A general computational model of word-form recognition and production(PDF), Department of General Linguistics,University of Helsinki\n",
      "* ^Joshi, A. K., & Weinstein, S. (1981, August).Control of Inference: Role of Some Aspects of Discourse Structure-Centering. InIJCAI(pp. 385–387).\n",
      "* ^Guida, G.; Mauri, G. (July 1986). \"Evaluation of natural language processing systems: Issues and approaches\".Proceedings of the IEEE.74(7):1026–1035.doi:10.1109/PROC.1986.13580.ISSN1558-2256.S2CID30688575.\n",
      "* ^Chomskyan linguistics encourages the investigation of \"corner cases\" that stress the limits of its theoretical models (comparable topathologicalphenomena in mathematics), typically created usingthought experiments, rather than the systematic investigation of typical phenomena that occur in real-world data, as is the case incorpus linguistics.  The creation and use of suchcorporaof real-world data is a fundamental part of machine-learning algorithms for natural language processing.  In addition, theoretical underpinnings of Chomskyan linguistics such as the so-called \"poverty of the stimulus\" argument entail that general learning algorithms, as are typically used in machine learning, cannot be successful in language processing.  As a result, the Chomskyan paradigm discouraged the application of such models to language processing.\n",
      "* ^Bengio, Yoshua; Ducharme, Réjean; Vincent, Pascal; Janvin, Christian (March 1, 2003).\"A neural probabilistic language model\".The Journal of Machine Learning Research.3:1137–1155 – via ACM Digital Library.\n",
      "* ^Mikolov, Tomáš; Karafiát, Martin; Burget, Lukáš; Černocký, Jan; Khudanpur, Sanjeev (26 September 2010).\"Recurrent neural network based language model\"(PDF).Interspeech 2010. pp.1045–1048.doi:10.21437/Interspeech.2010-343.S2CID17048224.{{cite book}}:|journal=ignored (help)\n",
      "* ^Goldberg, Yoav (2016). \"A Primer on Neural Network Models for Natural Language Processing\".Journal of Artificial Intelligence Research.57:345–420.arXiv:1807.10854.doi:10.1613/jair.4992.S2CID8273530.\n",
      "* ^Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016).Deep Learning. MIT Press.\n",
      "* ^Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016).Exploring the Limits of Language Modeling.arXiv:1602.02410.Bibcode:2016arXiv160202410J.\n",
      "* ^Choe, Do Kook; Charniak, Eugene.\"Parsing as Language Modeling\".Emnlp 2016. Archived fromthe originalon 2018-10-23. Retrieved2018-10-22.\n",
      "* ^Vinyals, Oriol; et al. (2014).\"Grammar as a Foreign Language\"(PDF).Nips2015.arXiv:1412.7449.Bibcode:2014arXiv1412.7449V.\n",
      "* ^Turchin, Alexander; Florez Builes, Luisa F. (2021-03-19).\"Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review\".Journal of Diabetes Science and Technology.15(3):553–560.doi:10.1177/19322968211000831.ISSN1932-2968.PMC8120048.PMID33736486.\n",
      "* ^Lee, Jennifer; Yang, Samuel; Holland-Hall, Cynthia; Sezgin, Emre; Gill, Manjot; Linwood, Simon; Huang, Yungui; Hoffman, Jeffrey (2022-06-10).\"Prevalence of Sensitive Terms in Clinical Notes Using Natural Language Processing Techniques: Observational Study\".JMIR Medical Informatics.10(6): e38482.doi:10.2196/38482.ISSN2291-9694.PMC9233261.PMID35687381.\n",
      "* ^Winograd, Terry (1971).Procedures as a Representation for Data in a Computer Program for Understanding Natural Language(Thesis).\n",
      "* ^Schank, Roger C.; Abelson, Robert P. (1977).Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Hillsdale: Erlbaum.ISBN0-470-99033-3.\n",
      "* ^Mark Johnson. How the statistical revolution changes (computational) linguistics.Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics.\n",
      "* ^Philip Resnik. Four revolutions.Language Log, February 5, 2011.\n",
      "* ^Socher, Richard.\"Deep Learning For NLP-ACL 2012 Tutorial\".www.socher.org. Retrieved2020-08-17.This was an early Deep Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability. Until 2015, deep learning had evolved into the major framework of NLP. [Link is broken, tryhttp://web.stanford.edu/class/cs224n/]\n",
      "* ^Segev, Elad (2022).Semantic Network Analysis in Social Sciences. London: Routledge.ISBN9780367636524.Archivedfrom the original on 5 December 2021. Retrieved5 December2021.\n",
      "* ^Yi, Chucai;Tian, Yingli(2012), \"Assistive Text Reading from Complex Background for Blind Persons\",Camera-Based Document Analysis and Recognition, Lecture Notes in Computer Science, vol. 7139, Springer Berlin Heidelberg, pp.15–28,CiteSeerX10.1.1.668.869,doi:10.1007/978-3-642-29364-1_2,ISBN9783642293634\n",
      "* ^ab\"Natural Language Processing (NLP) - A Complete Guide\".www.deeplearning.ai. 2023-01-11. Retrieved2024-05-05.\n",
      "* ^\"What is Natural Language Processing? Intro to NLP in Machine Learning\".GyanSetu!. 2020-12-06. Retrieved2021-01-09.\n",
      "* ^Kishorjit, N.; Vidya, Raj RK.; Nirmal, Y.; Sivaji, B. (2012).\"Manipuri Morpheme Identification\"(PDF).Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP). COLING 2012, Mumbai, December 2012:95–108.{{cite journal}}:  CS1 maint: location (link)\n",
      "* ^Klein, Dan; Manning, Christopher D. (2002).\"Natural language grammar induction using a constituent-context model\"(PDF).Advances in Neural Information Processing Systems.\n",
      "* ^Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Mathé, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023).\"Precision information extraction for rare disease epidemiology at scale\".Journal of Translational Medicine.21(1): 157.doi:10.1186/s12967-023-04011-y.PMC9972634.PMID36855134.\n",
      "* ^PASCAL Recognizing Textual Entailment Challenge (RTE-7)https://tac.nist.gov//2011/RTE/\n",
      "* ^Lippi, Marco; Torroni, Paolo (2016-04-20).\"Argumentation Mining: State of the Art and Emerging Trends\".ACM Transactions on Internet Technology.16(2):1–25.doi:10.1145/2850417.hdl:11585/523460.ISSN1533-5399.S2CID9561587.\n",
      "* ^\"Argument Mining – IJCAI2016 Tutorial\".www.i3s.unice.fr. Retrieved2021-03-09.\n",
      "* ^\"NLP Approaches to Computational Argumentation – ACL 2016, Berlin\". Retrieved2021-03-09.\n",
      "* ^Administration.\"Centre for Language Technology (CLT)\".Macquarie University. Retrieved2021-01-11.\n",
      "* ^\"Shared Task: Grammatical Error Correction\".www.comp.nus.edu.sg. Retrieved2021-01-11.\n",
      "* ^\"Shared Task: Grammatical Error Correction\".www.comp.nus.edu.sg. Retrieved2021-01-11.\n",
      "* ^Duan, Yucong; Cruz, Christophe (2011).\"Formalizing Semantic of Natural Language through Conceptualization from Existence\".International Journal of Innovation, Management and Technology.2(1):37–42. Archived fromthe originalon 2011-10-09.\n",
      "* ^\"U B U W E B :: Racter\".www.ubu.com. Retrieved2020-08-17.\n",
      "* ^Writer, Beta (2019).Lithium-Ion Batteries.doi:10.1007/978-3-030-16800-1.ISBN978-3-030-16799-8.S2CID155818532.\n",
      "* ^\"Document Understanding AI on Google Cloud (Cloud Next '19) – YouTube\".www.youtube.com. 11 April 2019. Archived fromthe originalon 2021-10-30. Retrieved2021-01-11.\n",
      "* ^Robertson, Adi (2022-04-06).\"OpenAI's DALL-E AI image generator can now edit pictures, too\".The Verge. Retrieved2022-06-07.\n",
      "* ^\"The Stanford Natural Language Processing Group\".nlp.stanford.edu. Retrieved2022-06-07.\n",
      "* ^Coyne, Bob; Sproat, Richard (2001-08-01).\"WordsEye\".Proceedings of the 28th annual conference on Computer graphics and interactive techniques. SIGGRAPH '01. New York, NY, USA: Association for Computing Machinery. pp.487–496.doi:10.1145/383259.383316.ISBN978-1-58113-374-5.S2CID3842372.\n",
      "* ^\"Google announces AI advances in text-to-video, language translation, more\".VentureBeat. 2022-11-02. Retrieved2022-11-09.\n",
      "* ^Vincent, James (2022-09-29).\"Meta's new text-to-video AI generator is like DALL-E for video\".The Verge. Retrieved2022-11-09.\n",
      "* ^\"Previous shared tasks | CoNLL\".www.conll.org. Retrieved2021-01-11.\n",
      "* ^\"Cognition\".Lexico.Oxford University PressandDictionary.com. Archived fromthe originalon July 15, 2020. Retrieved6 May2020.\n",
      "* ^\"Ask the Cognitive Scientist\".American Federation of Teachers. 8 August 2014.Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind.\n",
      "* ^Robinson, Peter (2008).Handbook of Cognitive Linguistics and Second Language Acquisition. Routledge. pp.3–8.ISBN978-0-805-85352-0.\n",
      "* ^Lakoff, George (1999).Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp.569–583.ISBN978-0-465-05674-3.\n",
      "* ^Strauss, Claudia (1999).A Cognitive Theory of Cultural Meaning. Cambridge University Press. pp.156–164.ISBN978-0-521-59541-4.\n",
      "* ^US patent 9269353\n",
      "* ^\"Universal Conceptual Cognitive Annotation (UCCA)\".Universal Conceptual Cognitive Annotation (UCCA). Retrieved2021-01-11.\n",
      "* ^Rodríguez, F. C., & Mairal-Usón, R. (2016).Building an RRG computational grammar.Onomazein, (34), 86–117.\n",
      "* ^\"Fluid Construction Grammar – A fully operational processing system for construction grammars\". Retrieved2021-01-11.\n",
      "* ^\"ACL Member Portal | The Association for Computational Linguistics Member Portal\".www.aclweb.org. Retrieved2021-01-11.\n",
      "* ^\"Chunks and Rules\".W3C. Retrieved2021-01-11.\n",
      "* ^Socher, Richard; Karpathy, Andrej; Le, Quoc V.; Manning, Christopher D.; Ng, Andrew Y. (2014).\"Grounded Compositional Semantics for Finding and Describing Images with Sentences\".Transactions of the Association for Computational Linguistics.2:207–218.doi:10.1162/tacl_a_00177.S2CID2317858.\n",
      "* ^Dasgupta, Ishita; Lampinen, Andrew K.; Chan, Stephanie C. Y.; Creswell, Antonia; Kumaran, Dharshan; McClelland, James L.; Hill, Felix (2022). \"Language models show human-like content effects on reasoning, Dasgupta, Lampinen et al\".arXiv:2207.07051[cs.CL].\n",
      "* ^Friston, Karl J. (2022).Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference. The MIT Press.ISBN978-0-262-36997-8.\n",
      "\n",
      "\n",
      "* Bates, M (1995).\"Models of natural language understanding\".Proceedings of the National Academy of Sciences of the United States of America.92(22):9977–9982.Bibcode:1995PNAS...92.9977B.doi:10.1073/pnas.92.22.9977.PMC40721.PMID7479812.\n",
      "* Steven Bird, Ewan Klein, and Edward Loper (2009).Natural Language Processing with Python. O'Reilly Media.ISBN978-0-596-51649-9.\n",
      "* Kenna Hughes-Castleberry, \"A Murder Mystery Puzzle: The literary puzzleCain's Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\",Scientific American, vol. 329, no. 4 (November 2023), pp. 81–82. \"This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount ofcontextthey receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyzeancient languages. In some cases, there are few historical records on long-gonecivilizationsto serve astraining datafor such a purpose.\" (p. 82.)\n",
      "* Daniel Jurafsky and James H. Martin (2008).Speech and Language Processing, 2nd edition. Pearson Prentice Hall.ISBN978-0-13-187321-6.\n",
      "* Mohamed Zakaria Kurdi (2016).Natural Language Processing and Computational Linguistics: speech, morphology, and syntax, Volume 1. ISTE-Wiley.ISBN978-1848218482.\n",
      "* Mohamed Zakaria Kurdi (2017).Natural Language Processing and Computational Linguistics: semantics, discourse, and applications, Volume 2. ISTE-Wiley.ISBN978-1848219212.\n",
      "* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008).Introduction to Information Retrieval. Cambridge University Press.ISBN978-0-521-86571-5.Official html and pdf versions available without charge.\n",
      "* Christopher D. Manning and Hinrich Schütze (1999).Foundations of Statistical Natural Language Processing. The MIT Press.ISBN978-0-262-13360-9.\n",
      "* David M. W. Powers and Christopher C. R. Turk (1989).Machine Learning of Natural Language. Springer-Verlag.ISBN978-0-387-19557-5.\n",
      "\n",
      "\n",
      "* Media related toNatural language processingat Wikimedia Commons\n",
      "\n",
      "\n",
      "* v\n",
      "* t\n",
      "* e\n",
      "\n",
      "\n",
      "* AI-complete\n",
      "* Bag-of-words\n",
      "* n-gramBigramTrigram\n",
      "* Bigram\n",
      "* Trigram\n",
      "* Computational linguistics\n",
      "* Natural language understanding\n",
      "* Stop words\n",
      "* Text processing\n",
      "\n",
      "\n",
      "* Bigram\n",
      "* Trigram\n",
      "\n",
      "\n",
      "* Argument mining\n",
      "* Collocation extraction\n",
      "* Concept mining\n",
      "* Coreference resolution\n",
      "* Deep linguistic processing\n",
      "* Distant reading\n",
      "* Information extraction\n",
      "* Named-entity recognition\n",
      "* Ontology learning\n",
      "* ParsingSemantic parsingSyntactic parsing\n",
      "* Semantic parsing\n",
      "* Syntactic parsing\n",
      "* Part-of-speech tagging\n",
      "* Semantic analysis\n",
      "* Semantic role labeling\n",
      "* Semantic decomposition\n",
      "* Semantic similarity\n",
      "* Sentiment analysis\n",
      "\n",
      "\n",
      "* Semantic parsing\n",
      "* Syntactic parsing\n",
      "\n",
      "\n",
      "* Terminology extraction\n",
      "* Text mining\n",
      "* Textual entailment\n",
      "* Truecasing\n",
      "* Word-sense disambiguation\n",
      "* Word-sense induction\n",
      "\n",
      "\n",
      "* Compound-term processing\n",
      "* Lemmatisation\n",
      "* Lexical analysis\n",
      "* Text chunking\n",
      "* Stemming\n",
      "* Sentence segmentation\n",
      "* Word segmentation\n",
      "\n",
      "\n",
      "* Multi-document summarization\n",
      "* Sentence extraction\n",
      "* Text simplification\n",
      "\n",
      "\n",
      "* Computer-assisted\n",
      "* Example-based\n",
      "* Rule-based\n",
      "* Statistical\n",
      "* Transfer-based\n",
      "* Neural\n",
      "\n",
      "\n",
      "* BERT\n",
      "* Document-term matrix\n",
      "* Explicit semantic analysis\n",
      "* fastText\n",
      "* GloVe\n",
      "* Language model(large)\n",
      "* Latent semantic analysis\n",
      "* Seq2seq\n",
      "* Word embedding\n",
      "* Word2vec\n",
      "\n",
      "\n",
      "* Corpus linguistics\n",
      "* Lexical resource\n",
      "* Linguistic Linked Open Data\n",
      "* Machine-readable dictionary\n",
      "* Parallel text\n",
      "* PropBank\n",
      "* Semantic network\n",
      "* Simple Knowledge Organization System\n",
      "* Speech corpus\n",
      "* Text corpus\n",
      "* Thesaurus (information retrieval)\n",
      "* Treebank\n",
      "* Universal Dependencies\n",
      "\n",
      "\n",
      "* BabelNet\n",
      "* Bank of English\n",
      "* DBpedia\n",
      "* FrameNet\n",
      "* Google Ngram Viewer\n",
      "* UBY\n",
      "* WordNet\n",
      "* Wikidata\n",
      "\n",
      "\n",
      "* Speech recognition\n",
      "* Speech segmentation\n",
      "* Speech synthesis\n",
      "* Natural language generation\n",
      "* Optical character recognition\n",
      "\n",
      "\n",
      "* Document classification\n",
      "* Latent Dirichlet allocation\n",
      "* Pachinko allocation\n",
      "\n",
      "\n",
      "* Automated essay scoring\n",
      "* Concordancer\n",
      "* Grammar checker\n",
      "* Predictive text\n",
      "* Pronunciation assessment\n",
      "* Spell checker\n",
      "\n",
      "\n",
      "* Chatbot\n",
      "* Interactive fiction(c.f.Syntax guessing)\n",
      "* Question answering\n",
      "* Virtual assistant\n",
      "* Voice user interface\n",
      "\n",
      "\n",
      "* Formal semantics\n",
      "* Hallucination\n",
      "* Natural Language Toolkit\n",
      "* spaCy\n",
      "\n",
      "\n",
      "* Language\n",
      "\n",
      "\n",
      "* United States\n",
      "* Japan\n",
      "* Czech Republic\n",
      "* Israel\n",
      "\n",
      "\n",
      "* Natural language processing\n",
      "* Computational fields of study\n",
      "* Computational linguistics\n",
      "* Speech recognition\n",
      "\n",
      "\n",
      "* All accuracy disputes\n",
      "* Accuracy disputes from December 2013\n",
      "* Harv and Sfn no-target errors\n",
      "* CS1 errors: periodical ignored\n",
      "* CS1 maint: location\n",
      "* Articles with short description\n",
      "* Short description is different from Wikidata\n",
      "* Articles needing additional references from May 2024\n",
      "* All articles needing additional references\n",
      "* All articles with unsourced statements\n",
      "* Articles with unsourced statements from May 2024\n",
      "* Commons category link from Wikidata\n",
      "\n",
      "\n",
      "* This page was last edited on 14 March 2025, at 19:45(UTC).\n",
      "* Text is available under theCreative Commons Attribution-ShareAlike 4.0 License;\n",
      "additional terms may apply. By using this site, you agree to theTerms of UseandPrivacy Policy. Wikipedia® is a registered trademark of theWikimedia Foundation, Inc., a non-profit organization.\n",
      "\n",
      "\n",
      "* Privacy policy\n",
      "* About Wikipedia\n",
      "* Disclaimers\n",
      "* Contact Wikipedia\n",
      "* Code of Conduct\n",
      "* Developers\n",
      "* Statistics\n",
      "* Cookie statement\n",
      "* Mobile view\n",
      "\n",
      "\n",
      "* \n",
      "* \n",
      "\n",
      "\n",
      "**Table**\n",
      "|  | This articleneeds additional citations forverification.Please helpimprove this articlebyadding citations to reliable sources. Unsourced material may be challenged and removed.Find sources:\"Natural language processing\"–news·newspapers·books·scholar·JSTOR(May 2024)(Learn how and when to remove this message) |\n",
      "\n",
      "\n",
      "**Table**\n",
      "| Part ofa serieson |\n",
      "| Formal languages |\n",
      "| Key conceptsFormal systemAlphabetSyntaxFormal semanticsSemantics (programming languages)Formal grammarFormation ruleWell-formed formulaAutomata theoryRegular expressionProductionGround expressionAtomic formula |\n",
      "| ApplicationsFormal methodsPropositional calculusPredicate logicMathematical notationNatural language processingProgramming language theoryMathematical linguisticsComputational linguisticsSyntax analysisFormal verificationAutomated theorem proving |\n",
      "| vte |\n",
      "\n",
      "\n",
      "**Table**\n",
      "| vteNatural language processing |\n",
      "| General terms | AI-completeBag-of-wordsn-gramBigramTrigramComputational linguisticsNatural language understandingStop wordsText processing |\n",
      "| Text analysis | Argument miningCollocation extractionConcept miningCoreference resolutionDeep linguistic processingDistant readingInformation extractionNamed-entity recognitionOntology learningParsingSemantic parsingSyntactic parsingPart-of-speech taggingSemantic analysisSemantic role labelingSemantic decompositionSemantic similaritySentiment analysisTerminology extractionText miningTextual entailmentTruecasingWord-sense disambiguationWord-sense inductionText segmentationCompound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation | Text segmentation | Compound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation |\n",
      "| Text segmentation | Compound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation |\n",
      "| Automatic summarization | Multi-document summarizationSentence extractionText simplification |\n",
      "| Machine translation | Computer-assistedExample-basedRule-basedStatisticalTransfer-basedNeural |\n",
      "| Distributional semanticsmodels | BERTDocument-term matrixExplicit semantic analysisfastTextGloVeLanguage model(large)Latent semantic analysisSeq2seqWord embeddingWord2vec |\n",
      "| Language resources,datasets and corpora | Types andstandardsCorpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal DependenciesDataBabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata | Types andstandards | Corpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies | Data | BabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata |\n",
      "| Types andstandards | Corpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies |\n",
      "| Data | BabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata |\n",
      "| Automatic identificationand data capture | Speech recognitionSpeech segmentationSpeech synthesisNatural language generationOptical character recognition |\n",
      "| Topic model | Document classificationLatent Dirichlet allocationPachinko allocation |\n",
      "| Computer-assistedreviewing | Automated essay scoringConcordancerGrammar checkerPredictive textPronunciation assessmentSpell checker |\n",
      "| Natural languageuser interface | ChatbotInteractive fiction(c.f.Syntax guessing)Question answeringVirtual assistantVoice user interface |\n",
      "| Related | Formal semanticsHallucinationNatural Language ToolkitspaCy |\n",
      "\n",
      "\n",
      "**Table**\n",
      "| Text segmentation | Compound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation |\n",
      "\n",
      "\n",
      "**Table**\n",
      "| Types andstandards | Corpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies |\n",
      "| Data | BabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata |\n",
      "\n",
      "\n",
      "**Table**\n",
      "| Authority control databases: National | United StatesJapanCzech RepublicIsrael |\n",
      "\n",
      "![](/static/images/icons/wikipedia.png)\n",
      "![Wikipedia](/static/images/mobile/copyright/wikipedia-wordmark-en.svg)\n",
      "![The Free Encyclopedia](/static/images/mobile/copyright/wikipedia-tagline-en.svg)\n",
      "![](//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/60px-Question_book-new.svg.png)\n",
      "![{\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\times {\\frac {1}{2d}}\\left(\\sum _{i=-d}^{d}{((PMM(token_{N})}\\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\\right)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/43ccadd794a4b84e20d1209997a463342e0dfbfe)\n",
      "![](//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png)\n",
      "![icon](//upload.wikimedia.org/wikipedia/commons/thumb/d/de/Globe_of_letters.svg/20px-Globe_of_letters.svg.png)\n",
      "![Edit this at Wikidata](//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png)\n",
      "![](https://auth.wikimedia.org/loginwiki/wiki/Special:CentralAutoLogin/start?useformat=desktop&type=1x1&usesul3=1)\n",
      "![Wikimedia Foundation](/static/images/footer/wikimedia.svg)\n",
      "![Powered by MediaWiki](/w/resources/assets/mediawiki_compact.svg)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_and_format_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Initialize the output text\n",
    "    formatted_text = \"\"\n",
    "\n",
    "    # Title\n",
    "    title = soup.title.string.strip() if soup.title else \"No title found\"\n",
    "    formatted_text += f\"# {title}\\n\\n\"  # Title as the first heading\n",
    "\n",
    "    # Meta Description\n",
    "    meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    formatted_text += f\"**Meta Description**: {meta_desc['content'].strip() if meta_desc else 'No meta description'}\\n\\n\"\n",
    "\n",
    "    # Headings (H1 to H3)\n",
    "    headings = {\"h1\": soup.find_all(\"h1\"), \"h2\": soup.find_all(\"h2\"), \"h3\": soup.find_all(\"h3\")}\n",
    "    for level in ['h1', 'h2', 'h3']:\n",
    "        for heading in headings[level]:\n",
    "            formatted_text += f\"{'#' * (int(level[1]) + 1)} {heading.get_text(strip=True)}\\n\\n\"\n",
    "\n",
    "    # Paragraphs\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 30]\n",
    "    for p in paragraphs:\n",
    "        formatted_text += f\"{p}\\n\\n\"\n",
    "\n",
    "    # Lists (ul and ol)\n",
    "    lists = []\n",
    "    for ul in soup.find_all(['ul', 'ol']):\n",
    "        items = [li.get_text(strip=True) for li in ul.find_all(\"li\")]\n",
    "        if items:\n",
    "            formatted_text += \"\\n* \" + \"\\n* \".join(items) + \"\\n\\n\"\n",
    "\n",
    "    # Tables (text only)\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        formatted_text += \"\\n**Table**\\n\"\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cols = [col.get_text(strip=True) for col in row.find_all([\"td\", \"th\"])]\n",
    "            if cols:\n",
    "                formatted_text += \"| \" + \" | \".join(cols) + \" |\\n\"\n",
    "        formatted_text += \"\\n\"  # Blank line after each table\n",
    "\n",
    "    # Images (alt text and src)\n",
    "    images = []\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        alt = img.get(\"alt\", \"\").strip()\n",
    "        src = img.get(\"src\", \"\")\n",
    "        if alt or src:\n",
    "            images.append(f\"![{alt}]({src})\")\n",
    "\n",
    "    formatted_text += \"\\n\".join(images) + \"\\n\"\n",
    "\n",
    "    return formatted_text\n",
    "url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "formatted_text = scrape_and_format_text(url)\n",
    "\n",
    "# You can print or save the formatted text\n",
    "print(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Natural language processing - Wikipedia\",\n",
      "  \"meta_description\": \"No meta description\",\n",
      "  \"headings\": {\n",
      "    \"h1\": [\n",
      "      \"Natural language processing\"\n",
      "    ],\n",
      "    \"h2\": [\n",
      "      \"Contents\",\n",
      "      \"History\",\n",
      "      \"Approaches: Symbolic, statistical, neural networks\",\n",
      "      \"Common NLP tasks\",\n",
      "      \"General tendencies and (possible) future directions\",\n",
      "      \"See also\",\n",
      "      \"References\",\n",
      "      \"Further reading\",\n",
      "      \"External links\"\n",
      "    ],\n",
      "    \"h3\": [\n",
      "      \"Symbolic NLP (1950s \\u2013 early 1990s)\",\n",
      "      \"Statistical NLP (1990s\\u2013present)\",\n",
      "      \"Statistical approach\",\n",
      "      \"Neural networks\",\n",
      "      \"Text and speech processing\",\n",
      "      \"Morphological analysis\",\n",
      "      \"Syntactic analysis\",\n",
      "      \"Lexical semantics (of individual words in context)\",\n",
      "      \"Relational semantics (semantics of individual sentences)\",\n",
      "      \"Discourse (semantics beyond individual sentences)\",\n",
      "      \"Higher-level NLP applications\",\n",
      "      \"Cognition\"\n",
      "    ]\n",
      "  },\n",
      "  \"paragraphs\": [\n",
      "    \"Natural language processing(NLP) is a subfield ofcomputer scienceand especiallyartificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded innatural languageand is thus closely related toinformation retrieval,knowledge representationandcomputational linguistics, a subfield oflinguistics.\",\n",
      "    \"Major tasks in natural language processing arespeech recognition,text classification,natural-language understanding, andnatural-language generation.\",\n",
      "    \"Natural language processing has its roots in the 1950s.[1]Already in 1950,Alan Turingpublished an article titled \\\"Computing Machinery and Intelligence\\\" which proposed what is now called theTuring testas a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\",\n",
      "    \"The premise of symbolic NLP is well-summarized byJohn Searle'sChinese roomexperiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\",\n",
      "    \"Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction ofmachine learningalgorithms for language processing.  This was due to both the steady increase in computational power (seeMoore's law) and the gradual lessening of the dominance ofChomskyantheories of linguistics (e.g.transformational grammar), whose theoretical underpinnings discouraged the sort ofcorpus linguisticsthat underlies the machine-learning approach to language processing.[8]\",\n",
      "    \"Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19]such as by writing grammars or devising heuristic rules forstemming.\",\n",
      "    \"Machine learningapproaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\",\n",
      "    \"Rule-based systems are commonly used:\",\n",
      "    \"In the late 1980s and mid-1990s, the statistical approach ended a period ofAI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\",\n",
      "    \"The earliestdecision trees, producing systems of hardif\\u2013then rules, were still very similar to the old rule-based approaches.\\nOnly the introduction of hiddenMarkov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\",\n",
      "    \"A major drawback of statistical methods is that they require elaboratefeature engineering. Since 2015,[22]the statistical approach has been replaced by theneural networksapproach, usingsemantic networks[23]andword embeddingsto capture semantic properties of words.\",\n",
      "    \"Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore.\",\n",
      "    \"Neural machine translation, based on then-newly inventedsequence-to-sequencetransformations, made obsolete the intermediate steps, such as word alignment, previously necessary forstatistical machine translation.\",\n",
      "    \"The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\",\n",
      "    \"Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\",\n",
      "    \"Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46]\",\n",
      "    \"Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\",\n",
      "    \"Cognitionrefers to \\\"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\\\"[47]Cognitive scienceis the interdisciplinary, scientific study of the mind and its processes.[48]Cognitive linguisticsis an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49]Especially during the age ofsymbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\",\n",
      "    \"As an example,George Lakoffoffers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50]with two defining aspects:\",\n",
      "    \"Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53]functional grammar,[54]construction grammar,[55]computational psycholinguistics and cognitive neuroscience (e.g.,ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56]of theACL). More recently, ideas of cognitive NLP have been revived as an approach to achieveexplainability, e.g., under the notion of \\\"cognitive AI\\\".[57]Likewise, ideas of cognitive NLP are inherent to neural modelsmultimodalNLP (although rarely made explicit)[58]and developments inartificial intelligence, specifically tools and technologies usinglarge language modelapproaches[59]and new directions inartificial general intelligencebased on thefree energy principle[60]by British neuroscientist and theoretician at University College LondonKarl J. Friston.\"\n",
      "  ],\n",
      "  \"lists\": [\n",
      "    [\n",
      "      \"Main page\",\n",
      "      \"Contents\",\n",
      "      \"Current events\",\n",
      "      \"Random article\",\n",
      "      \"About Wikipedia\",\n",
      "      \"Contact us\"\n",
      "    ],\n",
      "    [\n",
      "      \"Help\",\n",
      "      \"Learn to edit\",\n",
      "      \"Community portal\",\n",
      "      \"Recent changes\",\n",
      "      \"Upload file\",\n",
      "      \"Special pages\"\n",
      "    ],\n",
      "    [\n",
      "      \"Donate\",\n",
      "      \"Create account\",\n",
      "      \"Log in\"\n",
      "    ],\n",
      "    [\n",
      "      \"Donate\",\n",
      "      \"Create account\",\n",
      "      \"Log in\"\n",
      "    ],\n",
      "    [\n",
      "      \"Contributions\",\n",
      "      \"Talk\"\n",
      "    ],\n",
      "    [\n",
      "      \"(Top)\",\n",
      "      \"1HistoryToggle History subsection1.1Symbolic NLP (1950s \\u2013 early 1990s)1.2Statistical NLP (1990s\\u2013present)\",\n",
      "      \"1.1Symbolic NLP (1950s \\u2013 early 1990s)\",\n",
      "      \"1.2Statistical NLP (1990s\\u2013present)\",\n",
      "      \"2Approaches: Symbolic, statistical, neural networksToggle Approaches: Symbolic, statistical, neural networks subsection2.1Statistical approach2.2Neural networks\",\n",
      "      \"2.1Statistical approach\",\n",
      "      \"2.2Neural networks\",\n",
      "      \"3Common NLP tasksToggle Common NLP tasks subsection3.1Text and speech processing3.2Morphological analysis3.3Syntactic analysis3.4Lexical semantics (of individual words in context)3.5Relational semantics (semantics of individual sentences)3.6Discourse (semantics beyond individual sentences)3.7Higher-level NLP applications\",\n",
      "      \"3.1Text and speech processing\",\n",
      "      \"3.2Morphological analysis\",\n",
      "      \"3.3Syntactic analysis\",\n",
      "      \"3.4Lexical semantics (of individual words in context)\",\n",
      "      \"3.5Relational semantics (semantics of individual sentences)\",\n",
      "      \"3.6Discourse (semantics beyond individual sentences)\",\n",
      "      \"3.7Higher-level NLP applications\",\n",
      "      \"4General tendencies and (possible) future directionsToggle General tendencies and (possible) future directions subsection4.1Cognition\",\n",
      "      \"4.1Cognition\",\n",
      "      \"5See also\",\n",
      "      \"6References\",\n",
      "      \"7Further reading\",\n",
      "      \"8External links\"\n",
      "    ],\n",
      "    [\n",
      "      \"1.1Symbolic NLP (1950s \\u2013 early 1990s)\",\n",
      "      \"1.2Statistical NLP (1990s\\u2013present)\"\n",
      "    ],\n",
      "    [\n",
      "      \"2.1Statistical approach\",\n",
      "      \"2.2Neural networks\"\n",
      "    ],\n",
      "    [\n",
      "      \"3.1Text and speech processing\",\n",
      "      \"3.2Morphological analysis\",\n",
      "      \"3.3Syntactic analysis\",\n",
      "      \"3.4Lexical semantics (of individual words in context)\",\n",
      "      \"3.5Relational semantics (semantics of individual sentences)\",\n",
      "      \"3.6Discourse (semantics beyond individual sentences)\",\n",
      "      \"3.7Higher-level NLP applications\"\n",
      "    ],\n",
      "    [\n",
      "      \"4.1Cognition\"\n",
      "    ],\n",
      "    [\n",
      "      \"Afrikaans\",\n",
      "      \"\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629\",\n",
      "      \"\\u0531\\u0580\\u0565\\u0582\\u0574\\u057f\\u0561\\u0570\\u0561\\u0575\\u0565\\u0580\\u0567\\u0576\",\n",
      "      \"Az\\u0259rbaycanca\",\n",
      "      \"\\u09ac\\u09be\\u0982\\u09b2\\u09be\",\n",
      "      \"\\u95a9\\u5357\\u8a9e / B\\u00e2n-l\\u00e2m-g\\u00fa\",\n",
      "      \"\\u0411\\u0435\\u043b\\u0430\\u0440\\u0443\\u0441\\u043a\\u0430\\u044f\",\n",
      "      \"\\u0411\\u0435\\u043b\\u0430\\u0440\\u0443\\u0441\\u043a\\u0430\\u044f (\\u0442\\u0430\\u0440\\u0430\\u0448\\u043a\\u0435\\u0432\\u0456\\u0446\\u0430)\",\n",
      "      \"\\u0411\\u044a\\u043b\\u0433\\u0430\\u0440\\u0441\\u043a\\u0438\",\n",
      "      \"Bosanski\",\n",
      "      \"Brezhoneg\",\n",
      "      \"Catal\\u00e0\",\n",
      "      \"\\u010ce\\u0161tina\",\n",
      "      \"Cymraeg\",\n",
      "      \"Dansk\",\n",
      "      \"Deutsch\",\n",
      "      \"Eesti\",\n",
      "      \"\\u0395\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac\",\n",
      "      \"Espa\\u00f1ol\",\n",
      "      \"Esperanto\",\n",
      "      \"Euskara\",\n",
      "      \"\\u0641\\u0627\\u0631\\u0633\\u06cc\",\n",
      "      \"Fran\\u00e7ais\",\n",
      "      \"Gaeilge\",\n",
      "      \"Galego\",\n",
      "      \"\\ud55c\\uad6d\\uc5b4\",\n",
      "      \"\\u0540\\u0561\\u0575\\u0565\\u0580\\u0565\\u0576\",\n",
      "      \"\\u0939\\u093f\\u0928\\u094d\\u0926\\u0940\",\n",
      "      \"Hrvatski\",\n",
      "      \"Bahasa Indonesia\",\n",
      "      \"IsiZulu\",\n",
      "      \"\\u00cdslenska\",\n",
      "      \"Italiano\",\n",
      "      \"\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea\",\n",
      "      \"\\u0c95\\u0ca8\\u0ccd\\u0ca8\\u0ca1\",\n",
      "      \"\\u10e5\\u10d0\\u10e0\\u10d7\\u10e3\\u10da\\u10d8\",\n",
      "      \"Latvie\\u0161u\",\n",
      "      \"Lietuvi\\u0173\",\n",
      "      \"\\u041c\\u0430\\u043a\\u0435\\u0434\\u043e\\u043d\\u0441\\u043a\\u0438\",\n",
      "      \"\\u092e\\u0930\\u093e\\u0920\\u0940\",\n",
      "      \"\\u0645\\u0635\\u0631\\u0649\",\n",
      "      \"\\u041c\\u043e\\u043d\\u0433\\u043e\\u043b\",\n",
      "      \"\\u1019\\u103c\\u1014\\u103a\\u1019\\u102c\\u1018\\u102c\\u101e\\u102c\",\n",
      "      \"Nederlands\",\n",
      "      \"\\u65e5\\u672c\\u8a9e\",\n",
      "      \"Norsk bokm\\u00e5l\",\n",
      "      \"\\u0b13\\u0b21\\u0b3c\\u0b3f\\u0b06\",\n",
      "      \"\\u067e\\u069a\\u062a\\u0648\",\n",
      "      \"Picard\",\n",
      "      \"Piemont\\u00e8is\",\n",
      "      \"Polski\",\n",
      "      \"Portugu\\u00eas\",\n",
      "      \"Qaraqalpaqsha\",\n",
      "      \"Rom\\u00e2n\\u0103\",\n",
      "      \"Runa Simi\",\n",
      "      \"\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439\",\n",
      "      \"Shqip\",\n",
      "      \"Simple English\",\n",
      "      \"\\u06a9\\u0648\\u0631\\u062f\\u06cc\",\n",
      "      \"\\u0421\\u0440\\u043f\\u0441\\u043a\\u0438 / srpski\",\n",
      "      \"Srpskohrvatski / \\u0441\\u0440\\u043f\\u0441\\u043a\\u043e\\u0445\\u0440\\u0432\\u0430\\u0442\\u0441\\u043a\\u0438\",\n",
      "      \"Suomi\",\n",
      "      \"\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\",\n",
      "      \"\\u0c24\\u0c46\\u0c32\\u0c41\\u0c17\\u0c41\",\n",
      "      \"\\u0e44\\u0e17\\u0e22\",\n",
      "      \"T\\u00fcrk\\u00e7e\",\n",
      "      \"\\u0423\\u043a\\u0440\\u0430\\u0457\\u043d\\u0441\\u044c\\u043a\\u0430\",\n",
      "      \"Ti\\u1ebfng Vi\\u1ec7t\",\n",
      "      \"\\u7cb5\\u8a9e\",\n",
      "      \"\\u4e2d\\u6587\"\n",
      "    ],\n",
      "    [\n",
      "      \"Article\",\n",
      "      \"Talk\"\n",
      "    ],\n",
      "    [\n",
      "      \"Read\",\n",
      "      \"Edit\",\n",
      "      \"View history\"\n",
      "    ],\n",
      "    [\n",
      "      \"Read\",\n",
      "      \"Edit\",\n",
      "      \"View history\"\n",
      "    ],\n",
      "    [\n",
      "      \"What links here\",\n",
      "      \"Related changes\",\n",
      "      \"Upload file\",\n",
      "      \"Permanent link\",\n",
      "      \"Page information\",\n",
      "      \"Cite this page\",\n",
      "      \"Get shortened URL\",\n",
      "      \"Download QR code\"\n",
      "    ],\n",
      "    [\n",
      "      \"Download as PDF\",\n",
      "      \"Printable version\"\n",
      "    ],\n",
      "    [\n",
      "      \"Wikimedia Commons\",\n",
      "      \"Wikiversity\",\n",
      "      \"Wikidata item\"\n",
      "    ],\n",
      "    [\n",
      "      \"1950s: TheGeorgetown experimentin 1954 involved fullyautomatic translationof more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2]However, real progress was much slower, and after theALPAC reportin 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[3]) until the late 1980s when the firststatistical machine translationsystems were developed.\",\n",
      "      \"1960s: Some notably successful natural language processing systems developed in the 1960s wereSHRDLU, a natural language system working in restricted \\\"blocks worlds\\\" with restricted vocabularies, andELIZA, a simulation of aRogerian psychotherapist, written byJoseph Weizenbaumbetween 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \\\"patient\\\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \\\"My head hurts\\\" with \\\"Why do you say your head hurts?\\\".Ross Quillian's successful work on natural language was demonstrated with a vocabulary of onlytwentywords, because that was all that would fit in a computer  memory at the time.[4]\"\n",
      "    ],\n",
      "    [\n",
      "      \"1970s: During the 1970s, many programmers began to write \\\"conceptualontologies\\\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the firstchatterbotswere written (e.g.,PARRY).\",\n",
      "      \"1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development ofHPSGas a computational operationalization ofgenerative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g.,Lesk algorithm), reference (e.g., within Centering Theory[6]) and other areas of natural language understanding (e.g., in theRhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots withRacterandJabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[7]\"\n",
      "    ],\n",
      "    [\n",
      "      \"1990s: Many of the notable early successes in statistical methods in NLP occurred in the field ofmachine translation, due especially to work at IBM Research, such asIBM alignment models.  These systems were able to take advantage of existing multilingualtextual corporathat had been produced by theParliament of Canadaand theEuropean Unionas a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\",\n",
      "      \"2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused onunsupervisedandsemi-supervised learningalgorithms.  Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult thansupervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of theWorld Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enoughtime complexityto be practical.\",\n",
      "      \"2003:word n-gram model, at the time the best statistical algorithm, is outperformed by amulti-layer perceptron(with a single hidden layer andcontext lengthof several words, trained on up to 14 million words, byBengioet al.)[9]\",\n",
      "      \"2010:Tom\\u00e1\\u0161 Mikolov(then a PhD student atBrno University of Technology) with co-authors applied a simplerecurrent neural networkwith a single hidden layer to language modelling,[10]and in the following years he went on to developWord2vec. In the 2010s,representation learninganddeep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12]can achieve state-of-the-art results in many natural language tasks, e.g., inlanguage modeling[13]and parsing.[14][15]This is increasingly importantin medicine and healthcare, where NLP helps analyze notes and text inelectronic health recordsthat would otherwise be inaccessible for study when seeking to improve care[16]or protect patient privacy.[17]\"\n",
      "    ],\n",
      "    [\n",
      "      \"both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.\"\n",
      "    ],\n",
      "    [\n",
      "      \"language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.\"\n",
      "    ],\n",
      "    [\n",
      "      \"the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading tointractabilityproblems.\"\n",
      "    ],\n",
      "    [\n",
      "      \"when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by theApertiumsystem,\",\n",
      "      \"for preprocessing in NLP pipelines, e.g.,tokenization, or\",\n",
      "      \"for postprocessing and transforming the output of NLP pipelines, e.g., forknowledge extractionfrom syntactic parses.\"\n",
      "    ],\n",
      "    [\n",
      "      \"Formal system\",\n",
      "      \"Alphabet\",\n",
      "      \"Syntax\",\n",
      "      \"Formal semantics\",\n",
      "      \"Semantics (programming languages)\",\n",
      "      \"Formal grammar\",\n",
      "      \"Formation rule\",\n",
      "      \"Well-formed formula\",\n",
      "      \"Automata theory\",\n",
      "      \"Regular expression\",\n",
      "      \"Production\",\n",
      "      \"Ground expression\",\n",
      "      \"Atomic formula\"\n",
      "    ],\n",
      "    [\n",
      "      \"Formal methods\",\n",
      "      \"Propositional calculus\",\n",
      "      \"Predicate logic\",\n",
      "      \"Mathematical notation\",\n",
      "      \"Natural language processing\",\n",
      "      \"Programming language theory\",\n",
      "      \"Mathematical linguistics\",\n",
      "      \"Computational linguistics\",\n",
      "      \"Syntax analysis\",\n",
      "      \"Formal verification\",\n",
      "      \"Automated theorem proving\"\n",
      "    ],\n",
      "    [\n",
      "      \"v\",\n",
      "      \"t\",\n",
      "      \"e\"\n",
      "    ],\n",
      "    [\n",
      "      \"Interest on increasingly abstract, \\\"cognitive\\\" aspects of natural language (1999\\u20132001: shallow parsing, 2002\\u201303: named entity recognition, 2006\\u201309/2017\\u201318: dependency syntax, 2004\\u201305/2008\\u201309 semantic role labelling, 2011\\u201312 coreference, 2015\\u201316: discourse parsing, 2019: semantic parsing).\",\n",
      "      \"Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\",\n",
      "      \"Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\"\n",
      "    ],\n",
      "    [\n",
      "      \"Apply the theory ofconceptual metaphor, explained by Lakoff as \\\"the understanding of one idea, in terms of another\\\" which provides an idea of the intent of the author.[51]For example, consider the English wordbig. When used in a comparison (\\\"That is a big tree\\\"), the author's intent is to imply that the tree isphysically largerelative to other trees or the authors experience.  When used metaphorically (\\\"Tomorrow is a big day\\\"), the author's intent to implyimportance.  The intent behind other usages, like in \\\"She is a big person\\\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.\",\n",
      "      \"Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of aprobabilistic context-free grammar(PCFG). The mathematical equation for such algorithms is presented inUS Patent 9269353:[52]\"\n",
      "    ],\n",
      "    [\n",
      "      \"1 the Road\",\n",
      "      \"Artificial intelligence detection software\",\n",
      "      \"Automated essay scoring\",\n",
      "      \"Biomedical text mining\",\n",
      "      \"Compound term processing\",\n",
      "      \"Computational linguistics\",\n",
      "      \"Computer-assisted reviewing\",\n",
      "      \"Controlled natural language\",\n",
      "      \"Deep learning\",\n",
      "      \"Deep linguistic processing\",\n",
      "      \"Distributional semantics\",\n",
      "      \"Foreign language reading aid\",\n",
      "      \"Foreign language writing aid\",\n",
      "      \"Information extraction\",\n",
      "      \"Information retrieval\",\n",
      "      \"Language and Communication Technologies\",\n",
      "      \"Language model\",\n",
      "      \"Language technology\",\n",
      "      \"Latent semantic indexing\",\n",
      "      \"Multi-agent system\",\n",
      "      \"Native-language identification\",\n",
      "      \"Natural-language programming\",\n",
      "      \"Natural-language understanding\",\n",
      "      \"Natural-language search\",\n",
      "      \"Outline of natural language processing\",\n",
      "      \"Query expansion\",\n",
      "      \"Query understanding\",\n",
      "      \"Reification (linguistics)\",\n",
      "      \"Speech processing\",\n",
      "      \"Spoken dialogue systems\",\n",
      "      \"Text-proofing\",\n",
      "      \"Text simplification\",\n",
      "      \"Transformer (machine learning model)\",\n",
      "      \"Truecasing\",\n",
      "      \"Question answering\",\n",
      "      \"Word2vec\"\n",
      "    ],\n",
      "    [\n",
      "      \"^\\\"NLP\\\".\",\n",
      "      \"^Hutchins, J. (2005).\\\"The history of machine translation in a nutshell\\\"(PDF).[self-published source]\",\n",
      "      \"^\\\"ALPAC: the (in)famous report\\\", John Hutchins, MT News International, no. 14, June 1996, pp. 9\\u201312.\",\n",
      "      \"^Crevier 1993, pp.\\u00a0146\\u2013148harvnb error: no target: CITEREFCrevier1993 (help), see alsoBuchanan 2005, p.\\u00a056harvnb error: no target: CITEREFBuchanan2005 (help): \\\"Early programs were necessarily limited in scope by the size and speed of memory\\\"\",\n",
      "      \"^Koskenniemi, Kimmo(1983),Two-level morphology: A general computational model of word-form recognition and production(PDF), Department of General Linguistics,University of Helsinki\",\n",
      "      \"^Joshi, A. K., & Weinstein, S. (1981, August).Control of Inference: Role of Some Aspects of Discourse Structure-Centering. InIJCAI(pp. 385\\u2013387).\",\n",
      "      \"^Guida, G.; Mauri, G. (July 1986). \\\"Evaluation of natural language processing systems: Issues and approaches\\\".Proceedings of the IEEE.74(7):1026\\u20131035.doi:10.1109/PROC.1986.13580.ISSN1558-2256.S2CID30688575.\",\n",
      "      \"^Chomskyan linguistics encourages the investigation of \\\"corner cases\\\" that stress the limits of its theoretical models (comparable topathologicalphenomena in mathematics), typically created usingthought experiments, rather than the systematic investigation of typical phenomena that occur in real-world data, as is the case incorpus linguistics.  The creation and use of suchcorporaof real-world data is a fundamental part of machine-learning algorithms for natural language processing.  In addition, theoretical underpinnings of Chomskyan linguistics such as the so-called \\\"poverty of the stimulus\\\" argument entail that general learning algorithms, as are typically used in machine learning, cannot be successful in language processing.  As a result, the Chomskyan paradigm discouraged the application of such models to language processing.\",\n",
      "      \"^Bengio, Yoshua; Ducharme, R\\u00e9jean; Vincent, Pascal; Janvin, Christian (March 1, 2003).\\\"A neural probabilistic language model\\\".The Journal of Machine Learning Research.3:1137\\u20131155 \\u2013 via ACM Digital Library.\",\n",
      "      \"^Mikolov, Tom\\u00e1\\u0161; Karafi\\u00e1t, Martin; Burget, Luk\\u00e1\\u0161; \\u010cernock\\u00fd, Jan; Khudanpur, Sanjeev (26 September 2010).\\\"Recurrent neural network based language model\\\"(PDF).Interspeech 2010. pp.1045\\u20131048.doi:10.21437/Interspeech.2010-343.S2CID17048224.{{cite book}}:|journal=ignored (help)\",\n",
      "      \"^Goldberg, Yoav (2016). \\\"A Primer on Neural Network Models for Natural Language Processing\\\".Journal of Artificial Intelligence Research.57:345\\u2013420.arXiv:1807.10854.doi:10.1613/jair.4992.S2CID8273530.\",\n",
      "      \"^Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016).Deep Learning. MIT Press.\",\n",
      "      \"^Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016).Exploring the Limits of Language Modeling.arXiv:1602.02410.Bibcode:2016arXiv160202410J.\",\n",
      "      \"^Choe, Do Kook; Charniak, Eugene.\\\"Parsing as Language Modeling\\\".Emnlp 2016. Archived fromthe originalon 2018-10-23. Retrieved2018-10-22.\",\n",
      "      \"^Vinyals, Oriol; et\\u00a0al. (2014).\\\"Grammar as a Foreign Language\\\"(PDF).Nips2015.arXiv:1412.7449.Bibcode:2014arXiv1412.7449V.\",\n",
      "      \"^Turchin, Alexander; Florez Builes, Luisa F. (2021-03-19).\\\"Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review\\\".Journal of Diabetes Science and Technology.15(3):553\\u2013560.doi:10.1177/19322968211000831.ISSN1932-2968.PMC8120048.PMID33736486.\",\n",
      "      \"^Lee, Jennifer; Yang, Samuel; Holland-Hall, Cynthia; Sezgin, Emre; Gill, Manjot; Linwood, Simon; Huang, Yungui; Hoffman, Jeffrey (2022-06-10).\\\"Prevalence of Sensitive Terms in Clinical Notes Using Natural Language Processing Techniques: Observational Study\\\".JMIR Medical Informatics.10(6): e38482.doi:10.2196/38482.ISSN2291-9694.PMC9233261.PMID35687381.\",\n",
      "      \"^Winograd, Terry (1971).Procedures as a Representation for Data in a Computer Program for Understanding Natural Language(Thesis).\",\n",
      "      \"^Schank, Roger C.; Abelson, Robert P. (1977).Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Hillsdale: Erlbaum.ISBN0-470-99033-3.\",\n",
      "      \"^Mark Johnson. How the statistical revolution changes (computational) linguistics.Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics.\",\n",
      "      \"^Philip Resnik. Four revolutions.Language Log, February 5, 2011.\",\n",
      "      \"^Socher, Richard.\\\"Deep Learning For NLP-ACL 2012 Tutorial\\\".www.socher.org. Retrieved2020-08-17.This was an early Deep Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability. Until 2015, deep learning had evolved into the major framework of NLP. [Link is broken, tryhttp://web.stanford.edu/class/cs224n/]\",\n",
      "      \"^Segev, Elad (2022).Semantic Network Analysis in Social Sciences. London: Routledge.ISBN9780367636524.Archivedfrom the original on 5 December 2021. Retrieved5 December2021.\",\n",
      "      \"^Yi, Chucai;Tian, Yingli(2012), \\\"Assistive Text Reading from Complex Background for Blind Persons\\\",Camera-Based Document Analysis and Recognition, Lecture Notes in Computer Science, vol.\\u00a07139, Springer Berlin Heidelberg, pp.15\\u201328,CiteSeerX10.1.1.668.869,doi:10.1007/978-3-642-29364-1_2,ISBN9783642293634\",\n",
      "      \"^ab\\\"Natural Language Processing (NLP) - A Complete Guide\\\".www.deeplearning.ai. 2023-01-11. Retrieved2024-05-05.\",\n",
      "      \"^\\\"What is Natural Language Processing? Intro to NLP in Machine Learning\\\".GyanSetu!. 2020-12-06. Retrieved2021-01-09.\",\n",
      "      \"^Kishorjit, N.; Vidya, Raj RK.; Nirmal, Y.; Sivaji, B. (2012).\\\"Manipuri Morpheme Identification\\\"(PDF).Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP). COLING 2012, Mumbai, December 2012:95\\u2013108.{{cite journal}}:  CS1 maint: location (link)\",\n",
      "      \"^Klein, Dan; Manning, Christopher D. (2002).\\\"Natural language grammar induction using a constituent-context model\\\"(PDF).Advances in Neural Information Processing Systems.\",\n",
      "      \"^Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Math\\u00e9, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023).\\\"Precision information extraction for rare disease epidemiology at scale\\\".Journal of Translational Medicine.21(1): 157.doi:10.1186/s12967-023-04011-y.PMC9972634.PMID36855134.\",\n",
      "      \"^PASCAL Recognizing Textual Entailment Challenge (RTE-7)https://tac.nist.gov//2011/RTE/\",\n",
      "      \"^Lippi, Marco; Torroni, Paolo (2016-04-20).\\\"Argumentation Mining: State of the Art and Emerging Trends\\\".ACM Transactions on Internet Technology.16(2):1\\u201325.doi:10.1145/2850417.hdl:11585/523460.ISSN1533-5399.S2CID9561587.\",\n",
      "      \"^\\\"Argument Mining \\u2013 IJCAI2016 Tutorial\\\".www.i3s.unice.fr. Retrieved2021-03-09.\",\n",
      "      \"^\\\"NLP Approaches to Computational Argumentation \\u2013 ACL 2016, Berlin\\\". Retrieved2021-03-09.\",\n",
      "      \"^Administration.\\\"Centre for Language Technology (CLT)\\\".Macquarie University. Retrieved2021-01-11.\",\n",
      "      \"^\\\"Shared Task: Grammatical Error Correction\\\".www.comp.nus.edu.sg. Retrieved2021-01-11.\",\n",
      "      \"^\\\"Shared Task: Grammatical Error Correction\\\".www.comp.nus.edu.sg. Retrieved2021-01-11.\",\n",
      "      \"^Duan, Yucong; Cruz, Christophe (2011).\\\"Formalizing Semantic of Natural Language through Conceptualization from Existence\\\".International Journal of Innovation, Management and Technology.2(1):37\\u201342. Archived fromthe originalon 2011-10-09.\",\n",
      "      \"^\\\"U B U W E B\\u00a0:: Racter\\\".www.ubu.com. Retrieved2020-08-17.\",\n",
      "      \"^Writer, Beta (2019).Lithium-Ion Batteries.doi:10.1007/978-3-030-16800-1.ISBN978-3-030-16799-8.S2CID155818532.\",\n",
      "      \"^\\\"Document Understanding AI on Google Cloud (Cloud Next '19) \\u2013 YouTube\\\".www.youtube.com. 11 April 2019. Archived fromthe originalon 2021-10-30. Retrieved2021-01-11.\",\n",
      "      \"^Robertson, Adi (2022-04-06).\\\"OpenAI's DALL-E AI image generator can now edit pictures, too\\\".The Verge. Retrieved2022-06-07.\",\n",
      "      \"^\\\"The Stanford Natural Language Processing Group\\\".nlp.stanford.edu. Retrieved2022-06-07.\",\n",
      "      \"^Coyne, Bob; Sproat, Richard (2001-08-01).\\\"WordsEye\\\".Proceedings of the 28th annual conference on Computer graphics and interactive techniques. SIGGRAPH '01. New York, NY, USA: Association for Computing Machinery. pp.487\\u2013496.doi:10.1145/383259.383316.ISBN978-1-58113-374-5.S2CID3842372.\",\n",
      "      \"^\\\"Google announces AI advances in text-to-video, language translation, more\\\".VentureBeat. 2022-11-02. Retrieved2022-11-09.\",\n",
      "      \"^Vincent, James (2022-09-29).\\\"Meta's new text-to-video AI generator is like DALL-E for video\\\".The Verge. Retrieved2022-11-09.\",\n",
      "      \"^\\\"Previous shared tasks | CoNLL\\\".www.conll.org. Retrieved2021-01-11.\",\n",
      "      \"^\\\"Cognition\\\".Lexico.Oxford University PressandDictionary.com. Archived fromthe originalon July 15, 2020. Retrieved6 May2020.\",\n",
      "      \"^\\\"Ask the Cognitive Scientist\\\".American Federation of Teachers. 8 August 2014.Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind.\",\n",
      "      \"^Robinson, Peter (2008).Handbook of Cognitive Linguistics and Second Language Acquisition. Routledge. pp.3\\u20138.ISBN978-0-805-85352-0.\",\n",
      "      \"^Lakoff, George (1999).Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp.569\\u2013583.ISBN978-0-465-05674-3.\",\n",
      "      \"^Strauss, Claudia (1999).A Cognitive Theory of Cultural Meaning. Cambridge University Press. pp.156\\u2013164.ISBN978-0-521-59541-4.\",\n",
      "      \"^US patent 9269353\",\n",
      "      \"^\\\"Universal Conceptual Cognitive Annotation (UCCA)\\\".Universal Conceptual Cognitive Annotation (UCCA). Retrieved2021-01-11.\",\n",
      "      \"^Rodr\\u00edguez, F. C., & Mairal-Us\\u00f3n, R. (2016).Building an RRG computational grammar.Onomazein, (34), 86\\u2013117.\",\n",
      "      \"^\\\"Fluid Construction Grammar \\u2013 A fully operational processing system for construction grammars\\\". Retrieved2021-01-11.\",\n",
      "      \"^\\\"ACL Member Portal | The Association for Computational Linguistics Member Portal\\\".www.aclweb.org. Retrieved2021-01-11.\",\n",
      "      \"^\\\"Chunks and Rules\\\".W3C. Retrieved2021-01-11.\",\n",
      "      \"^Socher, Richard; Karpathy, Andrej; Le, Quoc V.; Manning, Christopher D.; Ng, Andrew Y. (2014).\\\"Grounded Compositional Semantics for Finding and Describing Images with Sentences\\\".Transactions of the Association for Computational Linguistics.2:207\\u2013218.doi:10.1162/tacl_a_00177.S2CID2317858.\",\n",
      "      \"^Dasgupta, Ishita; Lampinen, Andrew K.; Chan, Stephanie C. Y.; Creswell, Antonia; Kumaran, Dharshan; McClelland, James L.; Hill, Felix (2022). \\\"Language models show human-like content effects on reasoning, Dasgupta, Lampinen et al\\\".arXiv:2207.07051[cs.CL].\",\n",
      "      \"^Friston, Karl J. (2022).Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference. The MIT Press.ISBN978-0-262-36997-8.\"\n",
      "    ],\n",
      "    [\n",
      "      \"Bates, M (1995).\\\"Models of natural language understanding\\\".Proceedings of the National Academy of Sciences of the United States of America.92(22):9977\\u20139982.Bibcode:1995PNAS...92.9977B.doi:10.1073/pnas.92.22.9977.PMC40721.PMID7479812.\",\n",
      "      \"Steven Bird, Ewan Klein, and Edward Loper (2009).Natural Language Processing with Python. O'Reilly Media.ISBN978-0-596-51649-9.\",\n",
      "      \"Kenna Hughes-Castleberry, \\\"A Murder Mystery Puzzle: The literary puzzleCain's Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\\\",Scientific American, vol. 329, no. 4 (November 2023), pp. 81\\u201382. \\\"This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount ofcontextthey receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyzeancient languages. In some cases, there are few historical records on long-gonecivilizationsto serve astraining datafor such a purpose.\\\" (p. 82.)\",\n",
      "      \"Daniel Jurafsky and James H. Martin (2008).Speech and Language Processing, 2nd edition. Pearson Prentice Hall.ISBN978-0-13-187321-6.\",\n",
      "      \"Mohamed Zakaria Kurdi (2016).Natural Language Processing and Computational Linguistics: speech, morphology, and syntax, Volume 1. ISTE-Wiley.ISBN978-1848218482.\",\n",
      "      \"Mohamed Zakaria Kurdi (2017).Natural Language Processing and Computational Linguistics: semantics, discourse, and applications, Volume 2. ISTE-Wiley.ISBN978-1848219212.\",\n",
      "      \"Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch\\u00fctze (2008).Introduction to Information Retrieval. Cambridge University Press.ISBN978-0-521-86571-5.Official html and pdf versions available without charge.\",\n",
      "      \"Christopher D. Manning and Hinrich Sch\\u00fctze (1999).Foundations of Statistical Natural Language Processing. The MIT Press.ISBN978-0-262-13360-9.\",\n",
      "      \"David M. W. Powers and Christopher C. R. Turk (1989).Machine Learning of Natural Language. Springer-Verlag.ISBN978-0-387-19557-5.\"\n",
      "    ],\n",
      "    [\n",
      "      \"Media related toNatural language processingat Wikimedia Commons\"\n",
      "    ],\n",
      "    [\n",
      "      \"v\",\n",
      "      \"t\",\n",
      "      \"e\"\n",
      "    ],\n",
      "    [\n",
      "      \"AI-complete\",\n",
      "      \"Bag-of-words\",\n",
      "      \"n-gramBigramTrigram\",\n",
      "      \"Bigram\",\n",
      "      \"Trigram\",\n",
      "      \"Computational linguistics\",\n",
      "      \"Natural language understanding\",\n",
      "      \"Stop words\",\n",
      "      \"Text processing\"\n",
      "    ],\n",
      "    [\n",
      "      \"Bigram\",\n",
      "      \"Trigram\"\n",
      "    ],\n",
      "    [\n",
      "      \"Argument mining\",\n",
      "      \"Collocation extraction\",\n",
      "      \"Concept mining\",\n",
      "      \"Coreference resolution\",\n",
      "      \"Deep linguistic processing\",\n",
      "      \"Distant reading\",\n",
      "      \"Information extraction\",\n",
      "      \"Named-entity recognition\",\n",
      "      \"Ontology learning\",\n",
      "      \"ParsingSemantic parsingSyntactic parsing\",\n",
      "      \"Semantic parsing\",\n",
      "      \"Syntactic parsing\",\n",
      "      \"Part-of-speech tagging\",\n",
      "      \"Semantic analysis\",\n",
      "      \"Semantic role labeling\",\n",
      "      \"Semantic decomposition\",\n",
      "      \"Semantic similarity\",\n",
      "      \"Sentiment analysis\"\n",
      "    ],\n",
      "    [\n",
      "      \"Semantic parsing\",\n",
      "      \"Syntactic parsing\"\n",
      "    ],\n",
      "    [\n",
      "      \"Terminology extraction\",\n",
      "      \"Text mining\",\n",
      "      \"Textual entailment\",\n",
      "      \"Truecasing\",\n",
      "      \"Word-sense disambiguation\",\n",
      "      \"Word-sense induction\"\n",
      "    ],\n",
      "    [\n",
      "      \"Compound-term processing\",\n",
      "      \"Lemmatisation\",\n",
      "      \"Lexical analysis\",\n",
      "      \"Text chunking\",\n",
      "      \"Stemming\",\n",
      "      \"Sentence segmentation\",\n",
      "      \"Word segmentation\"\n",
      "    ],\n",
      "    [\n",
      "      \"Multi-document summarization\",\n",
      "      \"Sentence extraction\",\n",
      "      \"Text simplification\"\n",
      "    ],\n",
      "    [\n",
      "      \"Computer-assisted\",\n",
      "      \"Example-based\",\n",
      "      \"Rule-based\",\n",
      "      \"Statistical\",\n",
      "      \"Transfer-based\",\n",
      "      \"Neural\"\n",
      "    ],\n",
      "    [\n",
      "      \"BERT\",\n",
      "      \"Document-term matrix\",\n",
      "      \"Explicit semantic analysis\",\n",
      "      \"fastText\",\n",
      "      \"GloVe\",\n",
      "      \"Language model(large)\",\n",
      "      \"Latent semantic analysis\",\n",
      "      \"Seq2seq\",\n",
      "      \"Word embedding\",\n",
      "      \"Word2vec\"\n",
      "    ],\n",
      "    [\n",
      "      \"Corpus linguistics\",\n",
      "      \"Lexical resource\",\n",
      "      \"Linguistic Linked Open Data\",\n",
      "      \"Machine-readable dictionary\",\n",
      "      \"Parallel text\",\n",
      "      \"PropBank\",\n",
      "      \"Semantic network\",\n",
      "      \"Simple Knowledge Organization System\",\n",
      "      \"Speech corpus\",\n",
      "      \"Text corpus\",\n",
      "      \"Thesaurus (information retrieval)\",\n",
      "      \"Treebank\",\n",
      "      \"Universal Dependencies\"\n",
      "    ],\n",
      "    [\n",
      "      \"BabelNet\",\n",
      "      \"Bank of English\",\n",
      "      \"DBpedia\",\n",
      "      \"FrameNet\",\n",
      "      \"Google Ngram Viewer\",\n",
      "      \"UBY\",\n",
      "      \"WordNet\",\n",
      "      \"Wikidata\"\n",
      "    ],\n",
      "    [\n",
      "      \"Speech recognition\",\n",
      "      \"Speech segmentation\",\n",
      "      \"Speech synthesis\",\n",
      "      \"Natural language generation\",\n",
      "      \"Optical character recognition\"\n",
      "    ],\n",
      "    [\n",
      "      \"Document classification\",\n",
      "      \"Latent Dirichlet allocation\",\n",
      "      \"Pachinko allocation\"\n",
      "    ],\n",
      "    [\n",
      "      \"Automated essay scoring\",\n",
      "      \"Concordancer\",\n",
      "      \"Grammar checker\",\n",
      "      \"Predictive text\",\n",
      "      \"Pronunciation assessment\",\n",
      "      \"Spell checker\"\n",
      "    ],\n",
      "    [\n",
      "      \"Chatbot\",\n",
      "      \"Interactive fiction(c.f.Syntax guessing)\",\n",
      "      \"Question answering\",\n",
      "      \"Virtual assistant\",\n",
      "      \"Voice user interface\"\n",
      "    ],\n",
      "    [\n",
      "      \"Formal semantics\",\n",
      "      \"Hallucination\",\n",
      "      \"Natural Language Toolkit\",\n",
      "      \"spaCy\"\n",
      "    ],\n",
      "    [\n",
      "      \"Language\"\n",
      "    ],\n",
      "    [\n",
      "      \"United States\",\n",
      "      \"Japan\",\n",
      "      \"Czech Republic\",\n",
      "      \"Israel\"\n",
      "    ],\n",
      "    [\n",
      "      \"Natural language processing\",\n",
      "      \"Computational fields of study\",\n",
      "      \"Computational linguistics\",\n",
      "      \"Speech recognition\"\n",
      "    ],\n",
      "    [\n",
      "      \"All accuracy disputes\",\n",
      "      \"Accuracy disputes from December 2013\",\n",
      "      \"Harv and Sfn no-target errors\",\n",
      "      \"CS1 errors: periodical ignored\",\n",
      "      \"CS1 maint: location\",\n",
      "      \"Articles with short description\",\n",
      "      \"Short description is different from Wikidata\",\n",
      "      \"Articles needing additional references from May 2024\",\n",
      "      \"All articles needing additional references\",\n",
      "      \"All articles with unsourced statements\",\n",
      "      \"Articles with unsourced statements from May 2024\",\n",
      "      \"Commons category link from Wikidata\"\n",
      "    ],\n",
      "    [\n",
      "      \"This page was last edited on 14 March 2025, at 19:45(UTC).\",\n",
      "      \"Text is available under theCreative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to theTerms of UseandPrivacy Policy. Wikipedia\\u00ae is a registered trademark of theWikimedia Foundation, Inc., a non-profit organization.\"\n",
      "    ],\n",
      "    [\n",
      "      \"Privacy policy\",\n",
      "      \"About Wikipedia\",\n",
      "      \"Disclaimers\",\n",
      "      \"Contact Wikipedia\",\n",
      "      \"Code of Conduct\",\n",
      "      \"Developers\",\n",
      "      \"Statistics\",\n",
      "      \"Cookie statement\",\n",
      "      \"Mobile view\"\n",
      "    ],\n",
      "    [\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ]\n",
      "  ],\n",
      "  \"tables\": [\n",
      "    [\n",
      "      [\n",
      "        \"\",\n",
      "        \"This articleneeds additional citations forverification.Please helpimprove this articlebyadding citations to reliable sources. Unsourced material may be challenged and removed.Find sources:\\\"Natural language processing\\\"\\u2013news\\u00b7newspapers\\u00b7books\\u00b7scholar\\u00b7JSTOR(May 2024)(Learn how and when to remove this message)\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        \"Part ofa serieson\"\n",
      "      ],\n",
      "      [\n",
      "        \"Formal languages\"\n",
      "      ],\n",
      "      [\n",
      "        \"Key conceptsFormal systemAlphabetSyntaxFormal semanticsSemantics (programming languages)Formal grammarFormation ruleWell-formed formulaAutomata theoryRegular expressionProductionGround expressionAtomic formula\"\n",
      "      ],\n",
      "      [\n",
      "        \"ApplicationsFormal methodsPropositional calculusPredicate logicMathematical notationNatural language processingProgramming language theoryMathematical linguisticsComputational linguisticsSyntax analysisFormal verificationAutomated theorem proving\"\n",
      "      ],\n",
      "      [\n",
      "        \"vte\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        \"vteNatural language processing\"\n",
      "      ],\n",
      "      [\n",
      "        \"General terms\",\n",
      "        \"AI-completeBag-of-wordsn-gramBigramTrigramComputational linguisticsNatural language understandingStop wordsText processing\"\n",
      "      ],\n",
      "      [\n",
      "        \"Text analysis\",\n",
      "        \"Argument miningCollocation extractionConcept miningCoreference resolutionDeep linguistic processingDistant readingInformation extractionNamed-entity recognitionOntology learningParsingSemantic parsingSyntactic parsingPart-of-speech taggingSemantic analysisSemantic role labelingSemantic decompositionSemantic similaritySentiment analysisTerminology extractionText miningTextual entailmentTruecasingWord-sense disambiguationWord-sense inductionText segmentationCompound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\",\n",
      "        \"Text segmentation\",\n",
      "        \"Compound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\"\n",
      "      ],\n",
      "      [\n",
      "        \"Text segmentation\",\n",
      "        \"Compound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\"\n",
      "      ],\n",
      "      [\n",
      "        \"Automatic summarization\",\n",
      "        \"Multi-document summarizationSentence extractionText simplification\"\n",
      "      ],\n",
      "      [\n",
      "        \"Machine translation\",\n",
      "        \"Computer-assistedExample-basedRule-basedStatisticalTransfer-basedNeural\"\n",
      "      ],\n",
      "      [\n",
      "        \"Distributional semanticsmodels\",\n",
      "        \"BERTDocument-term matrixExplicit semantic analysisfastTextGloVeLanguage model(large)Latent semantic analysisSeq2seqWord embeddingWord2vec\"\n",
      "      ],\n",
      "      [\n",
      "        \"Language resources,datasets and corpora\",\n",
      "        \"Types andstandardsCorpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal DependenciesDataBabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\",\n",
      "        \"Types andstandards\",\n",
      "        \"Corpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\",\n",
      "        \"Data\",\n",
      "        \"BabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\"\n",
      "      ],\n",
      "      [\n",
      "        \"Types andstandards\",\n",
      "        \"Corpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\"\n",
      "      ],\n",
      "      [\n",
      "        \"Data\",\n",
      "        \"BabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\"\n",
      "      ],\n",
      "      [\n",
      "        \"Automatic identificationand data capture\",\n",
      "        \"Speech recognitionSpeech segmentationSpeech synthesisNatural language generationOptical character recognition\"\n",
      "      ],\n",
      "      [\n",
      "        \"Topic model\",\n",
      "        \"Document classificationLatent Dirichlet allocationPachinko allocation\"\n",
      "      ],\n",
      "      [\n",
      "        \"Computer-assistedreviewing\",\n",
      "        \"Automated essay scoringConcordancerGrammar checkerPredictive textPronunciation assessmentSpell checker\"\n",
      "      ],\n",
      "      [\n",
      "        \"Natural languageuser interface\",\n",
      "        \"ChatbotInteractive fiction(c.f.Syntax guessing)Question answeringVirtual assistantVoice user interface\"\n",
      "      ],\n",
      "      [\n",
      "        \"Related\",\n",
      "        \"Formal semanticsHallucinationNatural Language ToolkitspaCy\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        \"Text segmentation\",\n",
      "        \"Compound-term processingLemmatisationLexical analysisText chunkingStemmingSentence segmentationWord segmentation\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        \"Types andstandards\",\n",
      "        \"Corpus linguisticsLexical resourceLinguistic Linked Open DataMachine-readable dictionaryParallel textPropBankSemantic networkSimple Knowledge Organization SystemSpeech corpusText corpusThesaurus (information retrieval)TreebankUniversal Dependencies\"\n",
      "      ],\n",
      "      [\n",
      "        \"Data\",\n",
      "        \"BabelNetBank of EnglishDBpediaFrameNetGoogle Ngram ViewerUBYWordNetWikidata\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      [\n",
      "        \"Authority control databases: National\",\n",
      "        \"United StatesJapanCzech RepublicIsrael\"\n",
      "      ]\n",
      "    ]\n",
      "  ],\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"alt\": \"\",\n",
      "      \"src\": \"/static/images/icons/wikipedia.png\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"Wikipedia\",\n",
      "      \"src\": \"/static/images/mobile/copyright/wikipedia-wordmark-en.svg\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"The Free Encyclopedia\",\n",
      "      \"src\": \"/static/images/mobile/copyright/wikipedia-tagline-en.svg\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"\",\n",
      "      \"src\": \"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/60px-Question_book-new.svg.png\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"{\\\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\\\times {\\\\frac {1}{2d}}\\\\left(\\\\sum _{i=-d}^{d}{((PMM(token_{N})}\\\\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\\\\right)}\",\n",
      "      \"src\": \"https://wikimedia.org/api/rest_v1/media/math/render/svg/43ccadd794a4b84e20d1209997a463342e0dfbfe\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"\",\n",
      "      \"src\": \"//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"icon\",\n",
      "      \"src\": \"//upload.wikimedia.org/wikipedia/commons/thumb/d/de/Globe_of_letters.svg/20px-Globe_of_letters.svg.png\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"Edit this at Wikidata\",\n",
      "      \"src\": \"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"\",\n",
      "      \"src\": \"https://auth.wikimedia.org/loginwiki/wiki/Special:CentralAutoLogin/start?useformat=desktop&type=1x1&usesul3=1\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"Wikimedia Foundation\",\n",
      "      \"src\": \"/static/images/footer/wikimedia.svg\"\n",
      "    },\n",
      "    {\n",
      "      \"alt\": \"Powered by MediaWiki\",\n",
      "      \"src\": \"/w/resources/assets/mediawiki_compact.svg\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# search_engine.py for agent/src/xenq_agent/components/web_query/search_engine.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_important_elements(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    scraped_data = {}\n",
    "\n",
    "    # Title\n",
    "    title = soup.title.string.strip() if soup.title else \"No title found\"\n",
    "    scraped_data[\"title\"] = title\n",
    "\n",
    "    # Meta Description\n",
    "    meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    scraped_data[\"meta_description\"] = meta_desc[\"content\"].strip() if meta_desc else \"No meta description\"\n",
    "\n",
    "    # Headings (H1 to H3)\n",
    "    headings = {\n",
    "        \"h1\": [h.get_text(strip=True) for h in soup.find_all(\"h1\")],\n",
    "        \"h2\": [h.get_text(strip=True) for h in soup.find_all(\"h2\")],\n",
    "        \"h3\": [h.get_text(strip=True) for h in soup.find_all(\"h3\")]\n",
    "    }\n",
    "    scraped_data[\"headings\"] = headings\n",
    "\n",
    "    # Paragraphs\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 30]\n",
    "    scraped_data[\"paragraphs\"] = paragraphs\n",
    "\n",
    "    # Lists (ul and ol)\n",
    "    lists = []\n",
    "    for ul in soup.find_all(['ul', 'ol']):\n",
    "        items = [li.get_text(strip=True) for li in ul.find_all(\"li\")]\n",
    "        if items:\n",
    "            lists.append(items)\n",
    "    scraped_data[\"lists\"] = lists\n",
    "\n",
    "    # Tables (text only)\n",
    "    tables = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        rows = []\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cols = [col.get_text(strip=True) for col in row.find_all([\"td\", \"th\"])]\n",
    "            if cols:\n",
    "                rows.append(cols)\n",
    "        if rows:\n",
    "            tables.append(rows)\n",
    "    scraped_data[\"tables\"] = tables\n",
    "\n",
    "    # Images (alt text and src)\n",
    "    images = []\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        alt = img.get(\"alt\", \"\").strip()\n",
    "        src = img.get(\"src\", \"\")\n",
    "        if alt or src:\n",
    "            images.append({\"alt\": alt, \"src\": src})\n",
    "    scraped_data[\"images\"] = images\n",
    "\n",
    "    return scraped_data\n",
    "url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "result = scrape_important_elements(url)\n",
    "\n",
    "import json\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Simulate the LLM output with a single function invocation or multiple function invocations\n",
    "llm_output = '''\n",
    "{\n",
    "  \"function_call\": {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"parameters\": {\n",
    "      \"location\": \"Paris\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Alternatively, simulate multiple function calls:\n",
    "# llm_output = '''\n",
    "# {\n",
    "#   \"function_calls\": [\n",
    "#     {\n",
    "#       \"name\": \"get_weather\",\n",
    "#       \"parameters\": {\n",
    "#         \"location\": \"Paris\"\n",
    "#       }\n",
    "#     },\n",
    "#     {\n",
    "#       \"name\": \"search_web\",\n",
    "#       \"parameters\": {\n",
    "#         \"query\": \"latest news\"\n",
    "#       }\n",
    "#     }\n",
    "#   ]\n",
    "# }\n",
    "# '''\n",
    "\n",
    "# Step 1: Parse the JSON\n",
    "try:\n",
    "    data = json.loads(llm_output)\n",
    "\n",
    "    # Step 2: Check if it's a single function call or multiple function calls\n",
    "    if \"function_call\" in data:\n",
    "        print(\"Single function invocation detected.\")\n",
    "        function_call = data[\"function_call\"]\n",
    "        print(f\"Function Name: {function_call['name']}\")\n",
    "        print(f\"Parameters: {function_call['parameters']}\")\n",
    "\n",
    "    elif \"function_calls\" in data:\n",
    "        print(\"Multiple function invocations detected.\")\n",
    "        function_calls = data[\"function_calls\"]\n",
    "        for call in function_calls:\n",
    "            print(f\"Function Name: {call['name']}\")\n",
    "            print(f\"Parameters: {call['parameters']}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No valid function invocation found.\")\n",
    "        \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_weather', 'description': 'Fetches real-time weather for a specified location.', 'parameters': {'location': {'type': 'string', 'description': 'City or region name.'}}, 'returns': 'Weather details including temperature and conditions.'}, {'name': 'search_web', 'description': 'Searches the web for a given query.', 'parameters': {'query': {'type': 'string', 'description': 'Search phrase.'}}, 'returns': 'Search results with summaries and links.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Simulate the LLM output with a JSON array\n",
    "llm_output = '''\n",
    "Here is some text before the JSON array.\n",
    "{\n",
    "\"tools\":[\n",
    "  {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Fetches real-time weather for a specified location.\",\n",
    "    \"parameters\": {\n",
    "      \"location\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"City or region name.\"\n",
    "      }\n",
    "    },\n",
    "    \"returns\": \"Weather details including temperature and conditions.\"\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"search_web\",\n",
    "    \"description\": \"Searches the web for a given query.\",\n",
    "    \"parameters\": {\n",
    "      \"query\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Search phrase.\"\n",
    "      }\n",
    "    },\n",
    "    \"returns\": \"Search results with summaries and links.\"\n",
    "  }\n",
    "]}\n",
    "More text after the JSON array.\n",
    "'''\n",
    "\n",
    "# Step 1: Extract the JSON array using regex\n",
    "json_block = re.search(r'\\[.*\\]', llm_output, re.DOTALL)  # Regex to match JSON array\n",
    "if json_block:\n",
    "    json_str = json_block.group(0)  # Extracted JSON string\n",
    "\n",
    "    # Step 2: Convert the JSON string to a Python list (array)\n",
    "    try:\n",
    "        data_array = json.loads(json_str)\n",
    "        print(data_array)  # This is now a Python list\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "else:\n",
    "    json_block = re.search(r'\\{.*\\}', llm_output, re.DOTALL)  # Regex to match JSON block\n",
    "    if json_block:\n",
    "        json_str = json_block.group(0)  # Extracted JSON string\n",
    "\n",
    "        # Step 2: Convert the JSON string to a Python dictionary\n",
    "        try:\n",
    "            data_dict = json.loads(json_str)\n",
    "            print(data_dict)  # This is now a Python dictionary\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "    else:\n",
    "        print(\"No JSON block found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xenq_server.components.query.history_store import HistoryStore\n",
    "\n",
    "hist = HistoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'content': 'Hello what is your name.',\n",
       "  'cum_word_len': 5},\n",
       " {'role': 'user', 'content': 'Hello what is your name.', 'cum_word_len': 10},\n",
       " {'role': 'assistant',\n",
       "  'content': 'network neural accuracy learning accuracy software technology compute innovation recognize model language predict training training input software output performance technology compute output learning innovation hardware accuracy compute machine pattern network future model modeling algorithm language science compute hardware input data predict recognize neural network output training output compute neural data compute learning predict modeling future recognize technology science predict future processing technology language input data science generate recognize compute recognize pattern modeling machine pattern predict generate analyze recognize pattern software technology network future network science model technology future innovation software science network compute model neural algorithm model language output hardware input machine future accuracy accuracy predict training future system performance future machine science predict AI data language recognize neural innovation analyze algorithm output network AI technology learning machine predict future algorithm network language compute predict technology compute generate science modeling future hardware AI AI data intelligence output system recognize processing processing machine network compute AI data language compute innovation analyze pattern processing training generate model language future modeling processing learning processing innovation compute input processing software machine science learning processing language predict algorithm predict language recognize model technology language performance language learning algorithm science recognize training machine AI learning technology data accuracy intelligence software learning data performance pattern hardware input software innovation language training technology input accuracy machine machine system performance analyze accuracy learning network AI network training language accuracy pattern AI system accuracy input AI recognize network system recognize recognize software language compute performance generate pattern analyze science innovation future language learning system generate performance processing predict technology software modeling AI future system technology input AI predict software output recognize AI future AI compute neural analyze model system processing generate machine neural input processing recognize system neural science accuracy hardware modeling pattern algorithm performance generate innovation training intelligence machine hardware language pattern network analyze neural intelligence network hardware predict learning accuracy data predict future machine AI algorithm hardware science intelligence analyze training hardware compute recognize language intelligence generate analyze software processing innovation language recognize training training predict system neural neural modeling machine algorithm generate science science science accuracy data network model intelligence data software analyze intelligence learning language accuracy system output algorithm language training language training technology machine AI algorithm recognize machine hardware innovation compute language training language machine future innovation algorithm innovation modeling hardware technology science technology neural predict technology learning software training',\n",
       "  'cum_word_len': 405}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>Your next-gen AI assistant, built to understand, generate, and evolve with every query. Ask smart. Get smarter.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Hello what is your name.<|eot_id|><|start_header_id|>user<|end_header_id|>Hello what is your name.<|eot_id|><|start_header_id|>assistant<|end_header_id|>network neural accuracy learning accuracy software technology compute innovation recognize model language predict training training input software output performance technology compute output learning innovation hardware accuracy compute machine pattern network future model modeling algorithm language science compute hardware input data predict recognize neural network output training output compute neural data compute learning predict modeling future recognize technology science predict future processing technology language input data science generate recognize compute recognize pattern modeling machine pattern predict generate analyze recognize pattern software technology network future network science model technology future innovation software science network compute model neural algorithm model language output hardware input machine future accuracy accuracy predict training future system performance future machine science predict AI data language recognize neural innovation analyze algorithm output network AI technology learning machine predict future algorithm network language compute predict technology compute generate science modeling future hardware AI AI data intelligence output system recognize processing processing machine network compute AI data language compute innovation analyze pattern processing training generate model language future modeling processing learning processing innovation compute input processing software machine science learning processing language predict algorithm predict language recognize model technology language performance language learning algorithm science recognize training machine AI learning technology data accuracy intelligence software learning data performance pattern hardware input software innovation language training technology input accuracy machine machine system performance analyze accuracy learning network AI network training language accuracy pattern AI system accuracy input AI recognize network system recognize recognize software language compute performance generate pattern analyze science innovation future language learning system generate performance processing predict technology software modeling AI future system technology input AI predict software output recognize AI future AI compute neural analyze model system processing generate machine neural input processing recognize system neural science accuracy hardware modeling pattern algorithm performance generate innovation training intelligence machine hardware language pattern network analyze neural intelligence network hardware predict learning accuracy data predict future machine AI algorithm hardware science intelligence analyze training hardware compute recognize language intelligence generate analyze software processing innovation language recognize training training predict system neural neural modeling machine algorithm generate science science science accuracy data network model intelligence data software analyze intelligence learning language accuracy system output algorithm language training language training technology machine AI algorithm recognize machine hardware innovation compute language training language machine future innovation algorithm innovation modeling hardware technology science technology neural predict technology learning software training<|eot_id|><|start_header_id|>assistant<|end_header_id|>Hello what is your name.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Hello what is your name.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.append_content(\"assistant\",\"Hello what is your name.\")\n",
    "hist.update_system_msg(\"\")\n",
    "hist.build_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network neural accuracy learning accuracy software technology compute innovation recognize model language predict training training input software output performance technology compute output learning innovation hardware accuracy compute machine pattern network future model modeling algorithm language science compute hardware input data predict recognize neural network output training output compute neural data compute learning predict modeling future recognize technology science predict future processing technology language input data science generate recognize compute recognize pattern modeling machine pattern predict generate analyze recognize pattern software technology network future network science model technology future innovation software science network compute model neural algorithm model language output hardware input machine future accuracy accuracy predict training future system performance future machine science predict AI data language recognize neural innovation analyz...\n",
      "\n",
      "Total word count: 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'network neural accuracy learning accuracy software technology compute innovation recognize model language predict training training input software output performance technology compute output learning innovation hardware accuracy compute machine pattern network future model modeling algorithm language science compute hardware input data predict recognize neural network output training output compute neural data compute learning predict modeling future recognize technology science predict future processing technology language input data science generate recognize compute recognize pattern modeling machine pattern predict generate analyze recognize pattern software technology network future network science model technology future innovation software science network compute model neural algorithm model language output hardware input machine future accuracy accuracy predict training future system performance future machine science predict AI data language recognize neural innovation analyze algorithm output network AI technology learning machine predict future algorithm network language compute predict technology compute generate science modeling future hardware AI AI data intelligence output system recognize processing processing machine network compute AI data language compute innovation analyze pattern processing training generate model language future modeling processing learning processing innovation compute input processing software machine science learning processing language predict algorithm predict language recognize model technology language performance language learning algorithm science recognize training machine AI learning technology data accuracy intelligence software learning data performance pattern hardware input software innovation language training technology input accuracy machine machine system performance analyze accuracy learning network AI network training language accuracy pattern AI system accuracy input AI recognize network system recognize recognize software language compute performance generate pattern analyze science innovation future language learning system generate performance processing predict technology software modeling AI future system technology input AI predict software output recognize AI future AI compute neural analyze model system processing generate machine neural input processing recognize system neural science accuracy hardware modeling pattern algorithm performance generate innovation training intelligence machine hardware language pattern network analyze neural intelligence network hardware predict learning accuracy data predict future machine AI algorithm hardware science intelligence analyze training hardware compute recognize language intelligence generate analyze software processing innovation language recognize training training predict system neural neural modeling machine algorithm generate science science science accuracy data network model intelligence data software analyze intelligence learning language accuracy system output algorithm language training language training technology machine AI algorithm recognize machine hardware innovation compute language training language machine future innovation algorithm innovation modeling hardware technology science technology neural predict technology learning software training processing intelligence intelligence output processing future recognize generate accuracy software training hardware predict language training software training recognize technology AI science innovation algorithm software neural data learning analyze input analyze performance data recognize neural technology output system recognize learning technology performance network processing machine modeling output pattern accuracy technology model science network language training hardware training intelligence model hardware machine technology intelligence machine performance analyze pattern neural software output learning software training future software machine predict output training innovation AI data science hardware system future intelligence generate recognize analyze future algorithm pattern intelligence generate system neural hardware software model analyze processing language AI science algorithm performance recognize output generate science modeling output hardware innovation processing pattern machine data predict output data recognize analyze future future science predict innovation future science data machine network training generate science data predict training pattern innovation model innovation innovation technology intelligence input hardware pattern innovation language machine language data AI innovation model network AI hardware science data output AI innovation recognize output performance neural output input compute technology network learning hardware machine training language training modeling generate processing innovation pattern output technology model language future model learning technology input compute predict modeling learning intelligence pattern neural predict generate input pattern generate input software future data technology performance pattern future hardware learning compute neural innovation AI hardware science machine performance output network algorithm learning recognize hardware language accuracy output software machine model predict performance system intelligence model technology software network network generate learning model analyze network pattern future intelligence pattern technology intelligence accuracy training modeling training network recognize system network algorithm predict science accuracy pattern recognize predict model future compute learning recognize neural language network input neural data software hardware input modeling output input modeling network hardware output science performance science processing algorithm learning learning technology language network neural predict intelligence modeling intelligence AI AI pattern generate language performance intelligence model learning generate data algorithm science technology training AI neural modeling output innovation output pattern technology machine learning intelligence compute future network neural accuracy neural intelligence future innovation innovation science training input machine software recognize science generate hardware recognize intelligence model intelligence software machine performance network pattern neural input science innovation algorithm AI modeling future compute network input accuracy training model network input generate compute pattern training science intelligence intelligence future neural learning hardware neural language intelligence analyze hardware learning processing input output model machine technology machine machine pattern processing learning modeling technology machine output predict language technology science analyze AI language network pattern system recognize model language future innovation future machine training system neural predict processing recognize future analyze input software predict intelligence input AI algorithm algorithm accuracy science model training AI technology analyze neural compute future predict system learning data output software data data future output system processing predict system software data technology learning innovation network neural future system generate pattern output network system input pattern AI processing recognize model science network data neural compute future software AI neural predict machine hardware performance data neural predict innovation input predict innovation input machine neural pattern processing processing output predict performance intelligence processing accuracy output intelligence analyze learning modeling system analyze future innovation training model model algorithm pattern system compute processing input pattern system output modeling innovation system input algorithm pattern modeling network accuracy accuracy software neural predict input model AI data modeling future model predict generate machine predict language analyze modeling software system model accuracy processing analyze science output learning predict performance network modeling science predict future future generate generate data analyze technology language generate input science algorithm accuracy intelligence AI processing data AI technology neural generate neural language processing processing innovation intelligence machine modeling modeling neural performance modeling network algorithm learning learning technology intelligence model AI modeling network neural intelligence accuracy language science science algorithm predict neural algorithm learning compute pattern processing hardware model performance output AI future software performance processing modeling network AI pattern predict future science AI model language modeling future science system performance training pattern predict neural compute innovation training AI model input neural network predict generate neural technology algorithm innovation processing pattern science technology analyze recognize predict predict recognize input compute pattern output AI algorithm algorithm language processing algorithm performance innovation model software neural system modeling intelligence predict algorithm science innovation accuracy hardware neural innovation software neural innovation system software compute technology intelligence modeling machine recognize neural modeling output AI input neural innovation neural network data compute innovation language model input compute training output generate AI recognize generate predict input recognize system science innovation future input neural network training performance machine processing innovation training training analyze compute learning input algorithm model compute analyze language learning innovation neural recognize learning accuracy training algorithm intelligence processing data processing compute generate output neural performance system algorithm pattern hardware neural network neural compute predict accuracy training performance learning data neural training learning innovation language compute compute predict output neural intelligence output model network modeling algorithm machine generate language technology pattern model compute science hardware accuracy neural system modeling pattern language network innovation science analyze processing compute machine analyze data analyze input machine language science innovation intelligence compute pattern learning system software learning input input predict output software AI hardware output modeling future neural innovation data AI compute language technology modeling learning system science data predict input recognize data hardware pattern machine pattern neural machine output innovation neural machine output accuracy modeling intelligence compute language technology modeling input recognize output system neural modeling machine algorithm data algorithm technology AI science recognize intelligence compute hardware algorithm compute generate accuracy hardware learning hardware innovation algorithm science accuracy predict science hardware AI model algorithm compute future software neural processing software training processing processing performance model processing software pattern algorithm training predict future neural compute performance input science machine pattern processing intelligence pattern input accuracy language hardware future learning compute network recognize future innovation modeling pattern network network accuracy intelligence network language generate generate future innovation compute predict learning neural recognize hardware accuracy output innovation intelligence recognize pattern modeling training compute future recognize modeling accuracy software hardware language neural input algorithm model hardware input data pattern data analyze modeling analyze modeling accuracy neural training input machine software model pattern AI pattern accuracy network hardware generate data compute software pattern innovation training analyze language machine machine pattern intelligence training intelligence modeling network processing science generate pattern analyze technology output training performance future network compute algorithm neural training output intelligence intelligence training innovation science technology modeling modeling technology analyze science network model predict predict language training software software system pattern hardware input AI training network modeling neural intelligence processing intelligence neural algorithm accuracy modeling innovation predict input hardware neural learning input technology software performance output processing generate language hardware learning model processing network modeling system system science processing technology generate processing training performance compute data predict network modeling AI model predict compute system learning AI AI neural algorithm network analyze system software generate hardware technology accuracy predict science future network AI input science model language pattern machine input hardware hardware neural learning model neural intelligence neural network data analyze training compute predict processing performance modeling hardware science performance future future AI recognize output network generate pattern recognize network machine output technology processing innovation machine generate training data future neural recognize innovation model future output input recognize innovation compute accuracy recognize pattern innovation performance analyze algorithm language software compute hardware software intelligence AI learning science generate technology recognize accuracy input data accuracy algorithm system processing science science training performance innovation system machine learning intelligence analyze compute training innovation training AI innovation data model recognize machine algorithm generate software learning pattern analyze predict generate compute model processing predict processing science future software performance accuracy processing AI recognize system algorithm software pattern performance generate output analyze accuracy intelligence technology intelligence input learning data analyze future modeling generate neural neural machine accuracy hardware performance neural neural future compute recognize generate recognize intelligence hardware pattern algorithm input network processing machine science machine data data AI input performance language compute AI future algorithm future neural intelligence science AI accuracy innovation learning performance generate AI analyze compute modeling accuracy software analyze machine software processing generate hardware intelligence modeling machine output learning training learning algorithm learning system AI AI learning compute pattern intelligence algorithm input recognize software accuracy model AI network compute machine input neural language modeling intelligence science machine innovation hardware processing input innovation science input software technology recognize hardware intelligence output pattern pattern network learning learning intelligence intelligence system network software compute performance input model machine intelligence science language learning modeling science future science predict modeling software learning data machine learning data modeling hardware processing science language recognize language technology technology network pattern algorithm intelligence learning intelligence science input accuracy system predict accuracy AI model input performance recognize language software generate neural AI machine processing software AI technology recognize neural compute software future machine input model technology innovation machine training hardware processing hardware science machine data science algorithm neural system training future performance accuracy analyze output future learning performance recognize modeling predict innovation system intelligence software performance predict recognize input pattern technology network performance software hardware pattern recognize software training learning language accuracy technology processing processing performance analyze accuracy training innovation data network accuracy pattern neural predict system generate accuracy predict compute compute data system predict output training intelligence science model hardware data science algorithm machine processing algorithm network training future analyze modeling learning training analyze output predict training recognize pattern generate recognize neural future input algorithm technology input analyze data learning neural intelligence compute input language pattern output technology technology compute hardware science AI intelligence algorithm system system machine modeling hardware'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample word list (you can extend this with your own vocabulary)\n",
    "word_list = [\n",
    "    \"technology\", \"innovation\", \"future\", \"AI\", \"machine\", \"learning\", \"data\", \"science\",\n",
    "    \"algorithm\", \"model\", \"intelligence\", \"neural\", \"network\", \"system\", \"performance\",\n",
    "    \"accuracy\", \"training\", \"input\", \"output\", \"processing\", \"hardware\", \"software\",\n",
    "    \"language\", \"modeling\", \"predict\", \"analyze\", \"compute\", \"generate\", \"pattern\", \"recognize\"\n",
    "]\n",
    "\n",
    "# Generate ~2000 words\n",
    "word_count = 2000\n",
    "generated_paragraph = ' '.join(random.choices(word_list, k=word_count))\n",
    "\n",
    "# Print or save the result\n",
    "print(generated_paragraph[:1000] + '...')  # print the first 1000 characters\n",
    "print(f\"\\nTotal word count: {len(generated_paragraph.split())}\")\n",
    "generated_paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Database connection details\n",
    "host = \"localhost\"       # or \"127.0.0.1\"\n",
    "port = 5432              # Port you exposed in Docker\n",
    "database = \"college\"\n",
    "user = \"admin\"\n",
    "password = \"admin123\"\n",
    "\n",
    "try:\n",
    "    # Establish connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        user=user,\n",
    "        password=password\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # SQL query\n",
    "    sql_query = \"SELECT * FROM employees LIMIT 5;\"\n",
    "    cursor.execute(sql_query)\n",
    "\n",
    "    # Fetch and print results\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "finally:\n",
    "    if conn:\n",
    "        cursor.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Database connection details\n",
    "host = \"localhost\"\n",
    "port = 5432\n",
    "database = \"college\"\n",
    "user = \"admin\"\n",
    "password = \"admin123\"\n",
    "\n",
    "try:\n",
    "\n",
    "    conn = psycopg2.connect(\"postgresql://admin:admin123@localhost:5432/college\")\n",
    "    cursor = conn.cursor()\n",
    "    # Continue with rest of the code...\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error type: InFailedSqlTransaction\n",
      "Error message: current transaction is aborted, commands ignored until end of transaction block\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "try:\n",
    "    sql_query = \"SELECT * FROM employees1 LIMIT 5;\"\n",
    "    cursor.execute(sql_query)\n",
    "\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    from tabulate import tabulate\n",
    "    formatted_output = tabulate(rows, headers=column_names, tablefmt=\"github\")\n",
    "    print(formatted_output.strip())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    print(f\"Error message: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|   emp_no | birth_date   | first_name   | last_name   | gender   | hire_date   |\\n|----------|--------------|--------------|-------------|----------|-------------|\\n|    10001 | 1953-09-02   | Georgi       | Facello     | M        | 1986-06-26  |\\n|    10002 | 1964-06-02   | Bezalel      | Simmel      | F        | 1985-11-21  |\\n|    10003 | 1959-12-03   | Parto        | Bamford     | M        | 1986-08-28  |\\n|    10004 | 1954-05-01   | Chirstian    | Koblick     | M        | 1986-12-01  |\\n|    10005 | 1955-01-21   | Kyoichi      | Maliniak    | M        | 1989-09-12  |'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "| emp_no | birth_date   | first_name   | last_name   | gender   | hire_date   |\n",
    "|--------|--------------|--------------|-------------|----------|-------------|\n",
    "| 10001  | 1953-09-02   | Georgi       | Facello     | M        | 1986-06-26  |\n",
    "| 10002  | 1964-06-02   | Bezalel      | Simmel      | F        | 1985-11-21  |\n",
    "| 10003  | 1959-12-03   | Parto        | Bamford     | M        | 1986-08-28  |\n",
    "| 10004  | 1954-05-01   | Chirstian    | Koblick     | M        | 1986-12-01  |\n",
    "| 10005  | 1955-01-21   | Kyoichi      | Maliniak    | M        | 1989-09-12  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urlparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pg_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgresql://admin:admin123@localhost:5432/college\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Parse the URL\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m parsed_url \u001b[38;5;241m=\u001b[39m \u001b[43murlparse\u001b[49m(pg_url)\n\u001b[1;32m      6\u001b[0m user \u001b[38;5;241m=\u001b[39m parsed_url\u001b[38;5;241m.\u001b[39musername\n\u001b[1;32m      7\u001b[0m password \u001b[38;5;241m=\u001b[39m parsed_url\u001b[38;5;241m.\u001b[39mpassword\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urlparse' is not defined"
     ]
    }
   ],
   "source": [
    "# Your PostgreSQL URL (example format, replace with your actual one)\n",
    "pg_url = \"postgresql://admin:admin123@localhost:5432/college\"\n",
    "\n",
    "# Parse the URL\n",
    "parsed_url = urlparse(pg_url)\n",
    "user = parsed_url.username\n",
    "password = parsed_url.password\n",
    "host = parsed_url.hostname\n",
    "port = parsed_url.port\n",
    "database = parsed_url.path[1:]  # remove leading '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Tables in the database:\n",
      "- departments\n",
      "- dept_emp\n",
      "- employees\n",
      "- dept_manager\n",
      "- salaries\n",
      "- titles\n",
      "\n",
      "📌 Columns and Data Types for each table:\n",
      "\n",
      "Table: departments\n",
      "  ▸ dept_no - character - Nullable: NO\n",
      "  ▸ dept_name - character varying - Nullable: NO\n",
      "\n",
      "Table: dept_emp\n",
      "  ▸ emp_no - integer - Nullable: NO\n",
      "  ▸ from_date - date - Nullable: NO\n",
      "  ▸ to_date - date - Nullable: NO\n",
      "  ▸ dept_no - character - Nullable: NO\n",
      "\n",
      "Table: employees\n",
      "  ▸ emp_no - integer - Nullable: NO\n",
      "  ▸ birth_date - date - Nullable: NO\n",
      "  ▸ gender - USER-DEFINED - Nullable: YES\n",
      "  ▸ hire_date - date - Nullable: NO\n",
      "  ▸ first_name - character varying - Nullable: NO\n",
      "  ▸ last_name - character varying - Nullable: NO\n",
      "\n",
      "Table: dept_manager\n",
      "  ▸ emp_no - integer - Nullable: NO\n",
      "  ▸ from_date - date - Nullable: NO\n",
      "  ▸ to_date - date - Nullable: NO\n",
      "  ▸ dept_no - character - Nullable: NO\n",
      "\n",
      "Table: salaries\n",
      "  ▸ emp_no - integer - Nullable: NO\n",
      "  ▸ salary - integer - Nullable: NO\n",
      "  ▸ from_date - date - Nullable: NO\n",
      "  ▸ to_date - date - Nullable: NO\n",
      "\n",
      "Table: titles\n",
      "  ▸ emp_no - integer - Nullable: NO\n",
      "  ▸ from_date - date - Nullable: NO\n",
      "  ▸ to_date - date - Nullable: YES\n",
      "  ▸ title - character varying - Nullable: NO\n",
      "\n",
      "🔑 Primary Keys:\n",
      "  ▸ Table: pg_proc, Column: oid\n",
      "  ▸ Table: pg_type, Column: oid\n",
      "  ▸ Table: pg_attribute, Column: attrelid\n",
      "  ▸ Table: pg_attribute, Column: attnum\n",
      "  ▸ Table: pg_class, Column: oid\n",
      "  ▸ Table: pg_attrdef, Column: oid\n",
      "  ▸ Table: pg_constraint, Column: oid\n",
      "  ▸ Table: pg_inherits, Column: inhrelid\n",
      "  ▸ Table: pg_inherits, Column: inhseqno\n",
      "  ▸ Table: pg_index, Column: indexrelid\n",
      "  ▸ Table: pg_operator, Column: oid\n",
      "  ▸ Table: pg_opfamily, Column: oid\n",
      "  ▸ Table: pg_opclass, Column: oid\n",
      "  ▸ Table: pg_am, Column: oid\n",
      "  ▸ Table: pg_amop, Column: oid\n",
      "  ▸ Table: pg_amproc, Column: oid\n",
      "  ▸ Table: pg_language, Column: oid\n",
      "  ▸ Table: pg_largeobject_metadata, Column: oid\n",
      "  ▸ Table: pg_largeobject, Column: loid\n",
      "  ▸ Table: pg_largeobject, Column: pageno\n",
      "  ▸ Table: pg_aggregate, Column: aggfnoid\n",
      "  ▸ Table: pg_statistic, Column: starelid\n",
      "  ▸ Table: pg_statistic, Column: staattnum\n",
      "  ▸ Table: pg_statistic, Column: stainherit\n",
      "  ▸ Table: pg_statistic_ext, Column: oid\n",
      "  ▸ Table: pg_statistic_ext_data, Column: stxoid\n",
      "  ▸ Table: pg_statistic_ext_data, Column: stxdinherit\n",
      "  ▸ Table: pg_rewrite, Column: oid\n",
      "  ▸ Table: pg_trigger, Column: oid\n",
      "  ▸ Table: pg_event_trigger, Column: oid\n",
      "  ▸ Table: pg_description, Column: objoid\n",
      "  ▸ Table: pg_description, Column: classoid\n",
      "  ▸ Table: pg_description, Column: objsubid\n",
      "  ▸ Table: pg_cast, Column: oid\n",
      "  ▸ Table: pg_enum, Column: oid\n",
      "  ▸ Table: pg_namespace, Column: oid\n",
      "  ▸ Table: pg_conversion, Column: oid\n",
      "  ▸ Table: pg_database, Column: oid\n",
      "  ▸ Table: pg_db_role_setting, Column: setdatabase\n",
      "  ▸ Table: pg_db_role_setting, Column: setrole\n",
      "  ▸ Table: pg_tablespace, Column: oid\n",
      "  ▸ Table: pg_authid, Column: oid\n",
      "  ▸ Table: pg_auth_members, Column: roleid\n",
      "  ▸ Table: pg_auth_members, Column: member\n",
      "  ▸ Table: pg_shdescription, Column: objoid\n",
      "  ▸ Table: pg_shdescription, Column: classoid\n",
      "  ▸ Table: pg_ts_config, Column: oid\n",
      "  ▸ Table: pg_ts_config_map, Column: mapcfg\n",
      "  ▸ Table: pg_ts_config_map, Column: maptokentype\n",
      "  ▸ Table: pg_ts_config_map, Column: mapseqno\n",
      "  ▸ Table: pg_ts_dict, Column: oid\n",
      "  ▸ Table: pg_ts_parser, Column: oid\n",
      "  ▸ Table: pg_ts_template, Column: oid\n",
      "  ▸ Table: pg_extension, Column: oid\n",
      "  ▸ Table: pg_foreign_data_wrapper, Column: oid\n",
      "  ▸ Table: pg_foreign_server, Column: oid\n",
      "  ▸ Table: pg_user_mapping, Column: oid\n",
      "  ▸ Table: pg_foreign_table, Column: ftrelid\n",
      "  ▸ Table: pg_policy, Column: oid\n",
      "  ▸ Table: pg_replication_origin, Column: roident\n",
      "  ▸ Table: pg_default_acl, Column: oid\n",
      "  ▸ Table: pg_init_privs, Column: objoid\n",
      "  ▸ Table: pg_init_privs, Column: classoid\n",
      "  ▸ Table: pg_init_privs, Column: objsubid\n",
      "  ▸ Table: pg_seclabel, Column: objoid\n",
      "  ▸ Table: pg_seclabel, Column: classoid\n",
      "  ▸ Table: pg_seclabel, Column: objsubid\n",
      "  ▸ Table: pg_seclabel, Column: provider\n",
      "  ▸ Table: pg_shseclabel, Column: objoid\n",
      "  ▸ Table: pg_shseclabel, Column: classoid\n",
      "  ▸ Table: pg_shseclabel, Column: provider\n",
      "  ▸ Table: pg_collation, Column: oid\n",
      "  ▸ Table: pg_parameter_acl, Column: oid\n",
      "  ▸ Table: pg_partitioned_table, Column: partrelid\n",
      "  ▸ Table: pg_range, Column: rngtypid\n",
      "  ▸ Table: pg_transform, Column: oid\n",
      "  ▸ Table: pg_sequence, Column: seqrelid\n",
      "  ▸ Table: pg_publication, Column: oid\n",
      "  ▸ Table: pg_publication_namespace, Column: oid\n",
      "  ▸ Table: pg_publication_rel, Column: oid\n",
      "  ▸ Table: pg_subscription, Column: oid\n",
      "  ▸ Table: pg_subscription_rel, Column: srrelid\n",
      "  ▸ Table: pg_subscription_rel, Column: srsubid\n",
      "  ▸ Table: departments, Column: dept_no\n",
      "  ▸ Table: dept_emp, Column: emp_no\n",
      "  ▸ Table: dept_emp, Column: dept_no\n",
      "  ▸ Table: dept_manager, Column: emp_no\n",
      "  ▸ Table: dept_manager, Column: dept_no\n",
      "  ▸ Table: employees, Column: emp_no\n",
      "  ▸ Table: salaries, Column: emp_no\n",
      "  ▸ Table: salaries, Column: from_date\n",
      "  ▸ Table: titles, Column: emp_no\n",
      "  ▸ Table: titles, Column: title\n",
      "  ▸ Table: titles, Column: from_date\n",
      "\n",
      "🔗 Foreign Keys:\n",
      "  ▸ dept_emp.dept_no → departments.dept_no\n",
      "  ▸ dept_emp.emp_no → employees.emp_no\n",
      "  ▸ dept_manager.dept_no → departments.dept_no\n",
      "  ▸ dept_manager.emp_no → employees.emp_no\n",
      "  ▸ salaries.emp_no → employees.emp_no\n",
      "  ▸ titles.emp_no → employees.emp_no\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connect using your DB details\n",
    "conn = psycopg2.connect(\"postgresql://admin:admin123@localhost:5432/college\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 📌 Get all tables\n",
    "print(\"📁 Tables in the database:\")\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'public';\n",
    "\"\"\")\n",
    "tables = cursor.fetchall()\n",
    "for t in tables:\n",
    "    print(f\"- {t[0]}\")\n",
    "\n",
    "print(\"\\n📌 Columns and Data Types for each table:\")\n",
    "for table in tables:\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT column_name, data_type, is_nullable\n",
    "        FROM information_schema.columns \n",
    "        WHERE table_name = %s;\n",
    "    \"\"\", (table[0],))\n",
    "    print(f\"\\nTable: {table[0]}\")\n",
    "    for col in cursor.fetchall():\n",
    "        print(f\"  ▸ {col[0]} - {col[1]} - Nullable: {col[2]}\")\n",
    "\n",
    "# 🔐 Primary Keys\n",
    "print(\"\\n🔑 Primary Keys:\")\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT\n",
    "        kcu.table_name,\n",
    "        tco.constraint_name,\n",
    "        kcu.column_name\n",
    "    FROM information_schema.table_constraints tco\n",
    "    JOIN information_schema.key_column_usage kcu \n",
    "      ON kcu.constraint_name = tco.constraint_name\n",
    "    WHERE tco.constraint_type = 'PRIMARY KEY';\n",
    "\"\"\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  ▸ Table: {row[0]}, Column: {row[2]}\")\n",
    "\n",
    "# 🔗 Foreign Keys\n",
    "print(\"\\n🔗 Foreign Keys:\")\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        tc.table_name, \n",
    "        kcu.column_name, \n",
    "        ccu.table_name AS foreign_table_name,\n",
    "        ccu.column_name AS foreign_column_name\n",
    "    FROM \n",
    "        information_schema.table_constraints AS tc \n",
    "        JOIN information_schema.key_column_usage AS kcu\n",
    "          ON tc.constraint_name = kcu.constraint_name\n",
    "        JOIN information_schema.constraint_column_usage AS ccu\n",
    "          ON ccu.constraint_name = tc.constraint_name\n",
    "    WHERE constraint_type = 'FOREIGN KEY';\n",
    "\"\"\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  ▸ {row[0]}.{row[1]} → {row[2]}.{row[3]}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tables\": {\n",
      "    \"departments\": {\n",
      "      \"columns\": {\n",
      "        \"dept_no\": \"character NOT NULL\",\n",
      "        \"dept_name\": \"character varying NOT NULL\"\n",
      "      },\n",
      "      \"primary_key\": [\n",
      "        \"dept_no\"\n",
      "      ],\n",
      "      \"foreign_keys\": {}\n",
      "    },\n",
      "    \"dept_emp\": {\n",
      "      \"columns\": {\n",
      "        \"emp_no\": \"integer NOT NULL\",\n",
      "        \"from_date\": \"date NOT NULL\",\n",
      "        \"to_date\": \"date NOT NULL\",\n",
      "        \"dept_no\": \"character NOT NULL\"\n",
      "      },\n",
      "      \"primary_key\": [\n",
      "        \"emp_no\",\n",
      "        \"dept_no\"\n",
      "      ],\n",
      "      \"foreign_keys\": {\n",
      "        \"dept_no\": [\n",
      "          \"departments\",\n",
      "          \"dept_no\"\n",
      "        ],\n",
      "        \"emp_no\": [\n",
      "          \"employees\",\n",
      "          \"emp_no\"\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"employees\": {\n",
      "      \"columns\": {\n",
      "        \"emp_no\": \"integer NOT NULL\",\n",
      "        \"birth_date\": \"date NOT NULL\",\n",
      "        \"gender\": \"USER-DEFINED NULL\",\n",
      "        \"hire_date\": \"date NOT NULL\",\n",
      "        \"first_name\": \"character varying NOT NULL\",\n",
      "        \"last_name\": \"character varying NOT NULL\"\n",
      "      },\n",
      "      \"primary_key\": [\n",
      "        \"emp_no\"\n",
      "      ],\n",
      "      \"foreign_keys\": {}\n",
      "    },\n",
      "    \"dept_manager\": {\n",
      "      \"columns\": {\n",
      "        \"emp_no\": \"integer NOT NULL\",\n",
      "        \"from_date\": \"date NOT NULL\",\n",
      "        \"to_date\": \"date NOT NULL\",\n",
      "        \"dept_no\": \"character NOT NULL\"\n",
      "      },\n",
      "      \"primary_key\": [\n",
      "        \"emp_no\",\n",
      "        \"dept_no\"\n",
      "      ],\n",
      "      \"foreign_keys\": {\n",
      "        \"dept_no\": [\n",
      "          \"departments\",\n",
      "          \"dept_no\"\n",
      "        ],\n",
      "        \"emp_no\": [\n",
      "          \"employees\",\n",
      "          \"emp_no\"\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"salaries\": {\n",
      "      \"columns\": {\n",
      "        \"emp_no\": \"integer NOT NULL\",\n",
      "        \"salary\": \"integer NOT NULL\",\n",
      "        \"from_date\": \"date NOT NULL\",\n",
      "        \"to_date\": \"date NOT NULL\"\n",
      "      },\n",
      "      \"primary_key\": [\n",
      "        \"emp_no\",\n",
      "        \"from_date\"\n",
      "      ],\n",
      "      \"foreign_keys\": {\n",
      "        \"emp_no\": [\n",
      "          \"employees\",\n",
      "          \"emp_no\"\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    \"titles\": {\n",
      "      \"columns\": {\n",
      "        \"emp_no\": \"integer NOT NULL\",\n",
      "        \"from_date\": \"date NOT NULL\",\n",
      "        \"to_date\": \"date NULL\",\n",
      "        \"title\": \"character varying NOT NULL\"\n",
      "      },\n",
      "      \"primary_key\": [\n",
      "        \"emp_no\",\n",
      "        \"title\",\n",
      "        \"from_date\"\n",
      "      ],\n",
      "      \"foreign_keys\": {\n",
      "        \"emp_no\": [\n",
      "          \"employees\",\n",
      "          \"emp_no\"\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "\n",
    "def parse_postgres_url(db_url):\n",
    "    parsed = urlparse(db_url)\n",
    "    return {\n",
    "        \"dbname\": parsed.path[1:],\n",
    "        \"user\": parsed.username,\n",
    "        \"password\": parsed.password,\n",
    "        \"host\": parsed.hostname,\n",
    "        \"port\": parsed.port\n",
    "    }\n",
    "\n",
    "def extract_schema(db_url):\n",
    "    db_conf = parse_postgres_url(db_url)\n",
    "    conn = psycopg2.connect(**db_conf)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get all tables\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = 'public';\n",
    "    \"\"\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    schema = {\"tables\": {}}\n",
    "\n",
    "    for table in tables:\n",
    "        schema[\"tables\"][table] = {\"columns\": {}, \"primary_key\": [], \"foreign_keys\": {}}\n",
    "\n",
    "        # Get columns\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = %s;\n",
    "        \"\"\", (table,))\n",
    "        for col, dtype, nullable in cursor.fetchall():\n",
    "            nullable_str = \"NULL\" if nullable == \"YES\" else \"NOT NULL\"\n",
    "            schema[\"tables\"][table][\"columns\"][col] = f\"{dtype} {nullable_str}\"\n",
    "\n",
    "        # Primary keys\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT kcu.column_name\n",
    "            FROM information_schema.table_constraints tco\n",
    "            JOIN information_schema.key_column_usage kcu \n",
    "              ON tco.constraint_name = kcu.constraint_name\n",
    "            WHERE tco.table_name = %s AND tco.constraint_type = 'PRIMARY KEY';\n",
    "        \"\"\", (table,))\n",
    "        schema[\"tables\"][table][\"primary_key\"] = [r[0] for r in cursor.fetchall()]\n",
    "\n",
    "        # Foreign keys\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT kcu.column_name, ccu.table_name, ccu.column_name\n",
    "            FROM information_schema.table_constraints AS tc\n",
    "            JOIN information_schema.key_column_usage AS kcu\n",
    "              ON tc.constraint_name = kcu.constraint_name\n",
    "            JOIN information_schema.constraint_column_usage AS ccu\n",
    "              ON ccu.constraint_name = tc.constraint_name\n",
    "            WHERE tc.constraint_type = 'FOREIGN KEY' AND tc.table_name = %s;\n",
    "        \"\"\", (table,))\n",
    "        for col, ref_table, ref_col in cursor.fetchall():\n",
    "            schema[\"tables\"][table][\"foreign_keys\"][col] = [ref_table, ref_col]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return schema\n",
    "\n",
    "# ✅ Example usage\n",
    "url = \"postgresql://admin:admin123@localhost:5432/college\"\n",
    "schema_summary = extract_schema(url)\n",
    "print(json.dumps(schema_summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "\n",
    "def get_enhanced_schema(uri: str):\n",
    "    conn = psycopg2.connect(uri)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Tables\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = 'public' AND table_type = 'BASE TABLE';\n",
    "    \"\"\")\n",
    "    tables = [r[0] for r in cur.fetchall()]\n",
    "\n",
    "    # Columns with datatype, nullability, default\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT table_name, column_name, data_type, is_nullable, column_default\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public';\n",
    "    \"\"\")\n",
    "    columns_info = {}\n",
    "    for table, col, dtype, nullable, default in cur.fetchall():\n",
    "        columns_info.setdefault(table, []).append({\n",
    "            \"name\": col,\n",
    "            \"type\": dtype,\n",
    "            \"nullable\": nullable == \"YES\",\n",
    "            \"default\": default\n",
    "        })\n",
    "\n",
    "    # Primary Keys\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT kcu.table_name, kcu.column_name\n",
    "        FROM information_schema.table_constraints tc\n",
    "        JOIN information_schema.key_column_usage kcu\n",
    "        ON tc.constraint_name = kcu.constraint_name\n",
    "        WHERE tc.constraint_type = 'PRIMARY KEY';\n",
    "    \"\"\")\n",
    "    pk_map = {}\n",
    "    for table, col in cur.fetchall():\n",
    "        pk_map.setdefault(table, set()).add(col)\n",
    "\n",
    "    # Foreign Keys\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT\n",
    "            kcu.table_name, kcu.column_name,\n",
    "            ccu.table_name AS foreign_table, ccu.column_name AS foreign_column\n",
    "        FROM information_schema.table_constraints AS tc\n",
    "        JOIN information_schema.key_column_usage AS kcu\n",
    "            ON tc.constraint_name = kcu.constraint_name\n",
    "        JOIN information_schema.constraint_column_usage AS ccu\n",
    "            ON ccu.constraint_name = tc.constraint_name\n",
    "        WHERE tc.constraint_type = 'FOREIGN KEY';\n",
    "    \"\"\")\n",
    "    fk_map = {}\n",
    "    for table, col, f_table, f_col in cur.fetchall():\n",
    "        fk_map.setdefault(table, {}).setdefault(col, f\"{f_table}.{f_col}\")\n",
    "\n",
    "    # Indexes (optional for query planning)\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT\n",
    "            t.relname AS table_name,\n",
    "            a.attname AS column_name\n",
    "        FROM\n",
    "            pg_class t,\n",
    "            pg_class i,\n",
    "            pg_index ix,\n",
    "            pg_attribute a\n",
    "        WHERE\n",
    "            t.oid = ix.indrelid\n",
    "            AND i.oid = ix.indexrelid\n",
    "            AND a.attrelid = t.oid\n",
    "            AND a.attnum = ANY(ix.indkey)\n",
    "            AND t.relkind = 'r';\n",
    "    \"\"\")\n",
    "    indexed_map = {}\n",
    "    for table, col in cur.fetchall():\n",
    "        indexed_map.setdefault(table, set()).add(col)\n",
    "\n",
    "    # Row counts per table\n",
    "    row_count = {}\n",
    "    for table in tables:\n",
    "        try:\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM public.\\\"{table}\\\";\")\n",
    "            row_count[table] = cur.fetchone()[0]\n",
    "        except:\n",
    "            row_count[table] = \"?\"\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    # Build final schema structure\n",
    "    schema = {}\n",
    "    for table in tables:\n",
    "        schema[table] = {\n",
    "            \"row_count\": row_count[table],\n",
    "            \"columns\": []\n",
    "        }\n",
    "        for col_info in columns_info.get(table, []):\n",
    "            col_name = col_info[\"name\"]\n",
    "            schema[table][\"columns\"].append({\n",
    "                \"name\": col_name,\n",
    "                \"type\": col_info[\"type\"],\n",
    "                \"nullable\": col_info[\"nullable\"],\n",
    "                \"default\": col_info[\"default\"],\n",
    "                \"primary_key\": col_name in pk_map.get(table, set()),\n",
    "                \"foreign_key\": fk_map.get(table, {}).get(col_name),\n",
    "                \"indexed\": col_name in indexed_map.get(table, set())\n",
    "            })\n",
    "\n",
    "    return schema\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    uri = \"postgresql://admin:admin123@localhost:5432/college\"\n",
    "    import json\n",
    "    print(json.dumps(get_enhanced_schema(uri), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### departments (9 rows)\n",
      "- dept_no: character (PK), NOT NULL\n",
      "- dept_name: character varying, NOT NULL\n",
      "\n",
      "### dept_emp (331041 rows)\n",
      "- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\n",
      "- from_date: date, NOT NULL\n",
      "- to_date: date, NOT NULL\n",
      "- dept_no: character (PK) (FK → departments.dept_no), NOT NULL\n",
      "\n",
      "### employees (299509 rows)\n",
      "- emp_no: integer (PK), NOT NULL\n",
      "- birth_date: date, NOT NULL\n",
      "- gender: USER-DEFINED\n",
      "- hire_date: date, NOT NULL\n",
      "- first_name: character varying, NOT NULL\n",
      "- last_name: character varying, NOT NULL\n",
      "\n",
      "### dept_manager (24 rows)\n",
      "- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\n",
      "- from_date: date, NOT NULL\n",
      "- to_date: date, NOT NULL\n",
      "- dept_no: character (PK) (FK → departments.dept_no), NOT NULL\n",
      "\n",
      "### salaries (2839079 rows)\n",
      "- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\n",
      "- salary: integer, NOT NULL\n",
      "- from_date: date (PK), NOT NULL\n",
      "- to_date: date, NOT NULL\n",
      "\n",
      "### titles (442547 rows)\n",
      "- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\n",
      "- from_date: date (PK), NOT NULL\n",
      "- to_date: date\n",
      "- title: character varying (PK), NOT NULL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from urllib.parse import urlparse, unquote\n",
    "\n",
    "\n",
    "def parse_postgres_uri(uri):\n",
    "    result = urlparse(uri)\n",
    "    return {\n",
    "        \"dbname\": result.path.lstrip(\"/\"),\n",
    "        \"user\": result.username,\n",
    "        \"password\": result.password,\n",
    "        \"host\": result.hostname,\n",
    "        \"port\": result.port or 5432,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_llm_friendly_schema(uri):\n",
    "    config = parse_postgres_uri(uri)\n",
    "    conn = psycopg2.connect(**config)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema='public' AND table_type='BASE TABLE'\n",
    "    \"\"\")\n",
    "    tables = [r[0] for r in cur.fetchall()]\n",
    "\n",
    "    schema_output = []\n",
    "\n",
    "    for table in tables:\n",
    "        # Get row count\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        row_count = cur.fetchone()[0]\n",
    "\n",
    "        # Get columns\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = %s\n",
    "        \"\"\", (table,))\n",
    "        columns = cur.fetchall()\n",
    "\n",
    "        # Get primary keys\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT kcu.column_name\n",
    "            FROM information_schema.table_constraints tc\n",
    "            JOIN information_schema.key_column_usage kcu\n",
    "            ON tc.constraint_name = kcu.constraint_name\n",
    "            WHERE tc.table_name = %s AND tc.constraint_type = 'PRIMARY KEY'\n",
    "        \"\"\", (table,))\n",
    "        pk_columns = {r[0] for r in cur.fetchall()}\n",
    "\n",
    "        # Get foreign keys\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT\n",
    "                kcu.column_name,\n",
    "                ccu.table_name AS foreign_table,\n",
    "                ccu.column_name AS foreign_column\n",
    "            FROM\n",
    "                information_schema.table_constraints tc\n",
    "            JOIN information_schema.key_column_usage kcu\n",
    "                ON tc.constraint_name = kcu.constraint_name\n",
    "            JOIN information_schema.constraint_column_usage ccu\n",
    "                ON ccu.constraint_name = tc.constraint_name\n",
    "            WHERE\n",
    "                tc.constraint_type = 'FOREIGN KEY' AND\n",
    "                tc.table_name = %s\n",
    "        \"\"\", (table,))\n",
    "        fk_map = {(r[0]): f\"{r[1]}.{r[2]}\" for r in cur.fetchall()}\n",
    "\n",
    "        # Format output\n",
    "        schema_output.append(f\"### {table} ({row_count} rows)\")\n",
    "        for col, dtype, nullable in columns:\n",
    "            line = f\"- {col}: {dtype}\"\n",
    "            if col in pk_columns:\n",
    "                line += \" (PK)\"\n",
    "            if col in fk_map:\n",
    "                line += f\" (FK → {fk_map[col]})\"\n",
    "            if nullable == \"NO\":\n",
    "                line += \", NOT NULL\"\n",
    "            schema_output.append(line)\n",
    "        schema_output.append(\"\")  # blank line for spacing\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return \"\\n\".join(schema_output)\n",
    "\n",
    "\n",
    "# 🧪 Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    uri = \"postgresql://admin:admin123@localhost:5432/college\"\n",
    "    print(get_llm_friendly_schema(uri))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xenq_server.components.query.query_manager import QueryManager\n",
    "uri = \"postgresql://admin:admin123@localhost:5432/college\"\n",
    "postgres = QueryManager(uri)\n",
    "postgres.conn_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=\"Which manager has served the longest duration in each department, and what was their full name and duration in years?\"\n",
    "q2=\"List all employees who held the same title for more than 10 continuous years. Include emp_no, full name, title, and number of years.\"\n",
    "q3=\"Which departments have more than 70% of employees of one gender (current employees only)? Show dept_name, gender, and percentage.\"\n",
    "q4=\"List the top 3 departments with the highest average salary (among currently employed people) in the last 5 years.\"\n",
    "q4=\"Which students had the same class for their entire career? List their std_no and full name.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a professional PostgreSQL expert who generates optimized, correct SQL queries from natural language questions based on the schema provided. \n",
      "Think step by step to break down the user's request, reason about how to join and filter relevant tables, and form an efficient SQL query. \n",
      "Use Common Table Expressions (CTEs) when needed for readability and performance. Always use explicit JOINs.\n",
      "\n",
      "Schema Format:\n",
      "- TableName (Row count)\n",
      "  - column_name: data_type [PK|FK → referenced_table.column], [NOT NULL]\n",
      "\n",
      "Instructions:\n",
      "- Think aloud step by step and genertate.\n",
      "- Explain any assumptions briefly.\n",
      "- At the end, return the one final SQL query inside a single ```sql code block.\n",
      "- Do not include markdown outside the code block.\n",
      "- If the query includes unknown terms or entities not found in the schema, respond only with:\n",
      "\n",
      "```sql\n",
      "SELECT 'Unable to generate query: entity not found in schema.' AS message;\n",
      "```\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "## Schema\n",
      "### departments (9 rows)\n",
      "- dept_no: character (PK), NOT NULL\n",
      "- dept_name: character varying, NOT NULL\n",
      "\n",
      "### dept_emp (331041 rows)\n",
      "- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\n",
      "- from_date: date, NOT NULL\n",
      "- to_date: date, NOT NULL\n",
      "- dept_no: character (PK) (FK → departments.dept_no), NOT NULL\n",
      "\n",
      "### employees (299509 rows)\n",
      "- emp_no: integer (PK), NOT NULL\n",
      "- birth_date: date, NOT NULL\n",
      "- gender: USER-DEFINED\n",
      "- hire_date: date, NOT NULL\n",
      "- first_name: character varying, NOT NULL\n",
      "- last_name: character varying, NOT NULL\n",
      "\n",
      "### dept_manager (24 rows)\n",
      "- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\n",
      "- from_date: date, NOT NULL\n",
      "- to_date: date, NOT NULL\n",
      "- dept_no: character (PK) (FK → departments.dept_no), NOT NULL\n",
      "\n",
      "### salaries (2839079 rows)\n",
      "- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\n",
      "- salary: integer, NOT NULL\n",
      "- from_date: date (PK), NOT NULL\n",
      "- to_date: date, NOT NULL\n",
      "\n",
      "### titles (442547 rows)\n",
      "- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\n",
      "- from_date: date (PK), NOT NULL\n",
      "- to_date: date\n",
      "- title: character varying (PK), NOT NULL\n",
      "\n",
      "\n",
      "## Query Request\n",
      "Which departments have more than 70% of employees of one gender (current employees only)? Show dept_name, gender, and percentage.\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a professional PostgreSQL expert who generates optimized, correct SQL queries from natural language questions based on the schema provided. \\nThink step by step to break down the user's request, reason about how to join and filter relevant tables, and form an efficient SQL query. \\nUse Common Table Expressions (CTEs) when needed for readability and performance. Always use explicit JOINs.\\n\\nSchema Format:\\n- TableName (Row count)\\n  - column_name: data_type [PK|FK → referenced_table.column], [NOT NULL]\\n\\nInstructions:\\n- Think aloud step by step and genertate.\\n- Explain any assumptions briefly.\\n- At the end, return the one final SQL query inside a single ```sql code block.\\n- Do not include markdown outside the code block.\\n- If the query includes unknown terms or entities not found in the schema, respond only with:\\n\\n```sql\\nSELECT 'Unable to generate query: entity not found in schema.' AS message;\\n```\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n## Schema\\n### departments (9 rows)\\n- dept_no: character (PK), NOT NULL\\n- dept_name: character varying, NOT NULL\\n\\n### dept_emp (331041 rows)\\n- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\\n- from_date: date, NOT NULL\\n- to_date: date, NOT NULL\\n- dept_no: character (PK) (FK → departments.dept_no), NOT NULL\\n\\n### employees (299509 rows)\\n- emp_no: integer (PK), NOT NULL\\n- birth_date: date, NOT NULL\\n- gender: USER-DEFINED\\n- hire_date: date, NOT NULL\\n- first_name: character varying, NOT NULL\\n- last_name: character varying, NOT NULL\\n\\n### dept_manager (24 rows)\\n- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\\n- from_date: date, NOT NULL\\n- to_date: date, NOT NULL\\n- dept_no: character (PK) (FK → departments.dept_no), NOT NULL\\n\\n### salaries (2839079 rows)\\n- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\\n- salary: integer, NOT NULL\\n- from_date: date (PK), NOT NULL\\n- to_date: date, NOT NULL\\n\\n### titles (442547 rows)\\n- emp_no: integer (PK) (FK → employees.emp_no), NOT NULL\\n- from_date: date (PK), NOT NULL\\n- to_date: date\\n- title: character varying (PK), NOT NULL\\n\\n\\n## Query Request\\nWhich departments have more than 70% of employees of one gender (current employees only)? Show dept_name, gender, and percentage.\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "postgres.gen_template(q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| dept_name   | gender   | percentage   |\n",
      "|-------------|----------|--------------|\n"
     ]
    }
   ],
   "source": [
    "query = input(\":\")\n",
    "postgres.execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| dept_name   |   avg_salary |\n",
      "|-------------|--------------|\n",
      "| Marketing   |       106491 |\n"
     ]
    }
   ],
   "source": [
    "query=\"\"\"WITH \n",
    "  -- Get the most recent salary for each employee\n",
    "  recent_salaries AS (\n",
    "    SELECT \n",
    "      e.emp_no,\n",
    "      s.salary,\n",
    "      s.from_date\n",
    "    FROM \n",
    "      employees e\n",
    "    JOIN \n",
    "      salaries s ON e.emp_no = s.emp_no\n",
    "    WHERE \n",
    "      s.to_date = '9999-01-01' AND s.from_date <= '2022-01-01'\n",
    "  ),\n",
    "  -- Get the manager's department for each employee\n",
    "  manager_departments AS (\n",
    "    SELECT \n",
    "      d.dept_no\n",
    "    FROM \n",
    "      dept_manager m\n",
    "    JOIN \n",
    "      departments d ON m.dept_no = d.dept_no\n",
    "    WHERE \n",
    "      m.to_date = '9999-01-01'\n",
    "  ),\n",
    "  -- Get the department with the highest average salary for its managers\n",
    "  highest_avg_salary AS (\n",
    "    SELECT \n",
    "      md.dept_no,\n",
    "      AVG(rs.salary) AS avg_salary\n",
    "    FROM \n",
    "      manager_departments md\n",
    "    JOIN \n",
    "      recent_salaries rs ON md.dept_no IN (SELECT dept_no FROM departments)\n",
    "    WHERE \n",
    "      rs.from_date >= '2012-01-01'\n",
    "    GROUP BY \n",
    "      md.dept_no\n",
    "  )\n",
    "  SELECT \n",
    "    d.dept_name\n",
    "  FROM \n",
    "    highest_avg_salary h\n",
    "  JOIN \n",
    "    departments d ON h.dept_no = d.dept_no\n",
    "  ORDER BY \n",
    "    h.avg_salary DESC\n",
    "  LIMIT 1;\n",
    "\"\"\"\n",
    "query=\"\"\"WITH recent_salaries AS (\n",
    "  SELECT \n",
    "    s.emp_no,\n",
    "    s.salary\n",
    "  FROM \n",
    "    salaries s\n",
    "  WHERE \n",
    "    s.to_date = '9999-01-01'\n",
    "),\n",
    "active_managers_last_10_years AS (\n",
    "  SELECT \n",
    "    dm.emp_no,\n",
    "    dm.dept_no\n",
    "  FROM \n",
    "    dept_manager dm\n",
    "  WHERE \n",
    "    dm.to_date >= CURRENT_DATE - INTERVAL '10 years'\n",
    "),\n",
    "manager_avg_salary AS (\n",
    "  SELECT \n",
    "    am.dept_no,\n",
    "    AVG(rs.salary) AS avg_salary\n",
    "  FROM \n",
    "    active_managers_last_10_years am\n",
    "  JOIN \n",
    "    recent_salaries rs ON am.emp_no = rs.emp_no\n",
    "  GROUP BY \n",
    "    am.dept_no\n",
    ")\n",
    "SELECT \n",
    "  d.dept_name,\n",
    "  mas.avg_salary\n",
    "FROM \n",
    "  manager_avg_salary mas\n",
    "JOIN \n",
    "  departments d ON mas.dept_no = d.dept_no\n",
    "ORDER BY \n",
    "  mas.avg_salary DESC\n",
    "LIMIT 1;\n",
    "\"\"\"\n",
    "postgres.execute_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
